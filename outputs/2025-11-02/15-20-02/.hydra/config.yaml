model:
  _target_: builtins.dict
  model_type: ExplicitLM
  model_variant: model_memory
  dim: 64
  n_layers: 2
  n_heads: 16
  n_kv_heads: 8
  vocab_size: 6400
  max_seq_len: 512
  dropout: 0.0
  norm_eps: 1.0e-05
  rope_theta: 1000000.0
  flash_attn: true
  multiple_of: 64
  use_token_memory: true
  knowledge_dim: 128
  knowledge_length: 16
  knowledge_num: 1024
  cache_path: cache/knowledge_cache.pt
  recompute_cache: false
  disable_db: false
  database_init_path: null
  freeze_ratio: 0.2
  use_moe: false
  n_routed_experts: 4
  n_shared_experts: true
  num_experts_per_tok: 2
  aux_loss_alpha: 0.1
  gumbel_temperature: 1.0
  norm_topk_prob: true
  scoring_func: softmax
  use_ema_update: true
  ema_decay: 0.9
  ema_update_freq: 5
dataset:
  _target_: builtins.dict
  dataset_path: data/database/merged_pretrain.jsonl
  val_dataset_path: data/benchmarks/eval_data.jsonl
  max_subject_len: 8
  max_predicate_len: 4
  max_object_len: 8
logging:
  _target_: builtins.dict
  use_swanlab: true
  swanlab_online: true
  swanlab_project: ExplicitLM
  log_interval: 100
  out_dir: out
  save_dir: out
training:
  _target_: builtins.dict
  batch_size: 1
  accumulation_steps: 16
  epochs: 1
  embeddings_epoch: 2
  learning_rate: 0.0002
  seq_aux: true
  num_candidates: 16
  num_selected: 1
  transformers_version: 4.57.0
  zero_stage: 2
  mixed_precision: bf16
  seed: 1337
  devices: auto
  strategy: deepspeed_stage_2
  log_interval: 10
