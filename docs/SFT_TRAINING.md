# è®°å¿†ç»„ä»¶è®­ç»ƒæŒ‡å—ï¼šä½¿ç”¨ OMCQ æ•°æ®è®­ç»ƒçŸ¥è¯†èåˆæ¨¡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•ä½¿ç”¨ OMCQ æ•°æ®å¯¹ ExplicitLM çš„è®°å¿†ç»„ä»¶è¿›è¡Œè®­ç»ƒã€‚

## æ¦‚è¿°

**æ³¨æ„**ï¼šè¿™ä¸æ˜¯ä¼ ç»Ÿçš„ SFTï¼ˆSupervised Fine-Tuningï¼‰ï¼Œå› ä¸º Qwen3 backbone å®Œå…¨å†»ç»“ï¼Œåªè®­ç»ƒè®°å¿†ç›¸å…³ç»„ä»¶ã€‚

è®­ç»ƒçš„ç›®æ ‡æ˜¯ï¼š
- **å†»ç»“ Qwen3 ä¸»æ¨¡å‹å‚æ•°**ï¼šä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†
- **åªè®­ç»ƒçŸ¥è¯†èåˆæ¨¡å—**ï¼šåŒ…æ‹¬ `memory_gate`ã€`gated_memory_fusion`ã€`memory_norm`
- **ä½¿ç”¨ OMCQ æ•°æ®**ï¼šçº¦ 157 ä¸‡æ¡å¤šé€‰é¢˜æ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹å¦‚ä½•åˆ©ç”¨è®°å¿†åº“å›ç­”é—®é¢˜

## æ­¥éª¤ 1ï¼šæ•°æ®è½¬æ¢

å°† OMCQ æ•°æ®è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼ï¼š

```bash
cd /data2/zengzheni/lvchangwei/new_repo/ExplicitLM

# è½¬æ¢æ•°æ®ï¼ˆæµ‹è¯•æ¨¡å¼ï¼Œåªè½¬æ¢ 10 æ¡ï¼‰
python3 scripts/convert_omcq_to_sft.py \
    --input sft_data/omcq_trex_data.json \
    --output sft_data/omcq_trex_sft.jsonl \
    --max-samples 10

# è½¬æ¢å…¨éƒ¨æ•°æ®ï¼ˆçº¦ 157 ä¸‡æ¡ï¼‰
python3 scripts/convert_omcq_to_sft.py \
    --input sft_data/omcq_trex_data.json \
    --output sft_data/omcq_trex_sft.jsonl
```

è½¬æ¢åçš„æ•°æ®æ ¼å¼ï¼š
```json
{
  "conversations": [
    {
      "role": "user",
      "content": "What is Austroasiatic languages an instance of?\nA:language family,B:pteridosperms,C:FIBT World Championships\nè¯·é€‰æ‹©æ­£ç¡®ç­”æ¡ˆã€‚"
    },
    {
      "role": "assistant",
      "content": "A:language family"
    }
  ]
}
```

## æ­¥éª¤ 2ï¼šéªŒè¯æ•°æ®åŠ è½½

æµ‹è¯•è½¬æ¢åçš„æ•°æ®èƒ½å¦æ­£ç¡®åŠ è½½ï¼š

```bash
python3 scripts/test_sft_data.py \
    --data-path sft_data/omcq_trex_sft.jsonl \
    --qwen3-model-path /path/to/Qwen3-4b
```

## æ­¥éª¤ 3ï¼šé…ç½®è®­ç»ƒå‚æ•°

è®­ç»ƒå‚æ•°å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œç›´æ¥è¦†ç›–é»˜è®¤é…ç½®ã€‚é»˜è®¤é…ç½®åœ¨ `config/` ç›®å½•ä¸‹çš„å„ä¸ªæ–‡ä»¶ä¸­å®šä¹‰ã€‚

**æ¨èé…ç½®**ï¼š
- **å­¦ä¹ ç‡**ï¼š`5e-5` åˆ° `1e-4`ï¼ˆçŸ¥è¯†èåˆæ¨¡å—é€šå¸¸éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
- **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ® GPU æ˜¾å­˜è°ƒæ•´ï¼ˆ1-4ï¼Œåˆ†å¸ƒå¼è®­ç»ƒå»ºè®®ä½¿ç”¨ 1ï¼‰
- **æ¢¯åº¦ç´¯ç§¯**ï¼šä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°åœ¨ 128-256
- **è®­ç»ƒè½®æ•°**ï¼š1-3 è½®ï¼ˆSFT é€šå¸¸ä¸éœ€è¦å¤ªå¤šè½®æ¬¡ï¼‰
- **æ•°æ®åŠ è½½å™¨**ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸­è‡ªåŠ¨ä½¿ç”¨ `num_workers=0` ä»¥é¿å…åŒæ­¥é—®é¢˜

## æ­¥éª¤ 4ï¼šå¯åŠ¨è®­ç»ƒ

### 4.1 ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd /data2/zengzheni/lvchangwei/new_repo/ExplicitLM
bash scripts/run_sft.sh
```

### 4.2 ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°

```bash
cd /data2/zengzheni/lvchangwei/new_repo/ExplicitLM

uv run accelerate launch --config_file accelerate_config.yaml train_memory.py \
    model.qwen3_model_path=/path/to/Qwen3-4b \
    model.cache_path=data/cache/knowledge_cache.pt \
    model.recompute_cache=False \
    dataset.sft_dataset_path=sft_data/omcq_trex_sft.jsonl \
    dataset.pretrained_router_path=router_only.pt \
    dataset.pretrained_fusion_path="" \
    dataset.sft_val_dataset_path=data/benchmarks/eval_data.json \
    training.learning_rate=5e-5 \
    training.batch_size=1 \
    training.epochs=3 \
    training.accumulation_steps=128 \
    training.zero_stage=2 \
    model.keys_path=data/keys.pt \
    model.gate_rank=128 \
    model.fusion_rank=128 \
    logging.out_dir=out \
    logging.save_dir=out
```

**å‚æ•°è¯´æ˜**ï¼š
- ä½¿ç”¨ `key=value` æ ¼å¼ï¼Œæ”¯æŒç‚¹å·è®¿é—®ï¼ˆå¦‚ `model.qwen3_model_path`ï¼‰
- å‚æ•°ä¼šè‡ªåŠ¨è¿›è¡Œç±»å‹è½¬æ¢ï¼ˆå¸ƒå°”å€¼ã€æ•´æ•°ã€æµ®ç‚¹æ•°ï¼ŒåŒ…æ‹¬ç§‘å­¦è®¡æ•°æ³•å¦‚ `5e-5`ï¼‰
- æ‰€æœ‰å‚æ•°éƒ½æ˜¯å¯é€‰çš„ï¼ŒæœªæŒ‡å®šçš„å‚æ•°ä½¿ç”¨é»˜è®¤å€¼ï¼ˆå®šä¹‰åœ¨ `config/` ç›®å½•ä¸‹ï¼‰

## æ­¥éª¤ 5ï¼šç›‘æ§è®­ç»ƒ

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šï¼š
1. **è‡ªåŠ¨å†»ç»“ Qwen3 ä¸»æ¨¡å‹å‚æ•°**ï¼šåªè®­ç»ƒçŸ¥è¯†èåˆæ¨¡å—
2. **æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡**ï¼šç¡®è®¤åªæœ‰è®°å¿†ç›¸å…³ç»„ä»¶åœ¨è®­ç»ƒï¼ˆåº”è¯¥æ˜¾ç¤º **0.208B** å¯è®­ç»ƒå‚æ•°ï¼‰
3. **è®°å½•è®­ç»ƒæŸå¤±**ï¼šé€šè¿‡ SwanLab å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰

### å‚æ•°å†»ç»“éªŒè¯

è®­ç»ƒå¼€å§‹æ—¶ä¼šè¾“å‡ºï¼š
```
ğŸ”’ å†»ç»“Qwenä¸»æ¨¡å‹å‚æ•°...
âœ… å‚æ•°å†»ç»“å®Œæˆ:
  - å†»ç»“å‚æ•°: XXXX.XXX ç™¾ä¸‡
  - å¯è®­ç»ƒå‚æ•°: 208.XXX ç™¾ä¸‡ï¼ˆçº¦ 0.208Bï¼‰
  - å†»ç»“æ¯”ä¾‹: XX.XX%
```

**é‡è¦**ï¼šç¡®è®¤å¯è®­ç»ƒå‚æ•°ä¸º **0.208B**ï¼ˆä¸æ˜¯ 0.265B æˆ–æ›´é«˜ï¼‰ã€‚å¦‚æœæ˜¾ç¤ºæ›´é«˜ï¼Œè¯´æ˜ keys è¢«é”™è¯¯è®¾ç½®ä¸ºå¯è®­ç»ƒï¼Œéœ€è¦æ£€æŸ¥å‚æ•°å†»ç»“é€»è¾‘ã€‚

## æ­¥éª¤ 6ï¼šéªŒè¯è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼Œæ£€æŸ¥è¾“å‡ºç›®å½•ä¸­çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼š
- `out/sft_*.pth` - è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆæŒ‰æ­¥æ•°å‘½åï¼‰

å¯ä»¥ä½¿ç”¨è¿™äº›æ£€æŸ¥ç‚¹è¿›è¡Œæ¨ç†æˆ–ç»§ç»­è®­ç»ƒã€‚

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å° `batch_size`
- å¢åŠ  `accumulation_steps` ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
- å‡å° `max_seq_len`ï¼ˆåœ¨ `config/model.py` ä¸­ï¼‰

### Q2: è®­ç»ƒæŸå¤±ä¸ä¸‹é™ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°ï¼šå°è¯•è°ƒæ•´ `learning_rate`ï¼ˆ1e-5 åˆ° 1e-4ï¼‰
- æ•°æ®æ ¼å¼é—®é¢˜ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®è½¬æ¢
- å‚æ•°å†»ç»“é—®é¢˜ï¼šç¡®è®¤åªæœ‰çŸ¥è¯†èåˆæ¨¡å—åœ¨è®­ç»ƒ

### Q3: å¦‚ä½•åªè®­ç»ƒéƒ¨åˆ†çŸ¥è¯†èåˆæ¨¡å—ï¼Ÿ

ä¿®æ”¹ `train_memory.py` ä¸­çš„å‚æ•°å†»ç»“é€»è¾‘ï¼Œè°ƒæ•´ `is_memory_component` çš„åˆ¤æ–­æ¡ä»¶ã€‚

### Q4: è®­ç»ƒåç”Ÿæˆæ•ˆæœæ²¡æœ‰æ”¹å–„ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
- è®­ç»ƒè½®æ•°ä¸è¶³ï¼šå°è¯•å¢åŠ  `epochs`
- å­¦ä¹ ç‡ä¸åˆé€‚ï¼šå°è¯•ä¸åŒçš„å­¦ä¹ ç‡
- æ•°æ®è´¨é‡é—®é¢˜ï¼šæ£€æŸ¥æ•°æ®è½¬æ¢æ˜¯å¦æ­£ç¡®
- éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®ï¼šè€ƒè™‘ä½¿ç”¨æ›´å¤šæ•°æ®

## ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ï¼š
1. è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°
2. æµ‹è¯•ç”Ÿæˆè´¨é‡ï¼ˆä½¿ç”¨ `examples/quick_start.py`ï¼‰
3. è°ƒæ•´è¶…å‚æ•°å¹¶é‡æ–°è®­ç»ƒ
4. ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†

