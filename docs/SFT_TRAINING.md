# SFT è®­ç»ƒæŒ‡å—ï¼šä½¿ç”¨ OMCQ æ•°æ®è®­ç»ƒçŸ¥è¯†èåˆæ¨¡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•ä½¿ç”¨ OMCQ æ•°æ®å¯¹ ExplicitLM çš„çŸ¥è¯†èåˆæ¨¡å—è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚

## æ¦‚è¿°

SFT è®­ç»ƒçš„ç›®æ ‡æ˜¯ï¼š
- **å†»ç»“ Qwen3 ä¸»æ¨¡å‹å‚æ•°**ï¼šä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†
- **åªè®­ç»ƒçŸ¥è¯†èåˆæ¨¡å—**ï¼šåŒ…æ‹¬ `memory_gate`ã€`gated_memory_fusion`ã€`memory_norm`
- **ä½¿ç”¨ OMCQ æ•°æ®**ï¼šçº¦ 157 ä¸‡æ¡å¤šé€‰é¢˜æ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹å¦‚ä½•åˆ©ç”¨è®°å¿†åº“å›ç­”é—®é¢˜

## æ­¥éª¤ 1ï¼šæ•°æ®è½¬æ¢

å°† OMCQ æ•°æ®è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼ï¼š

```bash
cd /data2/zengzheni/lvchangwei/new_repo/ExplicitLM

# è½¬æ¢æ•°æ®ï¼ˆæµ‹è¯•æ¨¡å¼ï¼Œåªè½¬æ¢ 10 æ¡ï¼‰
python3 scripts/convert_omcq_to_sft.py \
    --input sft_data/omcq_trex_data.json \
    --output sft_data/omcq_trex_sft.jsonl \
    --max-samples 10

# è½¬æ¢å…¨éƒ¨æ•°æ®ï¼ˆçº¦ 157 ä¸‡æ¡ï¼‰
python3 scripts/convert_omcq_to_sft.py \
    --input sft_data/omcq_trex_data.json \
    --output sft_data/omcq_trex_sft.jsonl
```

è½¬æ¢åçš„æ•°æ®æ ¼å¼ï¼š
```json
{
  "conversations": [
    {
      "role": "user",
      "content": "What is Austroasiatic languages an instance of?\nA:language family,B:pteridosperms,C:FIBT World Championships\nè¯·é€‰æ‹©æ­£ç¡®ç­”æ¡ˆã€‚"
    },
    {
      "role": "assistant",
      "content": "A:language family"
    }
  ]
}
```

## æ­¥éª¤ 2ï¼šéªŒè¯æ•°æ®åŠ è½½

æµ‹è¯•è½¬æ¢åçš„æ•°æ®èƒ½å¦æ­£ç¡®åŠ è½½ï¼š

```bash
python3 scripts/test_sft_data.py \
    --data-path sft_data/omcq_trex_sft.jsonl \
    --qwen3-model-path /path/to/Qwen3-4b
```

## æ­¥éª¤ 3ï¼šé…ç½®è®­ç»ƒå‚æ•°

### 3.1 æ›´æ–°æ•°æ®é›†é…ç½®

ç¼–è¾‘ `config/dataset.py`ï¼Œè®¾ç½® SFT æ•°æ®è·¯å¾„ï¼š

```python
DatasetConf = builds(
    dict,
    # ... å…¶ä»–é…ç½® ...
    # ---- sft ç›¸å…³å­—æ®µ ----
    pretrained_sft_model_path="out/pretrain_latest.pth",  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    sft_dataset_path="sft_data/omcq_trex_sft.jsonl",      # SFT è®­ç»ƒæ•°æ®
    sft_val_dataset_path="data/benchmarks/eval_data.json", # SFT éªŒè¯æ•°æ®
)
```

### 3.2 æ›´æ–°æ¨¡å‹é…ç½®

ç¼–è¾‘ `config/model.py`ï¼Œç¡®ä¿ï¼š
- ä½¿ç”¨é¢„è®­ç»ƒçš„ cache çŸ¥è¯†åº“
- å‚æ•°å†»ç»“å·²å¯ç”¨ï¼ˆåœ¨ `model_initializer.py` ä¸­è‡ªåŠ¨å¤„ç†ï¼‰

```python
ModelConf = builds(
    dict,
    # ... å…¶ä»–é…ç½® ...
    cache_path="data/cache/knowledge_cache.pt",  # ä½¿ç”¨é¢„è®­ç»ƒçš„ cache
    recompute_cache=False,                        # ä¸é‡æ–°è®¡ç®—
    use_ema_update=False,                         # SFT æ—¶é€šå¸¸ä¸ä½¿ç”¨ EMA
)
```

### 3.3 æ›´æ–°è®­ç»ƒé…ç½®

ç¼–è¾‘ `config/training.py`ï¼Œè®¾ç½® SFT è®­ç»ƒè¶…å‚æ•°ï¼š

```python
TrainingConf = builds(
    dict,
    batch_size=4,                    # æ‰¹æ¬¡å¤§å°
    accumulation_steps=32,           # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    epochs=3,                        # è®­ç»ƒè½®æ•°
    learning_rate=5e-5,              # å­¦ä¹ ç‡ï¼ˆSFT é€šå¸¸è¾ƒå°ï¼‰
    # ... å…¶ä»–é…ç½® ...
)
```

**æ¨èé…ç½®**ï¼š
- **å­¦ä¹ ç‡**ï¼š`5e-5` åˆ° `1e-4`ï¼ˆçŸ¥è¯†èåˆæ¨¡å—é€šå¸¸éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
- **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ® GPU æ˜¾å­˜è°ƒæ•´ï¼ˆ4-8ï¼‰
- **æ¢¯åº¦ç´¯ç§¯**ï¼šä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°åœ¨ 128-256
- **è®­ç»ƒè½®æ•°**ï¼š1-3 è½®ï¼ˆSFT é€šå¸¸ä¸éœ€è¦å¤ªå¤šè½®æ¬¡ï¼‰

## æ­¥éª¤ 4ï¼šå¯åŠ¨è®­ç»ƒ

### 4.1 ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®

```bash
cd /data2/zengzheni/lvchangwei/new_repo/ExplicitLM

python3 2_sft.py \
    model.qwen3_model_path=/path/to/Qwen3-4b \
    model.cache_path=data/cache/knowledge_cache.pt \
    model.recompute_cache=False \
    dataset.sft_dataset_path=sft_data/omcq_trex_sft.jsonl \
    dataset.pretrained_sft_model_path=out/pretrain_latest.pth \
    training.learning_rate=5e-5 \
    training.batch_size=4 \
    training.epochs=3 \
    training.accumulation_steps=32
```

### 4.2 ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

åˆ›å»ºé…ç½®æ–‡ä»¶ `config/sft_omcq.yaml`ï¼š

```yaml
defaults:
  - model
  - dataset
  - training
  - logging

model:
  qwen3_model_path: /path/to/Qwen3-4b
  cache_path: data/cache/knowledge_cache.pt
  recompute_cache: false

dataset:
  sft_dataset_path: sft_data/omcq_trex_sft.jsonl
  pretrained_sft_model_path: out/pretrain_latest.pth

training:
  learning_rate: 5e-5
  batch_size: 4
  epochs: 3
  accumulation_steps: 32
```

ç„¶åè¿è¡Œï¼š
```bash
python3 2_sft.py --config-name sft_omcq
```

## æ­¥éª¤ 5ï¼šç›‘æ§è®­ç»ƒ

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šï¼š
1. **è‡ªåŠ¨å†»ç»“ Qwen3 ä¸»æ¨¡å‹å‚æ•°**ï¼šåªè®­ç»ƒçŸ¥è¯†èåˆæ¨¡å—
2. **æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡**ï¼šç¡®è®¤åªæœ‰è®°å¿†ç›¸å…³ç»„ä»¶åœ¨è®­ç»ƒ
3. **è®°å½•è®­ç»ƒæŸå¤±**ï¼šé€šè¿‡ SwanLab å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰

### å‚æ•°å†»ç»“éªŒè¯

è®­ç»ƒå¼€å§‹æ—¶ä¼šè¾“å‡ºï¼š
```
ğŸ”’ å†»ç»“Qwenä¸»æ¨¡å‹å‚æ•°...
âœ… å‚æ•°å†»ç»“å®Œæˆ:
  - å†»ç»“å‚æ•°: XXXX.XXX ç™¾ä¸‡
  - å¯è®­ç»ƒå‚æ•°: XX.XXX ç™¾ä¸‡
  - å†»ç»“æ¯”ä¾‹: XX.XX%
```

ç¡®è®¤å¯è®­ç»ƒå‚æ•°æ•°é‡åˆç†ï¼ˆé€šå¸¸åªæœ‰å‡ ç™¾ä¸‡å‚æ•°ï¼Œä¸»è¦æ˜¯çŸ¥è¯†èåˆæ¨¡å—ï¼‰ã€‚

## æ­¥éª¤ 6ï¼šéªŒè¯è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ `examples/quick_start.py` æµ‹è¯•ç”Ÿæˆæ•ˆæœï¼š

```python
# åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
args = {
    'qwen3_model_path': '/path/to/Qwen3-4b',
    'cache_path': 'data/cache/knowledge_cache.pt',
    # ... å…¶ä»–é…ç½® ...
}

model, tokenizer = init_model(args)
# åŠ è½½ SFT åçš„æƒé‡
model.load_state_dict(torch.load('out/sft_latest.pth'))

# æµ‹è¯•ç”Ÿæˆ
# ...
```

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å° `batch_size`
- å¢åŠ  `accumulation_steps` ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
- å‡å° `max_seq_len`ï¼ˆåœ¨ `config/model.py` ä¸­ï¼‰

### Q2: è®­ç»ƒæŸå¤±ä¸ä¸‹é™ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°ï¼šå°è¯•è°ƒæ•´ `learning_rate`ï¼ˆ1e-5 åˆ° 1e-4ï¼‰
- æ•°æ®æ ¼å¼é—®é¢˜ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®è½¬æ¢
- å‚æ•°å†»ç»“é—®é¢˜ï¼šç¡®è®¤åªæœ‰çŸ¥è¯†èåˆæ¨¡å—åœ¨è®­ç»ƒ

### Q3: å¦‚ä½•åªè®­ç»ƒéƒ¨åˆ†çŸ¥è¯†èåˆæ¨¡å—ï¼Ÿ

ä¿®æ”¹ `utils/model_initializer.py` ä¸­çš„ `_freeze_qwen_params` å‡½æ•°ï¼Œè°ƒæ•´ `is_memory_component` çš„åˆ¤æ–­é€»è¾‘ã€‚

### Q4: è®­ç»ƒåç”Ÿæˆæ•ˆæœæ²¡æœ‰æ”¹å–„ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
- è®­ç»ƒè½®æ•°ä¸è¶³ï¼šå°è¯•å¢åŠ  `epochs`
- å­¦ä¹ ç‡ä¸åˆé€‚ï¼šå°è¯•ä¸åŒçš„å­¦ä¹ ç‡
- æ•°æ®è´¨é‡é—®é¢˜ï¼šæ£€æŸ¥æ•°æ®è½¬æ¢æ˜¯å¦æ­£ç¡®
- éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®ï¼šè€ƒè™‘ä½¿ç”¨æ›´å¤šæ•°æ®

## ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ï¼š
1. è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°
2. æµ‹è¯•ç”Ÿæˆè´¨é‡ï¼ˆä½¿ç”¨ `examples/quick_start.py`ï¼‰
3. è°ƒæ•´è¶…å‚æ•°å¹¶é‡æ–°è®­ç»ƒ
4. ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†

