from transformers import PretrainedConfig
from typing import List


class LMConfig(PretrainedConfig):
    model_type = "ExplicitLM"

    def __init__(
            self,
            ####################################################
            # åŸºæœ¬æ¨¡å‹æ¶æ„å‚æ•°
            ####################################################
            dim: int = 512,
            n_layers: int = 8,
            n_heads: int = 16,
            n_kv_heads: int = 8,
            vocab_size: int = 6400,
            hidden_dim: int = None,
            multiple_of: int = 64,
            norm_eps: float = 1e-5,
            max_seq_len: int = 512,
            rope_theta: int = 1e6,
            dropout: float = 0.0,
            flash_attn: bool = True,
            embeddings_epoch: int = 2,
            ####################################################
            # DBç›¸å…³é…ç½®
            ####################################################
            disable_db: bool = False,  # ç‰¹æ®Šæ¨¡å¼ï¼šç¦ç”¨æ•°æ®åº“åŠŸèƒ½
            ####################################################
            # MOEç›¸å…³é…ç½®
            # å½“use_moeä¸ºfalseæ—¶ï¼Œä»¥ä¸‹å‚æ•°æ— æ•ˆ
            ####################################################
            use_moe: bool = False,
            num_experts_per_tok: int = 2,
            n_routed_experts: int = 4,
            n_shared_experts: bool = True,
            scoring_func: str = 'softmax',
            aux_loss_alpha: float = 0.1,
            seq_aux: bool = True,
            norm_topk_prob: bool = True,
            ####################################################
            # çŸ¥è¯†åº“ç›¸å…³é…ç½®
            ####################################################
            knowledge_num: int = 1024*1024,
            knowledge_length: int = 16,
            knowledge_dim: int = 128,
            ####################################################
            # è®°å¿†åº“ç›¸å…³é…ç½®
            ####################################################
            use_token_memory: bool = True,  # ğŸ”¥ 1.4.6: æ–°å¢token-based memory flag
            freeze_ratio: float = 0.2,     # ğŸ”¥ æ–°å¢: memory_bankå†»ç»“ç‡ (0.0è¡¨ç¤ºä¸å†»ç»“ï¼Œ0.2è¡¨ç¤º20%æ¡ç›®ä¸æ›´æ–°)
            ####################################################
            # å®éªŒ1.4.10: ä¼˜åŒ–çš„Gumbel-Softmax + å¤šæ ·æ€§æŸå¤±
            ####################################################
            num_candidates: int = 16,       # ğŸ”¥ å®éªŒ1.4.10ä¼˜åŒ–: å€™é€‰è®°å¿†æ¡ç›®æ•°é‡ (32â†’16 å‡å°‘50%æ˜¾å­˜)
            num_selected: int = 1,          # ğŸ”¥ å®éªŒ1.4.10: é€‰ä¸­çš„è®°å¿†æ¡ç›®æ•°é‡ (ç°åœ¨åªé€‰1ä¸ªæœ€ä½³)
            gumbel_temperature: float = 1.0, # ğŸ”¥ å®éªŒ1.4.10: Gumbel-Softmaxæ¸©åº¦å‚æ•°
            ####################################################
            # ä¸‰å…ƒç»„æå–ç›¸å…³é…ç½®
            ####################################################
            max_subject_len: int = 8,
            max_predicate_len: int = 4,
            max_object_len: int = 8,
            ####################################################
            # SwanLabå®éªŒè¿½è¸ªç›¸å…³é…ç½®
            ####################################################
            use_swanlab: bool = False,
            swanlab_online: bool = False,
            swanlab_project: str = "ExplicitLM",
            ####################################################
            # æ¨¡å‹åˆå§‹åŒ–ç›¸å…³é…ç½®
            ####################################################
            model_variant: str = "model_memory",  # æ¨¡å‹å˜ä½“ç±»å‹ï¼šmodel, model_original, model_no_feed, model_memory
            pretrained_embedding_path: str = None,  # é¢„è®­ç»ƒåµŒå…¥æƒé‡æ–‡ä»¶è·¯å¾„
            database_init_path: str = None,  # çŸ¥è¯†åº“/è®°å¿†åº“åˆå§‹åŒ–æ•°æ®æ–‡ä»¶è·¯å¾„
            cache_path: str = "cache/knowledge_cache.pt",  # å¤„ç†åæ•°æ®çš„ç¼“å­˜è·¯å¾„
            recompute_cache: bool = False,  # æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—ç¼“å­˜
            ####################################################
            # è®­ç»ƒç›¸å…³é…ç½®
            ####################################################
            dataset_path: str = "data/database/merged_pretrain.jsonl",  # é¢„è®­ç»ƒæ•°æ®é›†è·¯å¾„
            val_dataset_path: str = "data/benchmarks/eval_data.json",  # éªŒè¯æ•°æ®é›†è·¯å¾„
            batch_size: int = 48,  # è®­ç»ƒæ‰¹æ¬¡å¤§å°
            accumulation_steps: int = 16,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            epochs: int = 3,  # è®­ç»ƒè½®æ•°
            learning_rate: float = 2e-4,  # å­¦ä¹ ç‡
            out_dir: str = "out",  # è¾“å‡ºç›®å½•
            ####################################################
            # logç›¸å…³é…ç½®
            ####################################################
            log_interval:int = 10,
            **kwargs
    ):
        ####################################################
        # åŸºæœ¬æ¨¡å‹æ¶æ„å‚æ•°
        ####################################################
        self.dim = dim
        self.n_layers = n_layers
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.multiple_of = multiple_of
        self.norm_eps = norm_eps
        self.max_seq_len = max_seq_len
        self.rope_theta = rope_theta
        self.dropout = dropout
        self.flash_attn = flash_attn
        self.embeddings_epoch = embeddings_epoch
        ####################################################
        # DBç›¸å…³é…ç½®
        ####################################################
        self.disable_db = disable_db  # è®¾ç½®æ˜¯å¦ç¦ç”¨æ•°æ®åº“
        ####################################################
        # MOEç›¸å…³é…ç½®
        # å½“use_moeä¸ºfalseæ—¶ï¼Œä»¥ä¸‹å‚æ•°æ— æ•ˆ
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡
        ####################################################
        # çŸ¥è¯†åº“ç›¸å…³é…ç½®
        ####################################################
        self.knowledge_num = knowledge_num
        self.knowledge_length = knowledge_length
        self.knowledge_dim = knowledge_dim
        ####################################################
        # è®°å¿†åº“ç›¸å…³é…ç½®
        ####################################################
        # Memory bankåœ¨è®­ç»ƒæ—¶å›ºå®šï¼Œæ¨ç†æ—¶é€šè¿‡LLMLinguaæ›´æ–°
        self.use_token_memory = use_token_memory  # ğŸ”¥ 1.4.6: token-based memory flag
        self.freeze_ratio = freeze_ratio  # ğŸ”¥ æ–°å¢: memory_bankå†»ç»“ç‡
        ####################################################
        # å®éªŒ1.4.10: ä¼˜åŒ–çš„Gumbel-Softmax + å¤šæ ·æ€§æŸå¤±
        ####################################################
        self.num_candidates = num_candidates
        self.num_selected = num_selected
        self.gumbel_temperature = gumbel_temperature
        ####################################################
        # ä¸‰å…ƒç»„æå–ç›¸å…³é…ç½®
        ####################################################
        self.max_subject_len = max_subject_len
        self.max_predicate_len = max_predicate_len
        self.max_object_len = max_object_len
        ####################################################
        # SwanLabå®éªŒè¿½è¸ªç›¸å…³é…ç½®
        ####################################################
        self.use_swanlab = use_swanlab
        self.swanlab_online = swanlab_online
        self.swanlab_project = swanlab_project
        ####################################################
        # æ¨¡å‹åˆå§‹åŒ–ç›¸å…³é…ç½®
        ####################################################
        self.model_variant = model_variant
        self.pretrained_embedding_path = pretrained_embedding_path
        self.database_init_path = database_init_path
        self.cache_path = cache_path
        self.recompute_cache = recompute_cache
        ####################################################
        # è®­ç»ƒç›¸å…³é…ç½®
        ####################################################
        self.dataset_path = dataset_path
        self.val_dataset_path = val_dataset_path
        self.batch_size = batch_size
        self.accumulation_steps = accumulation_steps
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.out_dir = out_dir
        ####################################################
        # Logç›¸å…³é…ç½®
        ####################################################
        self.log_interval = log_interval
        super().__init__(**kwargs)