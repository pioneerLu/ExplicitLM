#!/usr/bin/env python3
"""
é˜¶æ®µ2ï¼šè®­ç»ƒçŸ¥è¯†èåˆç»„ä»¶ï¼ˆGatedMemoryFusion + memory_normï¼‰

è®­ç»ƒç›®æ ‡ï¼š
- åŠ è½½é¢„è®­ç»ƒçš„ MemoryGateï¼ˆå†»ç»“ï¼‰
- åªè®­ç»ƒ GatedMemoryFusion å’Œ memory_norm
- å†»ç»“ Backbone å’Œ MemoryGate
"""

import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import get_linear_schedule_with_warmup
from accelerate import Accelerator
from tqdm import tqdm
import swanlab
from pathlib import Path

from utils.model_initializer import init_model, load_pretrained_memory_gate
from utils.pretrain_datasets import create_pretrain_dataloader, create_validation_dataloader
from utils.logger import Logger


def freeze_parameters_for_fusion_training(model, accelerator):
    """
    å†»ç»“å‚æ•°ï¼šåªè®­ç»ƒ gated_memory_fusion å’Œ memory_norm
    å†»ç»“ï¼šbackbone, memory_gate
    """
    Logger("ğŸ”’ è®¾ç½®å‚æ•°å†»ç»“ç­–ç•¥ï¼ˆé˜¶æ®µ2ï¼šåªè®­ç»ƒ Fusionï¼‰", accelerator)
    
    frozen_params = 0
    trainable_params = 0
    
    for name, param in model.named_parameters():
        # å¯è®­ç»ƒï¼šgated_memory_fusion, memory_norm
        is_fusion_component = any(keyword in name for keyword in [
            "gated_memory_fusion",
            "memory_norm",
        ])
        
        # å†»ç»“ï¼šmemory_gate, backbone, memory_bank
        is_memory_gate = "memory_gate" in name
        is_memory_bank = "memory_bank" in name
        
        if is_memory_bank:
            param.requires_grad = False
            frozen_params += param.numel()
        elif is_fusion_component:
            param.requires_grad = True
            trainable_params += param.numel()
        elif is_memory_gate:
            # å†»ç»“ MemoryGateï¼ˆå·²é¢„è®­ç»ƒï¼‰
            param.requires_grad = False
            frozen_params += param.numel()
        else:
            # å†»ç»“æ‰€æœ‰å…¶ä»–å‚æ•°ï¼ˆbackboneï¼‰
            param.requires_grad = False
            frozen_params += param.numel()
    
    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    Logger(f"å‚æ•°å†»ç»“å®Œæˆ: å†»ç»“ {frozen_params / 1e6:.3f}M, å¯è®­ç»ƒ {trainable_params / 1e6:.3f}M", accelerator)
    
    return frozen_params, trainable_params


def main():
    parser = argparse.ArgumentParser(description="é˜¶æ®µ2ï¼šè®­ç»ƒçŸ¥è¯†èåˆç»„ä»¶")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--qwen3_model_path", type=str, required=True, help="Qwen3 æ¨¡å‹è·¯å¾„")
    parser.add_argument("--pretrained_memory_gate_path", type=str, required=True, help="é¢„è®­ç»ƒ MemoryGate æƒé‡è·¯å¾„")
    parser.add_argument("--knowledge_num", type=int, default=1024*1024, help="è®°å¿†åº“æ¡ç›®æ•°")
    parser.add_argument("--knowledge_length", type=int, default=16, help="æ¯ä¸ªè®°å¿†æ¡ç›®çš„ token æ•°")
    parser.add_argument("--knowledge_dim", type=int, default=128, help="è®°å¿†åµŒå…¥ç»´åº¦")
    parser.add_argument("--num_candidates", type=int, default=8, help="å€™é€‰è®°å¿†æ•°")
    parser.add_argument("--num_selected", type=int, default=1, help="é€‰ä¸­çš„è®°å¿†æ•°")
    parser.add_argument("--gumbel_temperature", type=float, default=1.0, help="Gumbel-Softmax æ¸©åº¦")
    
    # æ•°æ®é…ç½®
    parser.add_argument("--dataset_path", type=str, required=True, help="è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰")
    parser.add_argument("--val_dataset_path", type=str, default=None, help="éªŒè¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--accumulation_steps", type=int, default=16, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--warmup_steps", type=int, default=100, help="Warmup æ­¥æ•°")
    
    # Loss é…ç½®
    parser.add_argument("--ce_loss_coef", type=float, default=1.0, help="CE Loss ç³»æ•°")
    parser.add_argument("--similarity_loss_coef", type=float, default=0.0, help="Similarity Loss ç³»æ•°ï¼ˆé˜¶æ®µ2é»˜è®¤0ï¼‰")
    parser.add_argument("--diversity_loss_coef", type=float, default=0.0, help="Diversity Loss ç³»æ•°ï¼ˆé˜¶æ®µ2é»˜è®¤0ï¼‰")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output_dir", type=str, default="checkpoints/fusion", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--save_interval", type=int, default=500, help="ä¿å­˜é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--swanlab_project", type=str, default="explicitlm-fusion", help="SwanLab é¡¹ç›®å")
    parser.add_argument("--swanlab_online", action="store_true", help="ä½¿ç”¨ SwanLab åœ¨çº¿æ¨¡å¼")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ– Accelerator
    accelerator = Accelerator()
    
    # åˆå§‹åŒ– SwanLab
    if accelerator.is_main_process:
        swanlab.init(
            project=args.swanlab_project,
            config=vars(args),
            mode="cloud" if args.swanlab_online else "offline"
        )
    
    Logger("=" * 60, accelerator)
    Logger("é˜¶æ®µ2ï¼šè®­ç»ƒçŸ¥è¯†èåˆç»„ä»¶", accelerator)
    Logger("=" * 60, accelerator)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model_args = {
        "qwen3_model_path": args.qwen3_model_path,
        "knowledge_num": args.knowledge_num,
        "knowledge_length": args.knowledge_length,
        "knowledge_dim": args.knowledge_dim,
        "num_candidates": args.num_candidates,
        "num_selected": args.num_selected,
        "gumbel_temperature": args.gumbel_temperature,
        "use_moe": False,
        "dropout": 0.0,
        "cache_path": None,  # ä¸ä½¿ç”¨ cache
        "recompute_cache": False,
    }
    
    model, tokenizer = init_model(model_args, accelerator)
    Logger("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ", accelerator)
    
    # åŠ è½½é¢„è®­ç»ƒçš„ MemoryGate
    load_pretrained_memory_gate(model, args.pretrained_memory_gate_path, accelerator)
    
    # è®¾ç½®å‚æ•°å†»ç»“ï¼ˆåªè®­ç»ƒ Fusionï¼‰
    freeze_parameters_for_fusion_training(model, accelerator)
    
    # å‡†å¤‡æ•°æ®
    train_loader = create_pretrain_dataloader(
        data_path=args.dataset_path,
        tokenizer=tokenizer,
        batch_size=args.batch_size,
        max_length=args.max_length,
        shuffle=True,
        num_workers=4,
    )
    
    val_loader = None
    if args.val_dataset_path and os.path.exists(args.val_dataset_path):
        val_loader = create_validation_dataloader(
            data_path=args.val_dataset_path,
            tokenizer=tokenizer,
            batch_size=args.batch_size,
            max_length=args.max_length,
            num_workers=4,
        )
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    num_training_steps = args.epochs * len(train_loader) // args.accumulation_steps
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=num_training_steps,
    )
    
    # å‡†å¤‡
    model, optimizer, train_loader, scheduler = accelerator.prepare(
        model, optimizer, train_loader, scheduler
    )
    if val_loader is not None:
        val_loader = accelerator.prepare(val_loader)
    
    # æŸå¤±å‡½æ•°
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if accelerator.is_main_process:
        os.makedirs(args.output_dir, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    best_val_loss = float('inf')
    
    for epoch in range(args.epochs):
        model.train()
        epoch_loss_sum = 0.0
        epoch_ce_loss_sum = 0.0
        epoch_sim_loss_sum = 0.0
        epoch_div_loss_sum = 0.0
        steps_in_epoch = 0
        
        progress_bar = tqdm(train_loader, disable=not accelerator.is_local_main_process)
        
        for step, (X, Y, loss_mask) in enumerate(progress_bar):
            with accelerator.accumulate(model):
                # å‰å‘ä¼ æ’­
                res = model(X)
                
                # è®¡ç®— CE Loss
                ce_loss = loss_fct(
                    res.logits.view(-1, res.logits.size(-1)),
                    Y.view(-1)
                ).view(Y.size())
                ce_loss = (ce_loss * loss_mask).sum() / loss_mask.sum()
                
                # å¤„ç†è¾…åŠ©æŸå¤±
                similarity_loss = torch.tensor(0.0, device=ce_loss.device)
                diversity_loss = torch.tensor(0.0, device=ce_loss.device)
                
                if hasattr(res, 'aux_loss') and res.aux_loss is not None:
                    aux_loss = res.aux_loss
                    if isinstance(aux_loss, dict):
                        similarity_loss = aux_loss.get('similarity_loss', torch.tensor(0.0, device=ce_loss.device))
                        diversity_loss = aux_loss.get('diversity_loss', torch.tensor(0.0, device=ce_loss.device))
                        
                        if isinstance(similarity_loss, torch.Tensor):
                            similarity_loss = accelerator.gather(similarity_loss).mean()
                        if isinstance(diversity_loss, torch.Tensor):
                            diversity_loss = accelerator.gather(diversity_loss).mean()
                
                # æ€»æŸå¤±
                total_loss = (
                    args.ce_loss_coef * ce_loss +
                    args.similarity_loss_coef * similarity_loss +
                    args.diversity_loss_coef * diversity_loss
                ) / args.accumulation_steps
                
                # åå‘ä¼ æ’­
                accelerator.backward(total_loss)
                
                if (step + 1) % args.accumulation_steps == 0:
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1
                
                # ç´¯ç§¯ç»Ÿè®¡
                epoch_loss_sum += total_loss.item() * args.accumulation_steps
                epoch_ce_loss_sum += ce_loss.item()
                epoch_sim_loss_sum += similarity_loss.item() if isinstance(similarity_loss, torch.Tensor) else 0.0
                epoch_div_loss_sum += diversity_loss.item() if isinstance(diversity_loss, torch.Tensor) else 0.0
                steps_in_epoch += 1
                
                # æ—¥å¿—
                if global_step % 10 == 0 and accelerator.is_main_process:
                    running_loss = epoch_loss_sum / steps_in_epoch
                    running_ce = epoch_ce_loss_sum / steps_in_epoch
                    running_sim = epoch_sim_loss_sum / steps_in_epoch
                    running_div = epoch_div_loss_sum / steps_in_epoch
                    
                    swanlab.log({
                        "train/step_loss": total_loss.item() * args.accumulation_steps,
                        "train/running_loss": running_loss,
                        "train/ce_loss": running_ce,
                        "train/similarity_loss": running_sim,
                        "train/diversity_loss": running_div,
                        "train/lr": scheduler.get_last_lr()[0],
                    }, step=global_step)
                    
                    progress_bar.set_description(
                        f"Epoch {epoch} | Loss: {running_loss:.4f} | "
                        f"CE: {running_ce:.4f} | Sim: {running_sim:.4f} | Div: {running_div:.4f}"
                    )
                
                # ä¿å­˜ checkpoint
                if global_step % args.save_interval == 0 and accelerator.is_main_process:
                    save_path = os.path.join(args.output_dir, f"checkpoint_step_{global_step}")
                    os.makedirs(save_path, exist_ok=True)
                    accelerator.save_state(save_path)
                    Logger(f"Checkpoint saved: {save_path}", accelerator)
        
        # Epoch ç»“æŸï¼šè®¡ç®—å¹³å‡æŸå¤±
        avg_loss = epoch_loss_sum / steps_in_epoch
        avg_ce = epoch_ce_loss_sum / steps_in_epoch
        avg_sim = epoch_sim_loss_sum / steps_in_epoch
        avg_div = epoch_div_loss_sum / steps_in_epoch
        
        Logger(f"Epoch {epoch} å®Œæˆ: Loss={avg_loss:.4f}, CE={avg_ce:.4f}, Sim={avg_sim:.4f}, Div={avg_div:.4f}", accelerator)
        
        if accelerator.is_main_process:
            swanlab.log({
                "train/epoch_loss": avg_loss,
                "train/epoch_ce_loss": avg_ce,
                "train/epoch_similarity_loss": avg_sim,
                "train/epoch_diversity_loss": avg_div,
            }, step=global_step)
        
        # éªŒè¯
        if val_loader is not None:
            model.eval()
            val_loss_sum = 0.0
            val_steps = 0
            
            with torch.no_grad():
                for X, Y, loss_mask in val_loader:
                    res = model(X)
                    ce_loss = loss_fct(
                        res.logits.view(-1, res.logits.size(-1)),
                        Y.view(-1)
                    ).view(Y.size())
                    ce_loss = (ce_loss * loss_mask).sum() / loss_mask.sum()
                    val_loss_sum += ce_loss.item()
                    val_steps += 1
            
            avg_val_loss = val_loss_sum / val_steps if val_steps > 0 else 0.0
            Logger(f"éªŒè¯æŸå¤±: {avg_val_loss:.4f}", accelerator)
            
            if accelerator.is_main_process:
                swanlab.log({"val/loss": avg_val_loss}, step=global_step)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    unwrapped_model = accelerator.unwrap_model(model)
                    best_path = os.path.join(args.output_dir, "fusion_best.pth")
                    torch.save(unwrapped_model.state_dict(), best_path)
                    Logger(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path} (val_loss={best_val_loss:.4f})", accelerator)
        
        # ä¿å­˜ epoch checkpoint
        if accelerator.is_main_process:
            unwrapped_model = accelerator.unwrap_model(model)
            epoch_path = os.path.join(args.output_dir, f"fusion_epoch_{epoch}.pth")
            torch.save(unwrapped_model.state_dict(), epoch_path)
            Logger(f"Epoch checkpoint saved: {epoch_path}", accelerator)
    
    # è®­ç»ƒå®Œæˆ
    accelerator.end_training()
    if accelerator.is_main_process:
        swanlab.finish()
        Logger("é˜¶æ®µ2è®­ç»ƒå®Œæˆï¼", accelerator)


if __name__ == "__main__":
    main()

