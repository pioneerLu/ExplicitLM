#!/usr/bin/env python3
"""
è®°å¿†ç»„ä»¶è®­ç»ƒå…¥å£ï¼ˆMemory Components Trainingï¼‰

åŠŸèƒ½ï¼š
- åŸºäº Qwen3-4B é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªè®­ç»ƒè®°å¿†ç›¸å…³ç»„ä»¶
- å†»ç»“ Qwen3 backboneï¼Œåªè®­ç»ƒ MemoryGateã€GatedMemoryFusion å’Œ MemoryNorm
- ä½¿ç”¨å¯¹è¯æ ¼å¼æ•°æ®è®­ç»ƒ
- æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦
"""
import os
import time
import gc
from typing import Optional, Any

try:
    import setproctitle
    process_name = os.environ.get('PYTHON_PROCESS_NAME', 'llama-env')
    setproctitle.setproctitle(process_name)
except ImportError:
    # å¦‚æœæ²¡æœ‰ setproctitleï¼Œå°è¯•ä½¿ç”¨ prctl (Linux only)
    try:
        import prctl
        process_name = os.environ.get('PYTHON_PROCESS_NAME', 'llama-env')
        prctl.set_name(process_name.encode('utf-8'))
    except (ImportError, AttributeError):
        # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡ï¼ˆä¸å½±å“è®­ç»ƒï¼‰
        pass

from accelerate import Accelerator, DistributedDataParallelKwargs, DeepSpeedPlugin
from accelerate.utils import set_seed
import torch
import torch.optim as optim
from transformers import get_cosine_schedule_with_warmup
import argparse
import json
import sys
from config import get_default_config, merge_config
from utils.logger import Logger
from utils.sft_datasets import create_sft_dataloader, create_sft_eval_dataloader
from utils.train_loop_sft import train_epoch_sft
from utils.model_initializer import init_model, load_pretrained_memory_gate, load_pretrained_fusion
from pathlib import Path

try:
    import swanlab
except ImportError:
    swanlab = None


class ConfigDict:
    """é…ç½®å­—å…¸åŒ…è£…ç±»ï¼Œæ”¯æŒç‚¹å·è®¿é—®ï¼ˆå¦‚ cfg.model.qwen3_model_pathï¼‰"""
    def __init__(self, data):
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, dict):
                    setattr(self, key, ConfigDict(value))
                else:
                    setattr(self, key, value)
        else:
            raise ValueError("ConfigDict åªèƒ½ä»å­—å…¸åˆ›å»º")
    
    def get(self, key, default=None):
        """è·å–å±æ€§ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›é»˜è®¤å€¼"""
        return getattr(self, key, default)
    
    def __getitem__(self, key):
        """æ”¯æŒå­—å…¸å¼è®¿é—®"""
        return getattr(self, key)
    
    def __setitem__(self, key, value):
        """æ”¯æŒå­—å…¸å¼è®¾ç½®"""
        if isinstance(value, dict):
            setattr(self, key, ConfigDict(value))
        else:
            setattr(self, key, value)
    
    def items(self):
        """è¿”å› (key, value) å¯¹ï¼Œå…¼å®¹å­—å…¸æ¥å£"""
        result = []
        for k in dir(self):
            if not k.startswith('_') and not callable(getattr(self, k)):
                result.append((k, getattr(self, k)))
        return result
    
    def keys(self):
        """è¿”å›æ‰€æœ‰é”®ï¼Œå…¼å®¹å­—å…¸æ¥å£"""
        return [k for k in dir(self) if not k.startswith('_') and not callable(getattr(self, k))]
    
    def values(self):
        """è¿”å›æ‰€æœ‰å€¼ï¼Œå…¼å®¹å­—å…¸æ¥å£"""
        return [getattr(self, k) for k in dir(self) if not k.startswith('_') and not callable(getattr(self, k))]
    
    def __iter__(self):
        """æ”¯æŒè¿­ä»£"""
        return iter(self.keys())
    
    def to_dict(self):
        """è½¬æ¢ä¸ºæ™®é€šå­—å…¸"""
        result = {}
        for k in self.keys():
            v = getattr(self, k)
            if isinstance(v, ConfigDict):
                result[k] = v.to_dict()
            else:
                result[k] = v
        return result


def main(cfg):
    """cfg æ˜¯é…ç½®å¯¹è±¡ï¼ˆConfigDictï¼‰ï¼ŒåŒ…å« model, dataset, logging, training å››ä¸ªå­é…ç½®"""
    m_cfg = cfg.model
    d_cfg = cfg.dataset
    l_cfg = cfg.logging
    tr_cfg = cfg.training
    m_cfg = {k: getattr(m_cfg, k) for k in dir(m_cfg) if not k.startswith('_') and not callable(getattr(m_cfg, k))}
    d_cfg = {k: getattr(d_cfg, k) for k in dir(d_cfg) if not k.startswith('_') and not callable(getattr(d_cfg, k))}
    l_cfg = {k: getattr(l_cfg, k) for k in dir(l_cfg) if not k.startswith('_') and not callable(getattr(l_cfg, k))}
    tr_cfg = {k: getattr(tr_cfg, k) for k in dir(tr_cfg) if not k.startswith('_') and not callable(getattr(tr_cfg, k))}
    
    proj_root = Path(__file__).parent.resolve()

    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    ds_plugin = DeepSpeedPlugin(zero_stage=tr_cfg["zero_stage"])
    accelerator = Accelerator(
        kwargs_handlers=[ddp_kwargs],
        deepspeed_plugin=ds_plugin,
    )
    set_seed(tr_cfg["seed"] + accelerator.process_index)

    if accelerator.is_main_process:
        os.makedirs(l_cfg["out_dir"], exist_ok=True)
        os.makedirs(l_cfg["save_dir"], exist_ok=True)

    swanlab_run: Optional[Any] = None
    if l_cfg["use_swanlab"] and accelerator.is_main_process and swanlab is not None:
        mode = "cloud" if l_cfg["swanlab_online"] else "offline"
        Logger(f"SwanLab æ¨¡å¼ï¼š{mode}", accelerator)
        Logger(f"SwanLab è¿è¡Œä¸­...", accelerator)
        api_key = os.environ.get("SWANLAB_API_KEY", "GtiI1qjU5lco6MKKSrRmN")
        flat_config = {}
        for section in cfg.keys():
            values = getattr(cfg, section)
            if isinstance(values, ConfigDict):
                for key in values.keys():
                    value = getattr(values, key)
                    flat_config[f"{section}.{key}"] = value
            else:
                for key, value in values.items():
                    flat_config[f"{section}.{key}"] = value
        swanlab_run = swanlab.init(
            project=l_cfg["swanlab_project"],
            experiment_name=f"ExplicitLM-SFT-{tr_cfg['epochs']}e-{tr_cfg['batch_size']}b-{tr_cfg['learning_rate']}lr",
            config=flat_config,
            mode=mode,
            api_key=api_key
        )

    model, tokenizer = init_model(m_cfg)
    Logger("æ¨¡å‹æ¶æ„åˆå§‹åŒ–å®Œæˆ", accelerator)

    if d_cfg.get('pretrained_router_path'):
        router_path = proj_root / d_cfg['pretrained_router_path']
        try:
            load_pretrained_memory_gate(model, str(router_path), accelerator)
            Logger("âœ“ Router æƒé‡åŠ è½½å®Œæˆ", accelerator)
        except (FileNotFoundError, Exception) as e:
            Logger(f"è­¦å‘Š: åŠ è½½ Router æƒé‡å¤±è´¥: {e}ï¼Œå°†è·³è¿‡åŠ è½½", accelerator)

    if d_cfg.get('pretrained_fusion_path'):
        fusion_path = proj_root / d_cfg['pretrained_fusion_path']
        try:
            load_pretrained_fusion(model, str(fusion_path), accelerator)
        except (FileNotFoundError, Exception) as e:
            Logger(f"è­¦å‘Š: åŠ è½½ Fusion æƒé‡å¤±è´¥: {e}ï¼Œå°†è·³è¿‡åŠ è½½", accelerator)

    Logger("ğŸ”’ è®¾ç½®å‚æ•°å†»ç»“ç­–ç•¥ï¼ˆåªè®­ç»ƒ memory ç»„ä»¶ï¼‰", accelerator)
    frozen_params = 0
    trainable_params = 0
    memory_bank_params = 0
    keys_params = 0
    
    for name, param in model.named_parameters():
        is_keys = "keys" in name and "memory_gate" in name
        is_memory_bank = "memory_bank" in name
        is_memory_component = any(keyword in name for keyword in [
            "memory_gate", "gated_memory_fusion", "memory_norm"
        ]) and not is_keys
        
        if is_memory_bank:
            param.requires_grad = False
            memory_bank_params += param.numel()
            frozen_params += param.numel()
        elif is_keys:
            param.requires_grad = False
            keys_params += param.numel()
            frozen_params += param.numel()
        elif is_memory_component:
            param.requires_grad = True
            trainable_params += param.numel()
        else:
            param.requires_grad = False
            frozen_params += param.numel()
    
    Logger(f"å‚æ•°å†»ç»“: å†»ç»“ {frozen_params / 1e6:.3f}M, å¯è®­ç»ƒ {trainable_params / 1e6:.3f}M, Memory bank {memory_bank_params / 1e6:.3f}M, Keys {keys_params / 1e6:.3f}M", accelerator)

    optimizer_params = [p for p in model.parameters() if p.requires_grad]
    trainable_count = sum(p.numel() for p in optimizer_params)
    total_count = sum(p.numel() for p in model.parameters())
    Logger(f"ä¼˜åŒ–å™¨å‚æ•°: {trainable_count / 1e6:.3f}M / {total_count / 1e6:.3f}M ({trainable_count / total_count * 100:.2f}%)", accelerator)
    
    trainable_param_names = [name for name, param in model.named_parameters() if param.requires_grad]
    Logger(f"  - å¯è®­ç»ƒå‚æ•°æ¨¡å—: {len(trainable_param_names)} ä¸ª", accelerator)
    if len(trainable_param_names) <= 10:
        for name in trainable_param_names:
            Logger(f"    * {name}", accelerator)
    else:
        Logger(f"    * {trainable_param_names[0]} ... (å…±{len(trainable_param_names)}ä¸ª)", accelerator)

    optimizer = torch.optim.AdamW(
        optimizer_params,
        lr=tr_cfg["learning_rate"],
        betas=(0.9, 0.95),
        weight_decay=0.1,
    )

    model = model.cpu()
    Logger("æ¨¡å‹å‡†å¤‡å®Œæˆï¼ˆZeRO Stage 2ï¼‰", accelerator)

    train_loader = create_sft_dataloader(
        data_path=str(proj_root / d_cfg["sft_dataset_path"]),
        tokenizer=tokenizer,
        batch_size=tr_cfg["batch_size"],
        max_length=m_cfg["max_seq_len"],
        shuffle=True,
        num_workers=0,
        pin_memory=True,
    )
    val_loader = create_sft_eval_dataloader(
        eval_data_path=str(proj_root / d_cfg["sft_val_dataset_path"]),
        batch_size=1,
        max_samples=tr_cfg["eval_num_samples"],
    )

    steps_per_epoch = len(train_loader) // tr_cfg["accumulation_steps"]
    epochs = tr_cfg["epochs"]
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * steps_per_epoch * epochs),
        num_training_steps=steps_per_epoch * epochs,
    )

    model, optimizer, scheduler, train_loader, val_loader = accelerator.prepare(
        model, optimizer, scheduler, train_loader, val_loader
    )
    if isinstance(cfg, ConfigDict):
        cfg_dict = cfg.to_dict()
    else:
        cfg_dict = cfg
    Logger(f"é…ç½®ä¿¡æ¯: {json.dumps(cfg_dict, indent=2, default=str)}", accelerator)

    overall_start_time = time.time()

    for epoch in range(epochs):
        train_epoch_sft(
            epoch=epoch,
            accelerator=accelerator,
            model=model,
            train_loader=train_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            args=ConfigDict(cfg) if isinstance(cfg, dict) else cfg,  # ä¼ é€’é…ç½®å¯¹è±¡ä»¥æ”¯æŒç‚¹å·è®¿é—®
            overall_start_time=overall_start_time,
            swanlab_run=swanlab_run,
            tokenizer=tokenizer,
            eval_loader=val_loader,
        )
        gc.collect()
        torch.cuda.empty_cache()

    if accelerator.is_main_process and swanlab_run:
        # è·å– SwanLab å®éªŒ URL
        exp_url = str(swanlab_run.public.cloud.experiment_url) if l_cfg["swanlab_online"] else 'local-mode'
        with open('.swanlab_url', 'w') as f:
            f.write(exp_url)
        Logger(f"SwanLab URL å·²ä¿å­˜: {exp_url}", accelerator)

    if l_cfg["use_swanlab"] and accelerator.is_main_process:
        if swanlab_run is not None:
            swanlab_run.finish()
            Logger("SwanLab è¿è¡Œå·²ç»“æŸ", accelerator)

    Logger("è®°å¿†ç»„ä»¶è®­ç»ƒå®Œæˆï¼", accelerator)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    # æ”¯æŒ key=value æ ¼å¼çš„å‚æ•°
    overrides = {}
    
    def convert_value(value):
        """æ™ºèƒ½ç±»å‹è½¬æ¢"""
        # å¸ƒå°”å€¼
        if value.lower() == 'true':
            return True
        if value.lower() == 'false':
            return False
        
        # å°è¯•è½¬æ¢ä¸ºæ•´æ•°
        try:
            if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                return int(value)
        except:
            pass
        
        # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼ˆåŒ…æ‹¬ç§‘å­¦è®¡æ•°æ³•ï¼‰
        try:
            return float(value)
        except ValueError:
            pass
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        return value
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    for arg in sys.argv[1:]:
        if '=' in arg and not arg.startswith('--'):
            key, value = arg.split('=', 1)
            overrides[key] = convert_value(value)
    
    return overrides


if __name__ == "__main__":
    import sys
    cfg = get_default_config()
    overrides = parse_args()
    cfg = merge_config(cfg, overrides)
    
    if not cfg["model"].get("qwen3_model_path"):
        raise ValueError("å¿…é¡»æŒ‡å®š model.qwen3_model_path å‚æ•°ï¼ˆé€šè¿‡å‘½ä»¤è¡Œ: model.qwen3_model_path=/path/to/modelï¼‰")
    
    cfg_obj = ConfigDict(cfg)
    main(cfg_obj)
