"""
ExplicitLM å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨cacheçŸ¥è¯†åº“è¿›è¡Œæ¨¡å‹åˆå§‹åŒ–å’Œæ–‡æœ¬ç”Ÿæˆ

ä½¿ç”¨å‰è¯·é…ç½®ï¼š
1. QWEN3_MODEL_PATH: Qwen3-4B æ¨¡å‹è·¯å¾„
2. LLMLINGUA_MODEL_PATH: LLMLingua-2-BERT æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå¯ç”¨äº‹å®æå–ï¼‰
"""
import sys
import torch
sys.path.insert(0, '.')

from utils.model_initializer import init_model
from utils.dual_path_inference import DualPathInference
# from utils.fact_extractor import FactExtractor  # å¦‚æœå¯ç”¨äº‹å®æå–ï¼Œå–æ¶ˆæ³¨é‡Š


def main():
    print('='*60)
    print('ExplicitLM å¿«é€Ÿå¼€å§‹ç¤ºä¾‹')
    print('='*60)
    
    # ===== 0. é…ç½®æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰ =====
    QWEN3_MODEL_PATH = '/path/to/Qwen3-4b'  # è¯·æ›¿æ¢ä¸ºå®é™…çš„Qwen3æ¨¡å‹è·¯å¾„
    LLMLINGUA_MODEL_PATH = '/path/to/llmlingua-2-bert'  # è¯·æ›¿æ¢ä¸ºå®é™…çš„LLMLinguaæ¨¡å‹è·¯å¾„
    
    # ===== 1. æ¨¡å‹é…ç½® =====
    args = {
        'qwen3_model_path': QWEN3_MODEL_PATH,
        'knowledge_num': 1024 * 1024,  # 1048576 ä¸ªè®°å¿†æ¡ç›®
        'knowledge_length': 16,        # æ¯ä¸ªæ¡ç›®16ä¸ªtoken
        'knowledge_dim': 128,          # è®°å¿†åµŒå…¥ç»´åº¦
        'use_ema_update': False,
        'use_moe': False,
        'num_candidates': 8,
        'num_selected': 1,
        # ä½¿ç”¨é¢„å¤„ç†çš„cacheçŸ¥è¯†åº“
        'cache_path': 'data/cache/knowledge_cache.pt',
        'recompute_cache': False,
    }
    
    # ===== 2. åˆå§‹åŒ–æ¨¡å‹ =====
    print('\nğŸ“¦ åˆå§‹åŒ–æ¨¡å‹...')
    model, tokenizer = init_model(args, accelerator=None)
    model.eval()
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    print('âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ')
    print(f'  - Memory bankå½¢çŠ¶: {model.memory_bank.shape if hasattr(model, "memory_bank") else "N/A"}')
    
    # ===== 3. åˆå§‹åŒ–åŒè·¯æ¨ç† =====
    print('\nğŸ”§ åˆå§‹åŒ–åŒè·¯æ¨ç†åŒ…è£…å™¨...')
    
    # å¦‚æœéœ€è¦å¯ç”¨äº‹å®æå–ï¼Œéœ€è¦åˆå§‹åŒ–FactExtractor
    # from utils.fact_extractor import FactExtractor
    # fact_extractor = FactExtractor(
    #     model_path=LLMLINGUA_MODEL_PATH,
    #     compression_rate=0.4,
    # )
    
    dual_path = DualPathInference(
        model=model,
        tokenizer=tokenizer,
        # fact_extractor=fact_extractor,  # å¦‚æœå¯ç”¨äº‹å®æå–ï¼Œå–æ¶ˆæ³¨é‡Š
        enable_fact_extraction=False,  # å…ˆç¦ç”¨äº‹å®æå–ï¼Œåªæµ‹è¯•ç”Ÿæˆ
        fact_update_frequency=1,
        update_strategy='fifo',
    )
    print('âœ… åŒè·¯æ¨ç†åˆå§‹åŒ–å®Œæˆ')
    
    # ===== 4. æµ‹è¯•ç”Ÿæˆ =====
    test_cases = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è¯·ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€‚",
    ]
    
    print('\n' + '='*60)
    print('å¼€å§‹ç”Ÿæˆæµ‹è¯•')
    print('='*60)
    
    for i, test_text in enumerate(test_cases, 1):
        print(f'\n--- æµ‹è¯• {i}/{len(test_cases)} ---')
        print(f'è¾“å…¥: {test_text}')
        
        try:
            input_ids = tokenizer.encode(test_text, return_tensors='pt').to(model.device)
            
            with torch.no_grad():
                result = dual_path.generate(
                    input_ids,
                    input_text=test_text,
                    max_new_tokens=100,
                    temperature=0.7,
                    top_p=0.9,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id,
                    do_sample=True,
                )
            
            print(f'\nç”Ÿæˆç»“æœ:')
            print(result['generated_text'])
            
        except Exception as e:
            print(f'âŒ ç”Ÿæˆå¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
    
    print('\n' + '='*60)
    print('âœ… æµ‹è¯•å®Œæˆ')
    print('='*60)


if __name__ == '__main__':
    main()

