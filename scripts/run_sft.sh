#!/bin/bash
# è®°å¿†ç»„ä»¶è®­ç»ƒå¯åŠ¨è„šæœ¬ï¼ˆMemory Components Trainingï¼‰
# åªè®­ç»ƒ MemoryGateã€Fusionã€MemoryNormï¼ŒQwen3 backbone å®Œå…¨å†»ç»“

# ========== é…ç½®åŒºåŸŸ ==========
# è®¾ç½®GPUå¯è§è®¾å¤‡ï¼ˆå¹³è¡¡æ˜¾å­˜ï¼‰
export CUDA_VISIBLE_DEVICES=4,5

# è®¾ç½®PyTorchå†…å­˜åˆ†é…é…ç½®
export PYTORCH_ALLOC_CONF=expandable_segments:True

# è®¾ç½®SwanLab API Key
export SWANLAB_API_KEY=GtiI1qjU5lco6MKKSrRmN

# è¿›å…¥é¡¹ç›®ç›®å½•
cd /data2/zengzheni/lvchangwei/new_repo/ExplicitLM

# è®¾ç½®è¿›ç¨‹æ˜¾ç¤ºåç§°ï¼ˆåœ¨ nvidia-smi ä¸­æ˜¾ç¤ºçš„åç§°ï¼‰
export PYTHON_PROCESS_NAME="llama-env"

# ä¼˜å…ˆä½¿ç”¨ uv
if command -v uv &> /dev/null; then
    export PATH="$HOME/.local/bin:$PATH"
    echo "âœ… ä½¿ç”¨ uv è¿è¡Œè®­ç»ƒ"
    ACCELERATE_CMD="uv run accelerate launch --config_file accelerate_config.yaml"
elif [ -f ".venv/bin/activate" ]; then
    source .venv/bin/activate
    echo "âœ… å·²æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: $(which python)"
    ACCELERATE_CMD="accelerate launch --config_file accelerate_config.yaml"
elif [ -f "venv/bin/activate" ]; then
    source venv/bin/activate
    echo "âœ… å·²æ¿€æ´»venvè™šæ‹Ÿç¯å¢ƒ: $(which python)"
    ACCELERATE_CMD="accelerate launch --config_file accelerate_config.yaml"
else
    echo "âš ï¸  æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒï¼Œä½¿ç”¨ç³»ç»ŸPython: $(which python)"
    ACCELERATE_CMD="accelerate launch --config_file accelerate_config.yaml"
fi

# æ˜¾ç¤ºGPUä¿¡æ¯
echo "=========================================="
echo "ä½¿ç”¨GPU"
echo "=========================================="
nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv,noheader | grep -E "^[67],"

# ========== è®­ç»ƒé…ç½® ==========

QWEN3_MODEL_PATH="/data2/zengzheni/lvchangwei/new_repo/Qwen/models/Qwen3-4b" 
CACHE_PATH="data/cache/knowledge_cache.pt"
PRETRAINED_ROUTER_PATH=""  # Router é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
PRETRAINED_FUSION_PATH=""  # Fusion é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
SFT_DATASET_PATH="sft_data/omcq_trex_sft.jsonl"
SFT_VAL_DATASET_PATH="data/benchmarks/eval_data.json"

# è®­ç»ƒè¶…å‚æ•°ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
LEARNING_RATE=5e-5
BATCH_SIZE=1  # è¿›ä¸€æ­¥å‡å°æ‰¹æ¬¡å¤§å°ï¼š2 -> 1ï¼Œé¿å…OOMï¼ˆQwen3-4B hidden_size=2560ï¼Œå†…å­˜æ¶ˆè€—å¤§ï¼‰
ACCUMULATION_STEPS=128  # ç›¸åº”å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼š64 -> 128ï¼Œä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
EPOCHS=3
MAX_SEQ_LEN=256  # ä¿æŒ256ï¼Œè¿›ä¸€æ­¥å‡å°å¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ

echo ""
echo "=========================================="
echo "ğŸš€ å¯åŠ¨è®°å¿†ç»„ä»¶è®­ç»ƒï¼ˆMemory Components Trainingï¼‰"
echo "=========================================="
echo ""
echo "é…ç½®ï¼š"
echo "  - Qwen3 æ¨¡å‹: $QWEN3_MODEL_PATH"
echo "  - Cache è·¯å¾„: $CACHE_PATH"
echo "  - Router æƒé‡: $PRETRAINED_ROUTER_PATH"
echo "  - Fusion æƒé‡: $PRETRAINED_FUSION_PATH"
echo "  - è®­ç»ƒæ•°æ®: $SFT_DATASET_PATH"
echo "  - éªŒè¯æ•°æ®: $SFT_VAL_DATASET_PATH"
echo "  - å­¦ä¹ ç‡: $LEARNING_RATE"
echo "  - æ‰¹æ¬¡å¤§å°: $BATCH_SIZE"
echo "  - æ¢¯åº¦ç´¯ç§¯: $ACCUMULATION_STEPS"
    echo "  - è®­ç»ƒè½®æ•°: $EPOCHS"
    echo "  - æœ€å¤§åºåˆ—é•¿åº¦: $MAX_SEQ_LEN"
    echo "  - DeepSpeed Stage: 2 (ZeRO-2)"
    echo "  - Checkpoint ä¿å­˜ç›®å½•: out/"
    echo ""

# æ£€æŸ¥å¿…è¦æ–‡ä»¶
if [ ! -f "$SFT_DATASET_PATH" ]; then
    echo "âŒ é”™è¯¯: SFT è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: $SFT_DATASET_PATH"
    echo "è¯·å…ˆè¿è¡Œæ•°æ®è½¬æ¢è„šæœ¬:"
    echo "  python3 scripts/convert_omcq_to_sft.py --input sft_data/omcq_trex_data.json --output $SFT_DATASET_PATH"
    exit 1
fi

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
mkdir -p out

# å¯åŠ¨è®­ç»ƒ
echo "=========================================="
echo "å¼€å§‹è®­ç»ƒ..."
echo "=========================================="
echo ""

$ACCELERATE_CMD train_memory.py \
    model.qwen3_model_path="$QWEN3_MODEL_PATH" \
    model.cache_path="$CACHE_PATH" \
    model.recompute_cache=False \
    model.database_init_path="" \
    model.knowledge_num=1048576 \
    model.knowledge_dim=1536 \
    model.max_seq_len="$MAX_SEQ_LEN" \
    training.num_candidates=16 \
    dataset.sft_dataset_path="$SFT_DATASET_PATH" \
    dataset.pretrained_router_path="$PRETRAINED_ROUTER_PATH" \
    dataset.pretrained_fusion_path="$PRETRAINED_FUSION_PATH" \
    dataset.sft_val_dataset_path="$SFT_VAL_DATASET_PATH" \
    training.learning_rate="$LEARNING_RATE" \
    training.batch_size="$BATCH_SIZE" \
    training.accumulation_steps="$ACCUMULATION_STEPS" \
    training.epochs="$EPOCHS" \
    training.zero_stage=2 \
    model.keys_path="data/keys.pt" \
    model.gate_rank=128 \
    model.fusion_rank=128 \
    logging.out_dir="out" \
    logging.save_dir="out"

echo ""
echo "=========================================="
echo "âœ… è®­ç»ƒå®Œæˆ"
echo "=========================================="

