#!/usr/bin/env python3
"""
Hydra-Zen + DeepSpeed ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å…¥å£

åŠŸèƒ½ï¼š
- åŸºäºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒ
- ä½¿ç”¨å¯¹è¯æ ¼å¼æ•°æ®è®­ç»ƒ
- æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦
"""
import os
import time
import gc
from typing import Optional, Any

# è®¾ç½®è¿›ç¨‹åç§°ï¼ˆåœ¨ nvidia-smi ä¸­æ˜¾ç¤ºçš„åç§°ï¼‰
try:
    import setproctitle
    process_name = os.environ.get('PYTHON_PROCESS_NAME', 'llama-env')
    setproctitle.setproctitle(process_name)
except ImportError:
    # å¦‚æœæ²¡æœ‰ setproctitleï¼Œå°è¯•ä½¿ç”¨ prctl (Linux only)
    try:
        import prctl
        process_name = os.environ.get('PYTHON_PROCESS_NAME', 'llama-env')
        prctl.set_name(process_name.encode('utf-8'))
    except (ImportError, AttributeError):
        # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡ï¼ˆä¸å½±å“è®­ç»ƒï¼‰
        pass

from accelerate import Accelerator, DistributedDataParallelKwargs, DeepSpeedPlugin
from accelerate.utils import set_seed
import torch
import torch.optim as optim
from transformers import get_cosine_schedule_with_warmup
from hydra_zen import launch, instantiate
from config import store, _main_cfg_func          # è§¦å‘é…ç½®æ³¨å†Œ
from utils.logger import Logger
from utils.sft_datasets import create_sft_dataloader, create_sft_validation_dataloader
from utils.train_loop_sft import train_epoch_sft
from utils.model_initializer import init_model
from hydra.utils import get_original_cwd
from pathlib import Path

try:
    import swanlab
except ImportError:
    swanlab = None


def main(cfg):
    """cfg å°±æ˜¯ Hydra-Zen æ³¨å…¥çš„äº”å¤§é…ç½®èŠ‚ç‚¹"""
    # ------------------------------------------------------------------
    # ç¬¬ä¸€é˜¶æ®µï¼šè§£æ„é…ç½®
    # ------------------------------------------------------------------
    m_cfg = cfg.model
    d_cfg = cfg.dataset
    l_cfg = cfg.logging
    tr_cfg = cfg.training
    proj_root = Path(get_original_cwd())

    # é…ç½® DDP å‚æ•°ï¼šå…è®¸æœªä½¿ç”¨çš„å‚æ•°ï¼ˆç”¨äºéƒ¨åˆ†æ¨¡å‹ç»„ä»¶å¯èƒ½ä¸å‚ä¸æ¢¯åº¦è®¡ç®—çš„æƒ…å†µï¼‰
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    ds_plugin = DeepSpeedPlugin(zero_stage=tr_cfg.zero_stage)
    accelerator = Accelerator(
        kwargs_handlers=[ddp_kwargs],
        deepspeed_plugin=ds_plugin,
    )
    set_seed(tr_cfg.seed + accelerator.process_index)

    if accelerator.is_main_process:
        os.makedirs(l_cfg.out_dir, exist_ok=True)
        os.makedirs(l_cfg.save_dir, exist_ok=True)

    swanlab_run: Optional[Any] = None
    if l_cfg.use_swanlab and accelerator.is_main_process and swanlab is not None:
        mode = "cloud" if l_cfg.swanlab_online else "offline"
        Logger(f"SwanLab æ¨¡å¼ï¼š{mode}", accelerator)
        Logger(f"SwanLab è¿è¡Œä¸­...", accelerator)
        # ä»ç¯å¢ƒå˜é‡è·å–API keyï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é¡¹ç›®é»˜è®¤å€¼
        api_key = os.environ.get("SWANLAB_API_KEY", "GtiI1qjU5lco6MKKSrRmN")
        swanlab_run = swanlab.init(
            project=l_cfg.swanlab_project,
            experiment_name=f"ExplicitLM-SFT-{tr_cfg.epochs}e-{tr_cfg.batch_size}b-{tr_cfg.learning_rate}lr",
            config=instantiate(cfg),   # æŠŠå®Œæ•´é…ç½® flatten ä¸Šä¼ 
            mode=mode,
            api_key=api_key  # ä½¿ç”¨é¡¹ç›®ç‰¹å®šçš„API key
        )

    model, tokenizer = init_model(m_cfg)
    Logger("æ¨¡å‹æ¶æ„åˆå§‹åŒ–å®Œæˆ", accelerator)

    if hasattr(d_cfg, 'pretrained_sft_model_path') and d_cfg.pretrained_sft_model_path:
        Logger(f"å¼€å§‹åŠ è½½é¢„è®­ç»ƒæƒé‡: {d_cfg.pretrained_sft_model_path}", accelerator)

        try:
            # åŠ è½½é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
            pretrained_path = proj_root / d_cfg.pretrained_sft_model_path
            checkpoint = torch.load(pretrained_path, map_location='cpu')

            # è®°å½•æ£€æŸ¥ç‚¹ä¸­åŒ…å«çš„memoryç›¸å…³å‚æ•°
            memory_keys_in_checkpoint = [k for k in checkpoint.keys() if 'memory' in k.lower()]
            if memory_keys_in_checkpoint:
                Logger(f"æ£€æŸ¥ç‚¹ä¸­åŒ…å« {len(memory_keys_in_checkpoint)} ä¸ªmemoryç›¸å…³å‚æ•°", accelerator)
                Logger(f"ç¤ºä¾‹: {memory_keys_in_checkpoint[:3]}{'...' if len(memory_keys_in_checkpoint) > 3 else ''}", accelerator)

            # åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨ strict=False å…è®¸éƒ¨åˆ†åŠ è½½ï¼‰
            missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=False)

            if missing_keys:
                Logger(f"è­¦å‘Š: ä»¥ä¸‹å‚æ•°æœªåœ¨æ£€æŸ¥ç‚¹ä¸­æ‰¾åˆ°: {missing_keys[:5]}{'...' if len(missing_keys) > 5 else ''}", accelerator)
            if unexpected_keys:
                Logger(f"è­¦å‘Š: ä»¥ä¸‹å‚æ•°åœ¨æ£€æŸ¥ç‚¹ä¸­ä½†æ¨¡å‹ä¸éœ€è¦: {unexpected_keys[:5]}{'...' if len(unexpected_keys) > 5 else ''}", accelerator)

            Logger(f"âœ“ é¢„è®­ç»ƒæƒé‡åŠ è½½å®Œæˆï¼ˆåŒ…æ‹¬çŸ¥è¯†åº“å‚æ•°ï¼‰", accelerator)

            # é‡æ–°ç¡®è®¤å‚æ•°å†»ç»“çŠ¶æ€ï¼ˆç¡®ä¿åŠ è½½æƒé‡åå†»ç»“çŠ¶æ€ä¸å˜ï¼‰
            Logger("ğŸ”’ é‡æ–°ç¡®è®¤å‚æ•°å†»ç»“çŠ¶æ€...", accelerator)
            frozen_params = 0
            trainable_params = 0
            memory_bank_params = 0
            
            for name, param in model.named_parameters():
                is_memory_component = any(keyword in name for keyword in [
                    "memory_gate", "gated_memory_fusion", "memory_norm"
                ])
                is_memory_bank = "memory_bank" in name
                
                if is_memory_bank:
                    param.requires_grad = False
                    memory_bank_params += param.numel()
                    frozen_params += param.numel()
                elif is_memory_component:
                    param.requires_grad = True
                    trainable_params += param.numel()
                else:
                    param.requires_grad = False
                    frozen_params += param.numel()
            
            Logger(f"å‚æ•°å†»ç»“: å†»ç»“ {frozen_params / 1e6:.3f}M, å¯è®­ç»ƒ {trainable_params / 1e6:.3f}M, Memory bank {memory_bank_params / 1e6:.3f}M", accelerator)

        except FileNotFoundError:
            Logger(f"é”™è¯¯: é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}", accelerator)
            Logger("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒï¼ˆä¸æ¨èï¼‰", accelerator)
        except Exception as e:
            Logger(f"é”™è¯¯: åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", accelerator)
            Logger("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒï¼ˆä¸æ¨èï¼‰", accelerator)
    else:
        Logger("è­¦å‘Š: æœªæŒ‡å®š pretrained_sft_model_path å‚æ•°", accelerator)
        Logger("SFT è®­ç»ƒé€šå¸¸éœ€è¦åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå½“å‰å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼ˆä¸æ¨èï¼‰", accelerator)

    # ç¡®ä¿ä¼˜åŒ–å™¨åªåŒ…å«å¯è®­ç»ƒçš„å‚æ•°ï¼ˆmemory_gate å’Œ fusionï¼‰
    optimizer_params = [p for p in model.parameters() if p.requires_grad]
    trainable_count = sum(p.numel() for p in optimizer_params)
    total_count = sum(p.numel() for p in model.parameters())
    
    Logger(f"ä¼˜åŒ–å™¨å‚æ•°: {trainable_count / 1e6:.3f}M / {total_count / 1e6:.3f}M ({trainable_count / total_count * 100:.2f}%)", accelerator)
    
    # åˆ—å‡ºå¯è®­ç»ƒçš„å‚æ•°åç§°ï¼ˆç”¨äºéªŒè¯ï¼‰
    trainable_param_names = [name for name, param in model.named_parameters() if param.requires_grad]
    Logger(f"  - å¯è®­ç»ƒå‚æ•°æ¨¡å—: {len(trainable_param_names)} ä¸ª", accelerator)
    if len(trainable_param_names) <= 10:
        for name in trainable_param_names:
            Logger(f"    * {name}", accelerator)
    else:
        Logger(f"    * {trainable_param_names[0]} ... (å…±{len(trainable_param_names)}ä¸ª)", accelerator)

    optimizer = torch.optim.AdamW(
        optimizer_params,
        lr=tr_cfg.learning_rate,
        betas=(0.9, 0.95),
        weight_decay=0.1,
    )

    # ------------------------------------------------------------------
    # ç¬¬ä¸ƒé˜¶æ®µï¼šSFT æ•°æ®åŠ è½½å™¨
    # ------------------------------------------------------------------
    train_loader = create_sft_dataloader(
        data_path=str(proj_root / d_cfg.sft_dataset_path),
        tokenizer=tokenizer,
        batch_size=tr_cfg.batch_size,
        max_length=m_cfg.max_seq_len,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )
    val_loader = create_sft_validation_dataloader(
        val_data_path=str(proj_root / d_cfg.sft_val_dataset_path),
        tokenizer=tokenizer,
        batch_size=tr_cfg.batch_size,
        max_length=m_cfg.max_seq_len,
        num_samples=200,
    )

    # ------------------------------------------------------------------
    # ç¬¬å…«é˜¶æ®µï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨
    # ------------------------------------------------------------------
    steps_per_epoch = len(train_loader) // tr_cfg.accumulation_steps
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * steps_per_epoch * tr_cfg.epochs),
        num_training_steps=steps_per_epoch * tr_cfg.epochs,
    )

    # ------------------------------------------------------------------
    # ç¬¬ä¹é˜¶æ®µï¼šAccelerator.prepare
    # ------------------------------------------------------------------
    model, optimizer, scheduler, train_loader, val_loader = accelerator.prepare(
        model, optimizer, scheduler, train_loader, val_loader
    )
    Logger(instantiate(cfg), accelerator)

    # ------------------------------------------------------------------
    # ç¬¬åé˜¶æ®µï¼šSFT è®­ç»ƒå¾ªç¯
    # ------------------------------------------------------------------
    overall_start_time = time.time()

    for epoch in range(tr_cfg.epochs):
        train_epoch_sft(
            epoch=epoch,
            accelerator=accelerator,
            model=model,
            train_loader=train_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            args=instantiate(cfg),   # å…¼å®¹è®­ç»ƒå¾ªç¯çš„ args ç”¨æ³•
            overall_start_time=overall_start_time,
            swanlab_run=swanlab_run,
            tokenizer=tokenizer,
            eval_loader=val_loader,
        )
        gc.collect()
        torch.cuda.empty_cache()

    # ------------------------------------------------------------------
    # ç¬¬åä¸€é˜¶æ®µï¼šæ”¶å°¾
    # ------------------------------------------------------------------
    if accelerator.is_main_process and swanlab_run:
        # è·å– SwanLab å®éªŒ URL
        if l_cfg.swanlab_online:                       # äº‘ç‰ˆ
            exp_url = str(swanlab_run.public.cloud.experiment_url)
        else:                                          # æœ¬åœ°ç‰ˆ
            exp_url = 'local-mode'

        # å†™å…¥ä¸´æ—¶æ–‡ä»¶ä¾›è„šæœ¬è¯»å–ï¼ˆå’Œ pretrain ä¿æŒä¸€è‡´çš„æ–‡ä»¶åï¼‰
        with open('.swanlab_url', 'w') as f:
            f.write(exp_url)

        Logger(f"SwanLab URL å·²ä¿å­˜: {exp_url}", accelerator)

    # ------------------------------------------------------------------
    # ç¬¬åäºŒé˜¶æ®µï¼šå…³é—­ SwanLab
    # ------------------------------------------------------------------
    if l_cfg.use_swanlab and accelerator.is_main_process:
        if swanlab_run is not None:
            swanlab_run.finish()
            Logger("SwanLab è¿è¡Œå·²ç»“æŸ", accelerator)

    Logger("SFT è®­ç»ƒå®Œæˆï¼", accelerator)


if __name__ == "__main__":
    import sys
    # æå–å‘½ä»¤è¡Œå‚æ•°ç”¨äºé…ç½®è¦†ç›–ï¼Œè·³è¿‡è„šæœ¬åç§°
    # æ¥å— key=value æ ¼å¼çš„å‚æ•°
    overrides = []
    for arg in sys.argv[1:]:
        if '=' in arg and not arg.startswith('--'):
            overrides.append(arg)

    # ä½¿ç”¨å‘½ä»¤è¡Œè¦†ç›–å¯åŠ¨
    launch(_main_cfg_func, main, overrides=overrides)
