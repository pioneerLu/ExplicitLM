"""
ExplicitLM: åŸºäºæ˜¾å¼è®°å¿†å¢å¼ºçš„è¯­è¨€æ¨¡å‹

è¯¥æ¨¡å‹å®ç°äº†ä¸€ä¸ªåˆ›æ–°çš„Transformeræ¶æ„ï¼Œä½¿ç”¨æ˜¾å¼è®°å¿†åº“æ›¿ä»£ä¼ ç»Ÿçš„FFNå±‚ï¼š
- å…±äº«è®°å¿†åº“å­˜å‚¨å¯å­¦ä¹ çš„tokenåºåˆ—
- EMAæ›´æ–°æœºåˆ¶å®ç°ç±»ä¼¼VQ-VAEçš„codebookæ›´æ–°
- æ”¯æŒè®°å¿†å†»ç»“ç­–ç•¥ä»¥ä¿æŠ¤é‡è¦çŸ¥è¯†
- å¤šæŸå¤±ä¼˜åŒ–ç³»ç»Ÿï¼ˆç›¸ä¼¼åº¦æŸå¤±+å¤šæ ·æ€§æŸå¤±ï¼‰
- æ— KVç¼“å­˜çš„æµå¼ç”Ÿæˆèƒ½åŠ›
"""

from typing import Dict, List, Optional, Union, Iterator

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import PreTrainedModel
from transformers.modeling_outputs import CausalLMOutputWithPast

from models.configs.LMConfig import LMConfig
from models.core.ExplicitLMBlock import ExplicitLMBlock
from models.layers.RMSNorm import RMSNorm
from models.layers.Attention import precompute_pos_cis


class MiniMindLM(PreTrainedModel):
    """
    åŸºäºæ˜¾å¼è®°å¿†å¢å¼ºçš„å› æœè¯­è¨€æ¨¡å‹

    è¯¥æ¨¡å‹é€šè¿‡å…±äº«è®°å¿†åº“å¢å¼ºTransformeræ¶æ„ï¼Œè®°å¿†åº“å­˜å‚¨tokenåºåˆ—å¹¶é€šè¿‡
    EMAæœºåˆ¶åŠ¨æ€æ›´æ–°ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„çŸ¥è¯†å­˜å‚¨å’Œæ£€ç´¢æœºåˆ¶ã€‚
    """

    config_class = LMConfig

    def __init__(self, params: Optional[LMConfig] = None) -> None:
        """
        åˆå§‹åŒ–ExplicitLMæ¨¡å‹

        Args:
            params: æ¨¡å‹é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°è®¾ç½®
        """
        self.params = params
        super().__init__(self.params)

        # ===== åŸºç¡€æ¶æ„ç»„ä»¶ =====
        self.vocab_size: int = params.vocab_size
        self.n_layers: int = params.n_layers

        # TokenåµŒå…¥å±‚å’Œè¾“å‡ºå±‚ï¼ˆæƒé‡å…±äº«ï¼‰
        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)
        self.dropout = nn.Dropout(params.dropout)
        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)
        self.tok_embeddings.weight = self.output.weight  # æƒé‡ç»‘å®š

        # Transformerå±‚å †å 
        self.layers = nn.ModuleList([ExplicitLMBlock(l, params) for l in range(self.n_layers)])

        # æœ€ç»ˆå½’ä¸€åŒ–å±‚
        self.norm = RMSNorm(params.dim, eps=params.norm_eps)

        # ä½ç½®ç¼–ç é¢„è®¡ç®—ï¼ˆRoPEï¼‰
        self.register_buffer(
            "pos_cis",
            precompute_pos_cis(dim=params.dim // params.n_heads, theta=params.rope_theta),
            persistent=False
        )

        # ===== å…±äº«è®°å¿†åº“åˆå§‹åŒ– =====
        # å­˜å‚¨token_idåºåˆ—è€Œéç‰¹å¾å‘é‡ï¼ˆç±»ä¼¼VQ-VAEçš„codebookè®¾è®¡ï¼‰
        # å½¢çŠ¶: [knowledge_num, knowledge_length]
        if params.use_ema_update:
            # EMAæ¨¡å¼ï¼šç¦ç”¨æ¢¯åº¦æ›´æ–°ï¼Œé€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°è®°å¿†
            self.memory_bank = nn.Parameter(
                torch.randint(0, params.vocab_size, (params.knowledge_num, params.knowledge_length)),
                requires_grad=False
            )
        else:
            # æ¢¯åº¦æ¨¡å¼ï¼šä¼ ç»Ÿç«¯åˆ°ç«¯æ¢¯åº¦æ›´æ–°
            self.memory_bank = nn.Parameter(
                torch.randint(0, params.vocab_size, (params.knowledge_num, params.knowledge_length)),
                requires_grad=True
            )

        # ===== EMAæ›´æ–°ç›¸å…³ç¼“å†²åŒº =====
        if params.use_ema_update:
            # è®°å½•æ¯ä¸ªè®°å¿†æ¡ç›®çš„æ›´æ–°æ¬¡æ•°
            self.register_buffer(
                'ema_update_count',
                torch.zeros(params.knowledge_num),
                persistent=False
            )
            # EMAå…¨å±€æ­¥æ•°è®¡æ•°å™¨
            self.register_buffer(
                'ema_step_counter',
                torch.zeros(1, dtype=torch.long),
                persistent=False
            )

        # è®°å½•ä¸Šä¸€æ­¥çš„è®°å¿†åº“çŠ¶æ€ï¼Œç”¨äºè®¡ç®—æ›´æ–°ç»Ÿè®¡
        self.register_buffer(
            'prev_memory_bank',
            torch.zeros_like(self.memory_bank),
            persistent=False
        )

        # ===== è®°å¿†å†»ç»“æœºåˆ¶ =====
        # æ ‡è®°å“ªäº›è®°å¿†æ¡ç›®è¢«å†»ç»“ä»¥ä¿æŠ¤é‡è¦çŸ¥è¯†
        if params.freeze_ratio > 0.0:
            freeze_num = int(params.knowledge_num * params.freeze_ratio)
            freeze_mask = torch.zeros(params.knowledge_num, dtype=torch.bool)
            freeze_mask[:freeze_num] = True  # å†»ç»“å‰Nä¸ªæ¡ç›®
            self.register_buffer('freeze_mask', freeze_mask, persistent=False)
            print(
                f"ğŸ”¥ Memory bank freezing enabled: {freeze_num}/{params.knowledge_num} "
                f"entries ({params.freeze_ratio*100:.1f}%) frozen",
                flush=True
            )
        else:
            self.register_buffer(
                'freeze_mask',
                torch.zeros(params.knowledge_num, dtype=torch.bool),
                persistent=False
            )
            print("ğŸ”¥ Memory bank freezing disabled: all entries can be updated", flush=True)

        # è¾“å‡ºå®¹å™¨
        self.OUT = CausalLMOutputWithPast()
    
    def get_memory_update_stats(self) -> Dict[str, float]:
        """
        è®¡ç®—è®°å¿†åº“æ›´æ–°ç»Ÿè®¡ä¿¡æ¯

        è¯¥æ–¹æ³•é€šè¿‡æ¯”è¾ƒå½“å‰è®°å¿†åº“å’Œä¸Šä¸€æ­¥çš„è®°å¿†åº“çŠ¶æ€ï¼Œè®¡ç®—å„ç§æ›´æ–°æŒ‡æ ‡ï¼Œ
        åŒ…æ‹¬L2è·ç¦»å˜åŒ–ã€ä½™å¼¦ç›¸ä¼¼åº¦å’Œæ›´æ–°ç‡ç­‰ã€‚

        Returns:
            update_stats: åŒ…å«ä»¥ä¸‹é”®çš„ç»Ÿè®¡å­—å…¸ï¼š
                - memory_avg_l2_change: å¹³å‡L2è·ç¦»å˜åŒ–
                - memory_max_l2_change: æœ€å¤§L2è·ç¦»å˜åŒ–
                - memory_cosine_similarity: æ•´ä½“ä½™å¼¦ç›¸ä¼¼åº¦
                - memory_update_rate: æ›´æ–°ç‡ï¼ˆå˜åŒ–æ˜¾è‘—çš„è®°å¿†æ¯”ä¾‹ï¼‰
                - memory_updated_count: æ›´æ–°çš„è®°å¿†æ¡ç›®æ•°é‡
        """
        with torch.no_grad():
            if hasattr(self, 'prev_memory_bank') and self.prev_memory_bank.numel() > 0:
                # è®¡ç®—L2è·ç¦»å˜åŒ–
                l2_distance = torch.norm(self.memory_bank - self.prev_memory_bank, p=2, dim=-1)
                avg_l2_distance = l2_distance.mean().item()
                max_l2_distance = l2_distance.max().item()
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                cos_sim = F.cosine_similarity(
                    self.memory_bank.view(-1), 
                    self.prev_memory_bank.view(-1), 
                    dim=0
                ).item()
                
                # è®¡ç®—æ›´æ–°ç‡ï¼ˆå‘ç”Ÿæ˜¾è‘—å˜åŒ–çš„è®°å¿†æ¡ç›®æ¯”ä¾‹ï¼‰
                threshold = 0.01  # æ›´æ–°é˜ˆå€¼
                updated_memories = (l2_distance > threshold).sum().item()
                update_rate = updated_memories / self.memory_bank.size(0)
                
                update_stats = {
                    'memory_avg_l2_change': avg_l2_distance,
                    'memory_max_l2_change': max_l2_distance,
                    'memory_cosine_similarity': cos_sim,
                    'memory_update_rate': update_rate,
                    'memory_updated_count': updated_memories
                }
            else:
                # ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶çš„é»˜è®¤å€¼
                update_stats = {
                    'memory_avg_l2_change': 0.0,
                    'memory_max_l2_change': 0.0,
                    'memory_cosine_similarity': 1.0,
                    'memory_update_rate': 0.0,
                    'memory_updated_count': 0
                }
            
            # æ›´æ–°prev_memory_bank
            self.prev_memory_bank.copy_(self.memory_bank)
            
            return update_stats

    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        **args
    ) -> CausalLMOutputWithPast:
        """
        å‰å‘ä¼ æ’­ï¼ˆä¸æ”¯æŒKVç¼“å­˜ï¼‰

        è¯¥æ–¹æ³•å®ç°å®Œæ•´çš„å‰å‘ä¼ æ’­æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
        1. TokenåµŒå…¥å’Œä½ç½®ç¼–ç 
        2. é€šè¿‡æ‰€æœ‰Transformerå±‚ï¼ˆå«è®°å¿†å¢å¼ºæœºåˆ¶ï¼‰
        3. æ”¶é›†å„å±‚çš„æŸå¤±å’Œç»Ÿè®¡ä¿¡æ¯
        4. æœ€ç»ˆå½’ä¸€åŒ–å’Œè¾“å‡ºæŠ•å½±

        Args:
            input_ids: å½¢çŠ¶ä¸º[batch_size, seq_len]çš„è¾“å…¥token IDå¼ é‡
            **args: å…¶ä»–å‚æ•°ï¼Œæ”¯æŒï¼š
                - start_pos: èµ·å§‹ä½ç½®ï¼ˆé»˜è®¤0ï¼‰
                - collect_ema_stats: æ˜¯å¦æ”¶é›†EMAç»Ÿè®¡ä¿¡æ¯

        Returns:
            CausalLMOutputWithPast: åŒ…å«ä»¥ä¸‹å­—æ®µçš„è¾“å‡ºå¯¹è±¡ï¼š
                - logits: è¯­è¨€æ¨¡å‹çš„é¢„æµ‹logits
                - last_hidden_state: æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
                - aux_loss: è¾…åŠ©æŸå¤±å­—å…¸ï¼ˆç›¸ä¼¼åº¦æŸå¤±+å¤šæ ·æ€§æŸå¤±ï¼‰
                - layer_stats: å„å±‚çš„ç»Ÿè®¡ä¿¡æ¯
                - ema_stats: EMAæ›´æ–°ç»Ÿè®¡ï¼ˆå¦‚æœcollect_ema_stats=Trueï¼‰
                - cosine_stats: ä½™å¼¦ç›¸ä¼¼åº¦ç»Ÿè®¡
                - past_key_values: Noneï¼ˆä¸æ”¯æŒKVç¼“å­˜ï¼‰
        """
        # æå–å‚æ•°
        start_pos: int = args.get('start_pos', 0)
        collect_ema_stats: bool = args.get('collect_ema_stats', self.params.use_ema_update and self.training)

        # ===== ç¬¬ä¸€é˜¶æ®µï¼šåµŒå…¥å’Œä½ç½®ç¼–ç  =====
        h = self.dropout(self.tok_embeddings(input_ids))  # [batch_size, seq_len, dim]
        pos_cis = self.pos_cis[start_pos:start_pos + input_ids.size(1)]

        # ===== ç¬¬äºŒé˜¶æ®µï¼šTransformerå±‚å¤„ç† =====
        # æ”¶é›†æ‰€æœ‰å±‚çš„æŸå¤±å’Œç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒæŸå¤±ç³»ç»Ÿï¼šç›¸ä¼¼åº¦+å¤šæ ·æ€§ï¼‰
        total_similarity_loss = torch.tensor(0.0, device=h.device)
        total_diversity_loss = torch.tensor(0.0, device=h.device)
        all_layer_stats: Dict[str, float] = {}
        all_ema_stats: Dict[str, Dict] = {}
        all_cosine_stats: Dict[str, Union[torch.Tensor, float]] = {}

        for layer_idx, layer in enumerate(self.layers):
            if collect_ema_stats:
                # è®­ç»ƒæ¨¡å¼ï¼šæ”¶é›†EMAæ›´æ–°æ‰€éœ€çš„ç»Ÿè®¡ä¿¡æ¯
                h, similarity_loss, diversity_loss, layer_stats, ema_stats, cosine_stats = layer(
                    h, pos_cis, self.memory_bank, self.tok_embeddings, collect_ema_stats=True
                )
                all_ema_stats[f'layer_{layer_idx}'] = ema_stats
            else:
                # æ¨ç†æ¨¡å¼ï¼šä¸æ”¶é›†EMAç»Ÿè®¡
                h, similarity_loss, diversity_loss, layer_stats, cosine_stats = layer(
                    h, pos_cis, self.memory_bank, self.tok_embeddings, collect_ema_stats=False
                )

            # ç´¯åŠ åŒæŸå¤±
            total_similarity_loss += similarity_loss
            total_diversity_loss += diversity_loss

            # æ”¶é›†å„å±‚ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ·»åŠ å±‚çº§å‰ç¼€ï¼‰
            for key, value in layer_stats.items():
                all_layer_stats[f'layer_{layer_idx}_{key}'] = value

            # æ”¶é›†ä½™å¼¦ç›¸ä¼¼åº¦ç»Ÿè®¡
            for key, value in cosine_stats.items():
                all_cosine_stats[f'layer_{layer_idx}_{key}'] = value

        # ===== ç¬¬ä¸‰é˜¶æ®µï¼šè¾“å‡ºæŠ•å½± =====
        logits = self.output(self.norm(h))  # [batch_size, seq_len, vocab_size]

        # ===== ç¬¬å››é˜¶æ®µï¼šæ„å»ºè¾“å‡º =====
        # è®¡ç®—å¹³å‡è¾…åŠ©æŸå¤±
        n_layers = len(self.layers)
        aux_loss = {
            'similarity_loss': total_similarity_loss / n_layers,
            'diversity_loss': total_diversity_loss / n_layers,
        }

        # å¡«å……è¾“å‡ºå®¹å™¨
        self.OUT.__setitem__('last_hidden_state', h)
        self.OUT.__setitem__('logits', logits)
        self.OUT.__setitem__('aux_loss', aux_loss)
        self.OUT.__setitem__('layer_stats', all_layer_stats)
        self.OUT.__setitem__('ema_stats', all_ema_stats if collect_ema_stats else None)
        self.OUT.__setitem__('cosine_stats', all_cosine_stats)
        self.OUT.__setitem__('past_key_values', None)  # ä¸æ”¯æŒKVç¼“å­˜

        return self.OUT

    @torch.inference_mode()
    def generate(
        self,
        input_ids: torch.Tensor,
        eos_token_id: int = 2,
        max_new_tokens: int = 1024,
        temperature: float = 0.75,
        top_p: float = 0.90,
        stream: bool = False,
        rp: float = 1.,
        pad_token_id: int = 0,
        num_return_sequences: int = 1,
        **args
    ) -> torch.Tensor:
        """
        æ–‡æœ¬ç”Ÿæˆï¼ˆä¸æ”¯æŒKVç¼“å­˜ï¼‰

        è¯¥æ–¹æ³•æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼Œä½¿ç”¨top-pé‡‡æ ·å’Œé‡å¤æƒ©ç½šæœºåˆ¶ã€‚
        ç”±äºä¸æ”¯æŒKVç¼“å­˜ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—ã€‚

        Args:
            input_ids: è¾“å…¥tokenåºåˆ—ï¼Œå½¢çŠ¶[batch_size, seq_len]
            eos_token_id: ç»“æŸç¬¦token IDï¼ˆé»˜è®¤2ï¼‰
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°é‡ï¼ˆé»˜è®¤1024ï¼‰
            temperature: é‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºå¤šæ ·æ€§ï¼ˆé»˜è®¤0.75ï¼‰
            top_p: nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆé»˜è®¤0.90ï¼‰
            stream: æ˜¯å¦ä½¿ç”¨æµå¼ç”Ÿæˆï¼ˆé»˜è®¤Falseï¼‰
            rp: é‡å¤æƒ©ç½šç³»æ•°ï¼ˆé»˜è®¤1.0ï¼Œæ— æƒ©ç½šï¼‰
            pad_token_id: å¡«å……token IDï¼ˆé»˜è®¤0ï¼‰
            num_return_sequences: æ¯ä¸ªè¾“å…¥ç”Ÿæˆçš„åºåˆ—æ•°é‡ï¼ˆé»˜è®¤1ï¼‰
            **args: å…¶ä»–ä¼ é€’ç»™forwardçš„å‚æ•°

        Returns:
            ç”Ÿæˆçš„tokenåºåˆ—ï¼Œå½¢çŠ¶[batch_size * num_return_sequences, total_seq_len]
        """
        # æµå¼ç”Ÿæˆ
        if stream:
            return self._stream(input_ids, eos_token_id, max_new_tokens, temperature, top_p, rp, **args)

        # ç›´æ¥ç”Ÿæˆ
        generated = []
        for i in range(input_ids.size(0)):
            non_pad = input_ids[i][input_ids[i] != pad_token_id].unsqueeze(0)
            for _ in range(num_return_sequences):
                out = self._stream(non_pad, eos_token_id, max_new_tokens, temperature, top_p, rp, **args)
                tokens_list = [tokens[:, -1:] for tokens in out]
                gen = torch.cat(tokens_list, dim=-1) if tokens_list else non_pad
                full_sequence = torch.cat([non_pad, gen], dim=-1)
                generated.append(full_sequence)

        max_length = max(seq.size(1) for seq in generated)
        generated = [
            torch.cat(
                [seq, torch.full((1, max_length - seq.size(1)), pad_token_id, dtype=seq.dtype, device=seq.device)],
                dim=-1)
            for seq in generated
        ]
        output = torch.cat(generated, dim=0)
        res = output.view(input_ids.size(0) * num_return_sequences, -1)
        return res

    def _stream(
        self,
        input_ids: torch.Tensor,
        eos_token_id: int,
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        rp: float,
        **args
    ) -> Iterator[torch.Tensor]:
        """
        æµå¼ç”Ÿæˆï¼ˆä¸æ”¯æŒKVç¼“å­˜ï¼‰

        è¯¥æ–¹æ³•å®ç°æµå¼tokenç”Ÿæˆï¼Œæ¯æ¬¡è¿­ä»£è¿”å›æ–°ç”Ÿæˆçš„tokenåºåˆ—ã€‚
        ç”±äºä¸æ”¯æŒKVç¼“å­˜ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„è¡¨ç¤ºã€‚

        Args:
            input_ids: è¾“å…¥tokenåºåˆ—ï¼Œå½¢çŠ¶[1, seq_len]
            eos_token_id: ç»“æŸç¬¦token ID
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleusé‡‡æ ·é˜ˆå€¼
            rp: é‡å¤æƒ©ç½šç³»æ•°
            **args: å…¶ä»–ä¼ é€’ç»™forwardçš„å‚æ•°

        Yields:
            æ¯æ¬¡ç”Ÿæˆåæ–°å¢çš„tokenåºåˆ—ï¼Œå½¢çŠ¶[1, generated_len]
        """
        start = input_ids.shape[1]
        while input_ids.shape[1] < start + max_new_tokens:
	            # æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—ï¼ˆå› ä¸ºæ²¡æœ‰KV cacheï¼‰
            out = self(input_ids, **args)
            logits = out.logits[:, -1, :]
            
            # é‡å¤æƒ©ç½š
            logits[:, list(set(input_ids.tolist()[0]))] /= rp
            logits /= (temperature + 1e-9)
            
            # Top-pé‡‡æ ·
            if top_p is not None and top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
                sorted_probs = F.softmax(sorted_logits, dim=-1)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                sorted_indices_to_remove[:, 0] = False
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                logits[indices_to_remove] = -float('Inf')
                
            input_ids_next = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)
            input_ids = torch.cat((input_ids, input_ids_next), dim=1)
            yield input_ids[:, start:]
            if input_ids_next.item() == eos_token_id:
                break
    
    def apply_ema_update(self, ema_stats: Dict[str, Dict]) -> Dict[str, Union[bool, int, float]]:
        """
        åº”ç”¨åŸºäºEMAçš„è®°å¿†åº“æ›´æ–°ï¼ˆæ‰¹é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        è¯¥æ–¹æ³•å®ç°äº†ç±»ä¼¼VQ-VAEçš„EMAæ›´æ–°æœºåˆ¶ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤æ›´æ–°è®°å¿†åº“ï¼š
        1. æ”¶é›†æ‰€æœ‰å±‚é€‰ä¸­çš„è®°å¿†ç´¢å¼•å’Œå¯¹åº”çš„æŸ¥è¯¢ç‰¹å¾
        2. å¯¹æ¯ä¸ªè¢«é€‰ä¸­çš„è®°å¿†è®¡ç®—å¹³å‡æŸ¥è¯¢ç‰¹å¾
        3. ä½¿ç”¨EMAå…¬å¼æ›´æ–°è®°å¿†çš„ç‰¹å¾è¡¨ç¤º
        4. å°†æ›´æ–°åçš„ç‰¹å¾é‡æ–°ç¼–ç ä¸ºtokenåºåˆ—
        5. åº”ç”¨å†»ç»“maskä¿æŠ¤é‡è¦è®°å¿†

        Args:
            ema_stats: ä»forwardä¼ æ’­æ”¶é›†çš„EMAç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
                {
                    'layer_0': {
                        'memory_indices': torch.Tensor,  # [batch, seq_len, num_selected]
                        'h_for_memory': torch.Tensor,    # [batch, seq_len, dim]
                        ...
                    },
                    'layer_1': {...},
                    ...
                }

        Returns:
            update_stats: æ›´æ–°ç»Ÿè®¡å­—å…¸ï¼ŒåŒ…å«ï¼š
                - ema_update_applied: æ˜¯å¦æˆåŠŸåº”ç”¨æ›´æ–°
                - ema_step: å½“å‰EMAæ­¥æ•°
                - total_selections: æ€»é€‰æ‹©æ¬¡æ•°
                - total_layers: å‚ä¸æ›´æ–°çš„å±‚æ•°
                - updated_memories: å®é™…æ›´æ–°çš„è®°å¿†æ¡ç›®æ•°
                - update_ratio: æ›´æ–°æ¯”ä¾‹
                - frozen_memories: å†»ç»“çš„è®°å¿†æ•°é‡
                - frozen_ratio: å†»ç»“æ¯”ä¾‹
                - ema_decay: EMAè¡°å‡ç³»æ•°
                - selected_memory_coverage: è®°å¿†è¦†ç›–ç‡
        """
        if not self.params.use_ema_update:
            return {}

        # ===== ç¬¬ä¸€é˜¶æ®µï¼šæ›´æ–°é¢‘ç‡æ£€æŸ¥ =====
        self.ema_step_counter += 1

        if self.ema_step_counter % self.params.ema_update_freq != 0:
            return {'ema_update_applied': False, 'reason': 'frequency_check_failed'}

        with torch.no_grad():
            device = self.memory_bank.device
            knowledge_num, knowledge_length = self.memory_bank.shape
            dim = self.params.dim

            # ===== ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æ”¶é›† =====
            # æ‰¹é‡æ”¶é›†æ‰€æœ‰å±‚çš„é€‰æ‹©ä¿¡æ¯ï¼Œé¿å…é¢‘ç¹çš„å­—å…¸æ“ä½œ
            all_indices: List[torch.Tensor] = []
            all_features: List[torch.Tensor] = []
            total_selections = 0
            total_layers = 0
            
            # éå†æ‰€æœ‰å±‚çš„EMAç»Ÿè®¡ä¿¡æ¯
            for layer_ema_stats in ema_stats.values():
                if layer_ema_stats is None:
                    continue

                total_layers += 1
                memory_indices = layer_ema_stats['memory_indices']  # [batch, seq_len, num_selected]
                h_for_memory = layer_ema_stats['h_for_memory']      # [batch, seq_len, dim]

                bsz, seq_len, num_selected = memory_indices.shape
                total_selections += bsz * seq_len * num_selected

                # å±•å¹³ç´¢å¼•ç”¨äºæ‰¹é‡å¤„ç†
                flat_indices = memory_indices.view(-1)  # [batch * seq_len * num_selected]

                # ä¸ºæ¯ä¸ªé€‰æ‹©ä½ç½®å¤åˆ¶å¯¹åº”çš„æŸ¥è¯¢ç‰¹å¾
                h_expanded = h_for_memory.unsqueeze(2).expand(-1, -1, num_selected, -1)  # [batch, seq_len, num_selected, dim]
                flat_h = h_expanded.reshape(-1, dim)  # [batch * seq_len * num_selected, dim]

                all_indices.append(flat_indices)
                all_features.append(flat_h)

            if not all_indices:
                return {'ema_update_applied': False, 'reason': 'no_ema_stats'}

            # ===== ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®åˆå¹¶å’Œèšåˆ =====
            # åˆå¹¶æ‰€æœ‰å±‚çš„æ•°æ®
            all_indices = torch.cat(all_indices, dim=0)  # [total_selections]
            all_features = torch.cat(all_features, dim=0)  # [total_selections, dim]

            # è®¡ç®—æ¯ä¸ªå”¯ä¸€è®°å¿†ç´¢å¼•çš„å¹³å‡æŸ¥è¯¢ç‰¹å¾ï¼ˆæ‰¹é‡åŒ–é¿å…å¾ªç¯ï¼‰
            unique_indices, inverse_indices = torch.unique(all_indices, return_inverse=True)

            # ä½¿ç”¨scatter_addè¿›è¡Œæ‰¹é‡èšåˆ
            aggregated_features = torch.zeros(unique_indices.size(0), dim, device=device, dtype=all_features.dtype)
            count_per_memory = torch.zeros(unique_indices.size(0), device=device, dtype=all_features.dtype)

            aggregated_features.scatter_add_(0, inverse_indices.unsqueeze(1).expand(-1, dim), all_features)
            count_per_memory.scatter_add_(0, inverse_indices, torch.ones_like(inverse_indices, dtype=all_features.dtype))

            # è®¡ç®—æ¯ä¸ªè®°å¿†çš„å¹³å‡æŸ¥è¯¢ç‰¹å¾
            avg_features = aggregated_features / count_per_memory.unsqueeze(1)  # [unique_count, dim]

            # ===== ç¬¬å››é˜¶æ®µï¼šåˆ†æ‰¹EMAæ›´æ–° =====
            # åˆ†æ‰¹å¤„ç†ä»¥æ§åˆ¶æ˜¾å­˜ä½¿ç”¨
            batch_size = 4096  # æ¯æ‰¹å¤„ç†4096ä¸ªè®°å¿†æ¡ç›®
            updated_memories = 0

            for i in range(0, unique_indices.size(0), batch_size):
                end_i = min(i + batch_size, unique_indices.size(0))
                batch_indices = unique_indices[i:end_i]
                batch_avg_features = avg_features[i:end_i]

                # è§£ç å½“å‰è®°å¿†çš„tokenåºåˆ—ä¸ºç‰¹å¾å‘é‡
                current_tokens_batch = self.memory_bank[batch_indices]  # [batch_size, knowledge_length]
                current_embeddings_batch = self.tok_embeddings(current_tokens_batch.view(-1)).view(
                    batch_indices.size(0), knowledge_length, dim
                )  # [batch_size, knowledge_length, dim]

                # å‡†å¤‡EMAæ›´æ–°çš„ç‰¹å¾
                old_features_batch = current_embeddings_batch.view(batch_indices.size(0), -1)  # [batch_size, knowledge_length * dim]
                expanded_new_features = batch_avg_features.repeat(1, knowledge_length)  # [batch_size, knowledge_length * dim]

                # EMAæ›´æ–°å…¬å¼ï¼šnew = Î³ * old + (1-Î³) * new_avg
                updated_features_batch = (
                    self.params.ema_decay * old_features_batch +
                    (1 - self.params.ema_decay) * expanded_new_features
                )

                # å°†æ›´æ–°åçš„ç‰¹å¾é‡æ–°ç¼–ç ä¸ºtoken ID
                updated_reshaped = updated_features_batch.view(-1, dim)  # [batch_size * knowledge_length, dim]
                logits_batch = self.output(updated_reshaped)  # [batch_size * knowledge_length, vocab_size]
                new_token_ids_batch = torch.argmax(logits_batch, dim=-1).view(batch_indices.size(0), knowledge_length)

                # ===== ç¬¬äº”é˜¶æ®µï¼šåº”ç”¨å†»ç»“mask =====
                # åªæ›´æ–°æœªè¢«å†»ç»“çš„è®°å¿†æ¡ç›®
                unfrozen_mask_batch = ~self.freeze_mask[batch_indices]  # [batch_size]

                if unfrozen_mask_batch.any():
                    unfrozen_indices = batch_indices[unfrozen_mask_batch]
                    unfrozen_tokens = new_token_ids_batch[unfrozen_mask_batch]
                    self.memory_bank[unfrozen_indices] = unfrozen_tokens
                    updated_memories += unfrozen_indices.size(0)
            
            # ===== ç¬¬å…­é˜¶æ®µï¼šç»Ÿè®¡ä¿¡æ¯æ”¶é›† =====
            update_ratio = updated_memories / knowledge_num

            # è®¡ç®—å†»ç»“ç›¸å…³ç»Ÿè®¡
            frozen_count = self.freeze_mask.sum().item()
            total_memories = knowledge_num

            # æ„å»ºæ›´æ–°ç»Ÿè®¡å­—å…¸
            update_stats = {
                'ema_update_applied': True,
                'ema_step': self.ema_step_counter.item(),
                'total_selections': total_selections,
                'total_layers': total_layers,
                'updated_memories': updated_memories,
                'update_ratio': update_ratio,
                'frozen_memories': frozen_count,
                'frozen_ratio': frozen_count / total_memories,
                'ema_decay': self.params.ema_decay,
                'selected_memory_coverage': updated_memories / knowledge_num,
            }

            return update_stats