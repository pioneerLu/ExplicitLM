"""
ExplicitLM: åŸºäºŽæ˜¾å¼è®°å¿†å¢žå¼ºçš„è¯­è¨€æ¨¡åž‹ï¼ˆåŸºäºŽQwen3æž¶æž„ï¼‰

åœ¨Qwen3åŸºç¡€ä¸Šæ·»åŠ æ˜¾å¼è®°å¿†åº“æœºåˆ¶ï¼Œè®­ç»ƒæ—¶å›ºå®šï¼ŒæŽ¨ç†æ—¶é€šè¿‡LLMLinguaæ›´æ–°ã€‚
é‡‡ç”¨Shortcutæœºåˆ¶ç¡®ä¿backboneç‹¬ç«‹å·¥ä½œã€‚
"""

from typing import Dict, List, Optional, Union, Iterator

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import PreTrainedModel
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers.models.qwen3.modeling_qwen3 import (
    Qwen3Config,
    Qwen3Model,
    Qwen3RotaryEmbedding,
    Qwen3RMSNorm,
    Cache,
    DynamicCache,
    create_causal_mask,
    create_sliding_window_causal_mask,
)
from transformers.utils import TransformersKwargs
from typing import Unpack

from models.core.Qwen3ExplicitLMBlock import Qwen3ExplicitLMBlock


class ExplicitLM(PreTrainedModel):
    """
    åŸºäºŽæ˜¾å¼è®°å¿†å¢žå¼ºçš„å› æžœè¯­è¨€æ¨¡åž‹ï¼ˆåŸºäºŽQwen3æž¶æž„ï¼‰
    
    è®°å¿†åº“åœ¨è®­ç»ƒæ—¶å›ºå®šï¼ŒæŽ¨ç†æ—¶é€šè¿‡LLMLinguaåŠ¨æ€æ›´æ–°ã€‚
    """

    config_class = Qwen3Config

    def __init__(self, qwen3_config: Qwen3Config, memory_cfg: dict) -> None:
        """
        Args:
            qwen3_config: Qwen3Configé…ç½®å¯¹è±¡
            memory_cfg: è®°å¿†åº“ç›¸å…³é…ç½®å­—å…¸
        """
        super().__init__(qwen3_config)
        self.config = qwen3_config
        self.memory_cfg = memory_cfg
        
        self.vocab_size = qwen3_config.vocab_size
        self.hidden_size = qwen3_config.hidden_size
        
        self.embed_tokens = nn.Embedding(
            qwen3_config.vocab_size, 
            qwen3_config.hidden_size, 
            qwen3_config.pad_token_id
        )
        
        self.rotary_emb = Qwen3RotaryEmbedding(config=qwen3_config)
        
        self.layers = nn.ModuleList([
            Qwen3ExplicitLMBlock(qwen3_config, layer_idx, memory_cfg)
            for layer_idx in range(qwen3_config.num_hidden_layers)
        ])
        
        self.norm = Qwen3RMSNorm(qwen3_config.hidden_size, eps=qwen3_config.rms_norm_eps)
        self.lm_head = nn.Linear(qwen3_config.hidden_size, qwen3_config.vocab_size, bias=False)
        
        # TokenåµŒå…¥ç”¨äºŽè®°å¿†åº“è§£ç 
        if qwen3_config.tie_word_embeddings:
            self.tok_embeddings = self.embed_tokens
        else:
            self.tok_embeddings = nn.Embedding(
                qwen3_config.vocab_size,
                qwen3_config.hidden_size,
                qwen3_config.pad_token_id
            )

        use_moe = memory_cfg.get("use_moe", False)
        if not use_moe:
            knowledge_num = memory_cfg["knowledge_num"]
            knowledge_length = memory_cfg["knowledge_length"]
            
            # memory_bankå­˜å‚¨token IDsï¼Œè®­ç»ƒæ—¶å›ºå®šï¼ŒæŽ¨ç†æ—¶é€šè¿‡LLMLinguaæ›´æ–°
            self.register_buffer(
                "memory_bank",
                torch.randint(
                    0, qwen3_config.vocab_size, (knowledge_num, knowledge_length)
                ),
                persistent=True,  # æŒä¹…åŒ–ï¼Œç¡®ä¿ä¿å­˜å’ŒåŠ è½½æ—¶åŒ…å«
            )

            # è®°å½•ä¸Šä¸€æ­¥çš„è®°å¿†åº“çŠ¶æ€ï¼ˆç”¨äºŽç»Ÿè®¡ï¼‰
            self.register_buffer(
                "prev_memory_bank",
                torch.zeros_like(self.memory_bank),
                persistent=False,
            )

            freeze_ratio = memory_cfg.get("freeze_ratio", 0.0)
            if freeze_ratio > 0.0:
                freeze_num = int(knowledge_num * freeze_ratio)
                freeze_mask = torch.zeros(knowledge_num, dtype=torch.bool)
                freeze_mask[:freeze_num] = True
                self.register_buffer("freeze_mask", freeze_mask, persistent=False)
                print(
                    f"ðŸ”¥ Memory bank freezing enabled: {freeze_num}/{knowledge_num} "
                    f"entries ({freeze_ratio*100:.1f}%) frozen",
                    flush=True,
                )
            else:
                self.register_buffer(
                    "freeze_mask",
                    torch.zeros(knowledge_num, dtype=torch.bool),
                    persistent=False,
                )
                print("ðŸ”¥ Memory bank freezing disabled: all entries can be updated", flush=True)
        else:
            # MOE æ¨¡å¼ï¼šä¸éœ€è¦ memory_bank
            self.memory_bank = None
            print("ðŸ”¥ MOE mode enabled: using Mixture of Experts instead of memory bank", flush=True)

        self.OUT = CausalLMOutputWithPast()
        self.post_init()

    def get_memory_update_stats(self) -> Dict[str, float]:
        # MOE æ¨¡å¼ä¸‹ä¸è¿”å›žè®°å¿†åº“ç»Ÿè®¡ä¿¡æ¯
        if self.memory_cfg.get("use_moe", False) or self.memory_bank is None:
            return {
                "memory_avg_l2_change": 0.0,
                "memory_max_l2_change": 0.0,
                "memory_cosine_similarity": 1.0,
                "memory_update_rate": 0.0,
                "memory_updated_count": 0,
            }
        with torch.no_grad():
            if hasattr(self, "prev_memory_bank") and self.prev_memory_bank.numel() > 0:
                # memory_bankå­˜å‚¨çš„æ˜¯token IDsï¼ˆint64ï¼‰ï¼Œä¸èƒ½ç›´æŽ¥è®¡ç®—L2è·ç¦»
                # æ”¹ä¸ºè®¡ç®—tokenå·®å¼‚ï¼šæœ‰å¤šå°‘ä¸ªtokenä¸åŒ
                token_diff = (self.memory_bank != self.prev_memory_bank).sum(dim=-1).float()  # [knowledge_num]
                avg_token_diff = token_diff.mean().item()
                max_token_diff = token_diff.max().item()
                
                # è®¡ç®—å®Œå…¨ç›¸åŒçš„è®°å¿†æ¡ç›®æ¯”ä¾‹ä½œä¸ºç›¸ä¼¼åº¦
                identical_memories = (token_diff == 0).sum().item()
                similarity = identical_memories / self.memory_bank.size(0)
                
                # æ›´æ–°é˜ˆå€¼ï¼šå¦‚æžœè‡³å°‘æœ‰ä¸€ä¸ªtokenä¸åŒï¼Œåˆ™è®¤ä¸ºæ›´æ–°äº†
                threshold = 0.5  # è‡³å°‘æœ‰ä¸€ä¸ªtokenä¸åŒ
                updated_memories = (token_diff >= threshold).sum().item()
                update_rate = updated_memories / self.memory_bank.size(0)
                update_stats = {
                    "memory_avg_l2_change": avg_token_diff,  # å®žé™…ä¸Šæ˜¯å¹³å‡tokenå·®å¼‚æ•°
                    "memory_max_l2_change": max_token_diff,  # å®žé™…ä¸Šæ˜¯æœ€å¤§tokenå·®å¼‚æ•°
                    "memory_cosine_similarity": similarity,  # å®Œå…¨ç›¸åŒçš„è®°å¿†æ¡ç›®æ¯”ä¾‹
                    "memory_update_rate": update_rate,
                    "memory_updated_count": updated_memories,
                }
            else:
                update_stats = {
                    "memory_avg_l2_change": 0.0,
                    "memory_max_l2_change": 0.0,
                    "memory_cosine_similarity": 1.0,
                    "memory_update_rate": 0.0,
                    "memory_updated_count": 0,
                }
            self.prev_memory_bank.copy_(self.memory_bank)
            return update_stats

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        step: Optional[int] = None,  # å…¼å®¹æ—§æŽ¥å£ï¼ˆå·²åºŸå¼ƒï¼‰
        **kwargs: Unpack[TransformersKwargs],
    ) -> CausalLMOutputWithPast:
        """
        Args:
            input_ids: è¾“å…¥token IDs
            attention_mask: æ³¨æ„åŠ›æŽ©ç 
            position_ids: ä½ç½®IDs
            past_key_values: KVç¼“å­˜
            inputs_embeds: è¾“å…¥åµŒå…¥ï¼ˆå¯é€‰ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            cache_position: ç¼“å­˜ä½ç½®
        """
        
        if input_ids is None and inputs_embeds is None:
            raise ValueError("You must specify either input_ids or inputs_embeds")
        
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        
        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)
        
        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )
        
        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)
        
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            if hasattr(self.config, "layer_types") and "sliding_attention" in self.config.layer_types:
                causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)
        
        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids)
        
        total_similarity_loss = torch.tensor(0.0, device=hidden_states.device)
        total_diversity_loss = torch.tensor(0.0, device=hidden_states.device)
        total_moe_aux_loss = torch.tensor(0.0, device=hidden_states.device)
        all_layer_stats: Dict[str, float] = {}
        all_cosine_stats: Dict[str, Union[torch.Tensor, float]] = {}
        
        use_moe = self.memory_cfg.get("use_moe", False)
        
        for layer_idx, layer in enumerate(self.layers):
            layer_attention_mask = causal_mask_mapping.get(
                getattr(layer.qwen3_decoder, "attention_type", "full_attention"),
                causal_mask_mapping["full_attention"]
            )
            
            if use_moe:
                hidden_states, sim_loss, div_loss, layer_stats, cosine_stats = layer(
                    hidden_states=hidden_states,
                    attention_mask=layer_attention_mask,
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                    **kwargs,
                )
                if "moe_aux_loss" in layer_stats:
                    moe_aux = layer_stats["moe_aux_loss"]
                    if isinstance(moe_aux, (int, float)):
                        total_moe_aux_loss += torch.tensor(moe_aux, device=hidden_states.device)
                    elif isinstance(moe_aux, torch.Tensor):
                        total_moe_aux_loss += moe_aux
            else:
                hidden_states, sim_loss, div_loss, layer_stats, cosine_stats = layer(
                    hidden_states=hidden_states,
                    attention_mask=layer_attention_mask,
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                    memory_bank=self.memory_bank,
                    tok_embeddings=self.tok_embeddings,
                    **kwargs,
                )
                total_similarity_loss += sim_loss
                total_diversity_loss += div_loss
            
            for k, v in layer_stats.items():
                all_layer_stats[f"layer_{layer_idx}_{k}"] = v
            for k, v in cosine_stats.items():
                all_cosine_stats[f"layer_{layer_idx}_{k}"] = v
        
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        n_layers = len(self.layers)
        if use_moe:
            aux_loss = {
                "moe_aux_loss": total_moe_aux_loss / n_layers,
            }
        else:
            aux_loss = {
                "similarity_loss": total_similarity_loss / n_layers,
                "diversity_loss": total_diversity_loss / n_layers,
            }
        
        self.OUT.__setitem__("last_hidden_state", hidden_states)
        self.OUT.__setitem__("logits", logits)
        self.OUT.__setitem__("aux_loss", aux_loss)
        self.OUT.__setitem__("layer_stats", all_layer_stats)
        self.OUT.__setitem__("cosine_stats", all_cosine_stats)
        self.OUT.__setitem__("past_key_values", past_key_values if use_cache else None)
        return self.OUT

    @torch.inference_mode()
    def generate(
        self,
        input_ids: torch.Tensor,
        eos_token_id: int = 2,
        max_new_tokens: int = 1024,
        temperature: float = 0.75,
        top_p: float = 0.90,
        stream: bool = False,
        rp: float = 1.0,
        pad_token_id: int = 0,
        num_return_sequences: int = 1,
        **args,
    ) -> torch.Tensor:
        if stream:
            return self._stream(
                input_ids, eos_token_id, max_new_tokens, temperature, top_p, rp, **args
            )
        generated = []
        for i in range(input_ids.size(0)):
            non_pad = input_ids[i][input_ids[i] != pad_token_id].unsqueeze(0)
            for _ in range(num_return_sequences):
                out = self._stream(
                    non_pad, eos_token_id, max_new_tokens, temperature, top_p, rp, **args
                )
                tokens_list = [tokens[:, -1:] for tokens in out]
                gen = torch.cat(tokens_list, dim=-1) if tokens_list else non_pad
                full_sequence = torch.cat([non_pad, gen], dim=-1)
                generated.append(full_sequence)
        max_len = max(seq.size(1) for seq in generated)
        generated = [
            torch.cat(
                [
                    seq,
                    torch.full(
                        (1, max_len - seq.size(1)),
                        pad_token_id,
                        dtype=seq.dtype,
                        device=seq.device,
                    ),
                ],
                dim=-1,
            )
            for seq in generated
        ]
        output = torch.cat(generated, dim=0)
        return output.view(input_ids.size(0) * num_return_sequences, -1)

    def _stream(
        self,
        input_ids: torch.Tensor,
        eos_token_id: int,
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        rp: float,
        **args,
    ) -> Iterator[torch.Tensor]:
        start = input_ids.shape[1]
        while input_ids.shape[1] < start + max_new_tokens:
            out = self(input_ids, **args)
            logits = out.logits[:, -1, :]
            logits[:, list(set(input_ids.tolist()[0]))] /= rp
            logits /= temperature + 1e-9
            if top_p is not None and top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
                sorted_probs = F.softmax(sorted_logits, dim=-1)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                sorted_indices_to_remove[:, 0] = False
                indices_to_remove = sorted_indices_to_remove.scatter(
                    1, sorted_indices, sorted_indices_to_remove
                )
                logits[indices_to_remove] = -float("Inf")
            next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)
            input_ids = torch.cat((input_ids, next_tok), dim=1)
            yield input_ids[:, start:]
            if next_tok.item() == eos_token_id:
                break

