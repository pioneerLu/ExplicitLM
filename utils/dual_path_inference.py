"""
åŒè·¯æ¨ç†åŒ…è£…å™¨ï¼šä¸»è·¯ï¼ˆQwen3ExplicitLMï¼‰å’Œè¾…è·¯ï¼ˆllmlinguaäº‹å®æå–ï¼‰
"""
import torch
from typing import Optional, Dict, List, Union, Tuple
from pathlib import Path

from utils.fact_extractor import FactExtractor
from utils.memory_bank_updater import MemoryBankUpdater
from utils.logger import Logger


class DualPathInference:
    """åŒè·¯æ¨ç†åŒ…è£…å™¨"""
    
    def __init__(
        self,
        model,
        tokenizer,
        fact_extractor: Optional[FactExtractor] = None,
        memory_bank_updater: Optional[MemoryBankUpdater] = None,
        enable_fact_extraction: bool = True,
        fact_update_frequency: int = 1,  # æ¯Næ¬¡æ¨ç†æ›´æ–°ä¸€æ¬¡äº‹å®
        update_strategy: str = "fifo",
        compression_rate: float = 0.4,
        llmlingua_model_path: Optional[str] = None,  # LLMLinguaæ¨¡å‹è·¯å¾„
    ):
        """
        åˆå§‹åŒ–åŒè·¯æ¨ç†åŒ…è£…å™¨
        
        Args:
            model: ExplicitLM æ¨¡å‹å®ä¾‹
            tokenizer: tokenizer å®ä¾‹
            fact_extractor: äº‹å®æå–å™¨ï¼ˆå¦‚æœä¸ºNoneä¸”å¯ç”¨äº‹å®æå–ï¼Œå°†è‡ªåŠ¨åˆ›å»ºï¼‰
            memory_bank_updater: è®°å¿†åº“æ›´æ–°å™¨ï¼ˆå¦‚æœä¸ºNoneï¼Œå°†è‡ªåŠ¨åˆ›å»ºï¼‰
            enable_fact_extraction: æ˜¯å¦å¯ç”¨äº‹å®æå–
            fact_update_frequency: äº‹å®æ›´æ–°é¢‘ç‡ï¼ˆæ¯Næ¬¡æ¨ç†æ›´æ–°ä¸€æ¬¡ï¼‰
            update_strategy: è®°å¿†åº“æ›´æ–°ç­–ç•¥
            compression_rate: æ–‡æœ¬å‹ç¼©ç‡
            llmlingua_model_path: LLMLinguaæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœfact_extractorä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
        """
        self.model = model
        self.tokenizer = tokenizer
        self.enable_fact_extraction = enable_fact_extraction
        self.fact_update_frequency = fact_update_frequency
        self.inference_counter = 0
        
        # åˆå§‹åŒ–äº‹å®æå–å™¨
        if fact_extractor is None and enable_fact_extraction:
            if llmlingua_model_path is None:
                raise ValueError(
                    "å¯ç”¨äº‹å®æå–æ—¶ï¼Œå¿…é¡»æä¾› llmlingua_model_path å‚æ•°æˆ–ä¼ å…¥ fact_extractor å®ä¾‹"
                )
            self.fact_extractor = FactExtractor(
                model_path=llmlingua_model_path,
                compression_rate=compression_rate
            )
        else:
            self.fact_extractor = fact_extractor
        
        # åˆå§‹åŒ–è®°å¿†åº“æ›´æ–°å™¨
        if memory_bank_updater is None and enable_fact_extraction:
            self.memory_bank_updater = MemoryBankUpdater(
                model=model,
                tokenizer=tokenizer,
                fact_extractor=self.fact_extractor,
                update_strategy=update_strategy,
            )
        else:
            self.memory_bank_updater = memory_bank_updater
        
        Logger(f"âœ… åŒè·¯æ¨ç†åŒ…è£…å™¨åˆå§‹åŒ–å®Œæˆ")
        Logger(f"  - äº‹å®æå–: {'å¯ç”¨' if enable_fact_extraction else 'ç¦ç”¨'}ï¼ˆä»…åœ¨æ¨ç†æ—¶ï¼‰")
        Logger(f"  - æ›´æ–°é¢‘ç‡: æ¯ {fact_update_frequency} æ¬¡æ¨ç†")
        Logger(f"  - æ›´æ–°ç­–ç•¥: {update_strategy}")
        Logger(f"  - è®­ç»ƒæ¨¡å¼: åªæ›´æ–°çŸ¥è¯†èåˆéƒ¨åˆ†ï¼ˆé€šè¿‡æ¢¯åº¦ï¼‰ï¼Œä¸æ›´æ–°çŸ¥è¯†åº“")
        Logger(f"  - æ¨ç†æ¨¡å¼: æå–äº‹å®å¹¶æ›´æ–°çŸ¥è¯†åº“")
    
    def generate(
        self,
        input_ids: torch.Tensor,
        input_text: Optional[str] = None,
        **generation_kwargs,
    ) -> Dict[str, any]:
        """
        åŒè·¯æ¨ç†ç”Ÿæˆ
        
        æ¨ç†æ—¶è¿›è¡Œï¼š
        1. ä¸»è·¯ï¼šæ­£å¸¸ç”Ÿæˆæ–‡æœ¬
        2. è¾…è·¯ï¼šæå–æµ“ç¼©äº‹å®å¹¶æ›´æ–°çŸ¥è¯†åº“
        
        Args:
            input_ids: è¾“å…¥token IDs
            input_text: è¾“å…¥æ–‡æœ¬ï¼ˆç”¨äºäº‹å®æå–ï¼Œå¦‚æœä¸ºNoneå°†è‡ªåŠ¨è§£ç ï¼‰
            **generation_kwargs: ä¼ é€’ç»™model.generateçš„å‚æ•°
        
        Returns:
            {
                'generated_ids': torch.Tensor,  # ç”Ÿæˆçš„token IDs
                'generated_text': str,  # ç”Ÿæˆçš„æ–‡æœ¬
                'fact_extraction': Dict,  # äº‹å®æå–ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
                'memory_update': Dict,  # è®°å¿†åº“æ›´æ–°ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            }
        """
        # ===== ä¸»è·¯ï¼šæ­£å¸¸æ¨ç† =====
        with torch.no_grad():
            generated_ids = self.model.generate(input_ids, **generation_kwargs)
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆåªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œä¸åŒ…æ‹¬è¾“å…¥ï¼‰
        input_length = input_ids.shape[1]
        if generated_ids.dim() > 1:
            # åªå–æ–°ç”Ÿæˆçš„token
            new_tokens = generated_ids[0, input_length:]
            generated_text = self.tokenizer.decode(
                new_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
        else:
            new_tokens = generated_ids[input_length:]
            generated_text = self.tokenizer.decode(
                new_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
        
        result = {
            'generated_ids': generated_ids,
            'generated_text': generated_text,
        }
        
        # ===== è¾…è·¯ï¼šäº‹å®æå–å’Œè®°å¿†åº“æ›´æ–°ï¼ˆä»…åœ¨æ¨ç†æ—¶ï¼‰ =====
        if self.enable_fact_extraction:
            self.inference_counter += 1
            
            # è·å–è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if input_text is None:
                input_text = self.tokenizer.decode(
                    input_ids[0] if input_ids.dim() > 1 else input_ids,
                    skip_special_tokens=True
                )
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ ¹æ®é¢‘ç‡ï¼‰
            should_update = (self.inference_counter % self.fact_update_frequency == 0)
            
            if should_update:
                # æå–äº‹å®
                fact_result = self.fact_extractor.extract_facts(
                    input_text,
                    return_annotations=False,
                )
                result['fact_extraction'] = fact_result
                
                # æ›´æ–°è®°å¿†åº“
                if fact_result['compressed_text']:
                    update_result = self.memory_bank_updater.update_from_text(
                        input_text,
                        compression_rate=self.fact_extractor.compression_rate,
                    )
                    result['memory_update'] = update_result
                    Logger(
                        f"ğŸ“ è®°å¿†åº“å·²æ›´æ–°: {update_result.get('updated_count', 0)} æ¡äº‹å®",
                        accelerator=None
                    )
            else:
                result['fact_extraction'] = {"skipped": True, "reason": "frequency_check"}
                result['memory_update'] = {"skipped": True, "reason": "frequency_check"}
        
        return result
    
    def forward(
        self,
        input_ids: torch.Tensor,
        input_text: Optional[str] = None,
        **forward_kwargs,
    ) -> Dict[str, any]:
        """
        åŒè·¯å‰å‘ä¼ æ’­ï¼ˆç”¨äºè®­ç»ƒæˆ–è¯„ä¼°ï¼‰
        
        æ³¨æ„ï¼šè®­ç»ƒæ—¶åªè¿›è¡Œæ­£å¸¸çš„å‰å‘ä¼ æ’­ï¼Œä¸è¿›è¡Œäº‹å®æå–å’ŒçŸ¥è¯†åº“æ›´æ–°ã€‚
        çŸ¥è¯†èåˆéƒ¨åˆ†ä¼šé€šè¿‡æ¢¯åº¦è‡ªåŠ¨æ›´æ–°ã€‚
        
        Args:
            input_ids: è¾“å…¥token IDs
            input_text: è¾“å…¥æ–‡æœ¬ï¼ˆè®­ç»ƒæ—¶ä¸éœ€è¦ï¼Œä¼šè¢«å¿½ç•¥ï¼‰
            **forward_kwargs: ä¼ é€’ç»™model.forwardçš„å‚æ•°
        
        Returns:
            {
                'model_output': ModelOutput,  # æ¨¡å‹è¾“å‡º
            }
        """
        # ===== ä¸»è·¯ï¼šæ­£å¸¸å‰å‘ä¼ æ’­ =====
        # è®­ç»ƒæ—¶åªè¿›è¡Œæ­£å¸¸çš„å‰å‘ä¼ æ’­ï¼ŒçŸ¥è¯†èåˆéƒ¨åˆ†é€šè¿‡æ¢¯åº¦æ›´æ–°
        model_output = self.model(input_ids, **forward_kwargs)
        
        result = {
            'model_output': model_output,
        }
        
        # ===== è®­ç»ƒæ—¶ä¸è¿›è¡Œäº‹å®æå–å’ŒçŸ¥è¯†åº“æ›´æ–° =====
        # çŸ¥è¯†èåˆéƒ¨åˆ†ï¼ˆMemoryGate, GatedMemoryFusionï¼‰ä¼šé€šè¿‡æ¢¯åº¦è‡ªåŠ¨æ›´æ–°
        # çŸ¥è¯†åº“æ›´æ–°åªåœ¨æ¨ç†æ—¶è¿›è¡Œ
        
        return result
    
    def get_statistics(self) -> Dict[str, any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "inference_counter": self.inference_counter,
            "enable_fact_extraction": self.enable_fact_extraction,
            "fact_update_frequency": self.fact_update_frequency,
        }
        
        if self.memory_bank_updater:
            stats["memory_bank_stats"] = self.memory_bank_updater.get_statistics()
        
        return stats

