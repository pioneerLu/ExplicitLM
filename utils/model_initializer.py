"""
æ¨¡å‹åˆå§‹åŒ–å·¥å…·æ¨¡å—

æä¾›ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹å’Œåˆå§‹åŒ–ç­–ç•¥ã€‚
åŒ…å«æƒé‡åˆå§‹åŒ–ã€é¢„è®­ç»ƒåµŒå…¥åŠ è½½ã€çŸ¥è¯†æ•°æ®åº“å¤„ç†ç­‰åŠŸèƒ½ã€‚

å…¼å®¹æ–°ç‰ˆ dict é…ç½®ç³»ç»Ÿã€‚
"""

import os
import json
import time
from typing import Dict, List, Tuple, Optional, Any
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoConfig
from transformers.models.qwen3.modeling_qwen3 import Qwen3Config
from hydra.utils import get_original_cwd
from pathlib import Path
from utils.logger import Logger


# ---------- ä»¥ä¸‹ä»£ç ç”¨äºå¤„ç†è®°å¿†åº“æ•°æ® ----------
class MemoryBankProcessor:
    def __init__(self, tokenizer: AutoTokenizer):
        self.tokenizer = tokenizer
        self.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0

    def process_memory_bank(
        self,
        database_path: str,
        cache_path: str,
        knowledge_num: int,
        knowledge_length: int,
        recompute: bool = False,
    ) -> torch.Tensor:
        if not recompute and os.path.exists(cache_path):
            Logger(f"ä»ç¼“å­˜åŠ è½½memory_bankåˆå§‹åŒ–æ•°æ®: {cache_path}")
            processed_tensor = torch.load(cache_path)
            Logger(f"åŠ è½½çš„memory_bankæ•°æ®å½¢çŠ¶: {processed_tensor.shape}")
            return processed_tensor
        Logger(f"å¤„ç†æ–‡æœ¬æ•°æ®ç”¨äºmemory_bankåˆå§‹åŒ–: {database_path}")
        with open(database_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        Logger(f"ä» {database_path} åŠ è½½äº† {len(data)} æ¡å¥å­")
        processed_tensor, database_mapping = self._process_memory_sentences(
            data, knowledge_num, knowledge_length
        )
        self._save_memory_cache(
            processed_tensor, database_mapping, cache_path, database_path
        )
        return processed_tensor

    def _process_memory_sentences(
        self,
        data: List,
        knowledge_num: int,
        knowledge_length: int,
    ) -> Tuple[torch.Tensor, List[Dict]]:
        processed_rows = []
        database_mapping = []
        total_sentences = len(data)
        truncated_sentences = 0
        num_to_process = min(len(data), knowledge_num)
        Logger(f"å¤„ç† {num_to_process}/{total_sentences} æ¡å¥å­")
        for idx, item in enumerate(data[:num_to_process]):
            if idx % 1000 == 0:
                Logger(f"å¤„ç†å¥å­ {idx+1}/{num_to_process}")
            sentence_info = self._extract_sentence_info(item)
            sentence = sentence_info["sentence"]
            try:
                tokens_result = self.tokenizer(
                    sentence,
                    add_special_tokens=True,
                    truncation=True,
                    max_length=len(sentence),
                    padding=False,
                    return_tensors="pt",
                )
                tokens = tokens_result["input_ids"].squeeze().tolist()
                if not isinstance(tokens, list):
                    tokens = [tokens]
                original_length = len(tokens)
                if len(tokens) > knowledge_length:
                    tokens = tokens[:knowledge_length]
                    truncated_sentences += 1
                elif len(tokens) < knowledge_length:
                    tokens.extend([self.pad_token_id] * (knowledge_length - len(tokens)))
                processed_rows.append(tokens)
                database_mapping.append(
                    {
                        "database_index": idx,
                        "uuid": sentence_info["uuid"],
                        "sentence": sentence,
                        "subject": sentence_info["subject"],
                        "predicate": sentence_info["predicate"],
                        "object": sentence_info["object"],
                        "token_count": len(tokens),
                        "is_truncated": original_length > knowledge_length,
                    }
                )
            except Exception as e:
                Logger(f"å¤„ç†å¥å­ {idx} æ—¶å‡ºé”™: {e}")
                empty_tokens = [self.pad_token_id] * knowledge_length
                processed_rows.append(empty_tokens)
                database_mapping.append(
                    {
                        "database_index": idx,
                        "uuid": sentence_info["uuid"],
                        "sentence": sentence,
                        "subject": sentence_info["subject"],
                        "predicate": sentence_info["predicate"],
                        "object": sentence_info["object"],
                        "token_count": knowledge_length,
                        "is_truncated": False,
                        "processing_error": str(e),
                    }
                )
        while len(processed_rows) < knowledge_num:
            processed_rows.append([self.pad_token_id] * knowledge_length)
        processed_tensor = torch.tensor(processed_rows, dtype=torch.long)
        self._log_memory_statistics(
            total_sentences, truncated_sentences, num_to_process,
            knowledge_num, knowledge_length, processed_tensor.shape
        )
        return processed_tensor, database_mapping

    def _extract_sentence_info(self, item: Any) -> Dict[str, str]:
        if isinstance(item, dict):
            if "target" in item and len(item["target"]) > 0:
                target = item["target"][0]
                return {
                    "sentence": target.get("sentence", ""),
                    "uuid": target.get("uuid", ""),
                    "subject": target.get("subject", ""),
                    "predicate": target.get("predicate", ""),
                    "object": target.get("object", ""),
                }
            else:
                return {
                    "sentence": item.get("sentence", "") or item.get("text", "") or str(item),
                    "uuid": item.get("uuid", ""),
                    "subject": item.get("subject", ""),
                    "predicate": item.get("predicate", ""),
                    "object": item.get("object", ""),
                }
        else:
            return {
                "sentence": str(item),
                "uuid": "",
                "subject": "",
                "predicate": "",
                "object": "",
            }

    def _log_memory_statistics(
        self,
        total_sentences: int,
        truncated_sentences: int,
        num_processed: int,
        knowledge_num: int,
        knowledge_length: int,
        final_shape: torch.Size,
    ) -> None:
        truncation_ratio = truncated_sentences / total_sentences if total_sentences > 0 else 0.0
        Logger(f"æˆªæ–­å¥å­ç»Ÿè®¡:")
        Logger(f"  - æ€»å¥å­æ•°: {total_sentences}")
        Logger(f"  - æˆªæ–­å¥å­æ•°: {truncated_sentences}")
        Logger(f"  - æˆªæ–­å æ¯”: {truncation_ratio:.4f} ({truncation_ratio*100:.2f}%)")
        Logger(f"Memory_bankæ•°æ®å¤„ç†å®Œæˆ:")
        Logger(f"  - å¤„ç†å¥å­æ•°: {num_processed}")
        Logger(f"  - æ·»åŠ ç©ºæ¡ç›®æ•°: {knowledge_num - num_processed}")
        Logger(f"  - æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        Logger(f"  - æœŸæœ›å½¢çŠ¶: ({knowledge_num}, {knowledge_length})")

    def _save_memory_cache(
        self,
        processed_tensor: torch.Tensor,
        database_mapping: List[Dict],
        cache_path: str,
        database_path: str,
    ) -> None:
        try:
            torch.save(processed_tensor, cache_path)
            Logger(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {cache_path}")
        except Exception as e:
            Logger(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {e}")
        try:
            mapping_file_path = cache_path.replace(".pt", "_mapping.json")
            mapping_data = {
                "metadata": {
                    "total_entries": len(database_mapping),
                    "knowledge_num": processed_tensor.shape[0],
                    "knowledge_length": processed_tensor.shape[1],
                    "source_file": database_path,
                    "generation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                "mappings": database_mapping,
            }
            with open(mapping_file_path, "w", encoding="utf-8") as f:
                json.dump(mapping_data, f, ensure_ascii=False, indent=2)
            Logger(f"æ•°æ®åº“æ˜ å°„å·²ä¿å­˜åˆ°: {mapping_file_path}")
        except Exception as e:
            Logger(f"ä¿å­˜æ•°æ®åº“æ˜ å°„å¤±è´¥: {e}")


# ------------------------------------------------------------------
# ç»Ÿä¸€å…¥å£å‡½æ•°ï¼šåªæ”¹å‚æ•°è¯»å–æ–¹å¼ï¼Œå…¶ä½™ä¸åŠ¨
# ------------------------------------------------------------------
def init_model(args: dict, accelerator=None):
    """
    ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–æ¥å£ï¼ˆä»…æ”¯æŒQwen3æ¶æ„ï¼‰

    Args:
        args: é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - qwen3_model_path: Qwen3é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
            - è®°å¿†åº“ç›¸å…³é…ç½®ï¼ˆknowledge_num, knowledge_dimç­‰ï¼‰
        accelerator: Accelerator å¯¹è±¡ï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒæ—¶çš„æ—¥å¿—è¾“å‡º

    Returns:
        (model, tokenizer) tuple
    """
    # åªæ”¯æŒ Qwen3 æ¶æ„
    qwen3_model_path = args.get("qwen3_model_path", None)
    if qwen3_model_path is None:
        raise ValueError("å¿…é¡»æŒ‡å®šqwen3_model_pathå‚æ•°ï¼ŒæŒ‡å‘Qwen3-4Bé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    
    return _init_qwen3_model(args, accelerator)


def _init_qwen3_model(args: dict, accelerator=None):
    """Qwen3 æ¶æ„æ¨¡å‹åˆå§‹åŒ–"""
    qwen3_model_path = args.get("qwen3_model_path", None)
    if qwen3_model_path is None:
        raise ValueError("å¿…é¡»æŒ‡å®šqwen3_model_pathå‚æ•°ï¼ŒæŒ‡å‘Qwen3-4Bé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    
    database_init_path = args.get("database_init_path", None)
    cache_path = args.get("cache_path", "cache/knowledge_cache.pt")
    recompute_cache = args.get("recompute_cache", False)

    Logger("=" * 60, accelerator)
    Logger("ğŸš€ å¼€å§‹æ¨¡å‹åˆå§‹åŒ–æµç¨‹ï¼ˆQwen3æ¶æ„ï¼‰", accelerator)
    Logger("=" * 60, accelerator)
    Logger(f"ğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯:", accelerator)
    Logger(f"  - Qwen3æ¨¡å‹è·¯å¾„: {qwen3_model_path}", accelerator)
    Logger(f"  - æ•°æ®åº“åˆå§‹åŒ–è·¯å¾„: {database_init_path if database_init_path else 'æœªæŒ‡å®š'}", accelerator)
    Logger(f"  - ç¼“å­˜è·¯å¾„: {cache_path}", accelerator)
    Logger(f"  - é‡æ–°è®¡ç®—ç¼“å­˜: {recompute_cache}", accelerator)
    
    # åŠ è½½Qwen3é…ç½®
    Logger("ğŸ“¦ åŠ è½½Qwen3é…ç½®...", accelerator)
    qwen3_config = Qwen3Config.from_pretrained(qwen3_model_path)
    Logger(f"âœ… Qwen3é…ç½®åŠ è½½å®Œæˆ:", accelerator)
    Logger(f"  - hidden_size: {qwen3_config.hidden_size}", accelerator)
    Logger(f"  - num_hidden_layers: {qwen3_config.num_hidden_layers}", accelerator)
    Logger(f"  - num_attention_heads: {qwen3_config.num_attention_heads}", accelerator)
    Logger(f"  - vocab_size: {qwen3_config.vocab_size}", accelerator)
    
    # æå–è®°å¿†åº“é…ç½®
    memory_cfg = {
        "knowledge_num": args.get("knowledge_num", 1024 * 1024),
        "knowledge_length": args.get("knowledge_length", 16),
        "knowledge_dim": args.get("knowledge_dim", 128),
        "use_ema_update": args.get("use_ema_update", True),
        "ema_decay": args.get("ema_decay", 0.9),
        "ema_update_freq": args.get("ema_update_freq", 5),
        "freeze_ratio": args.get("freeze_ratio", 0.2),
        "num_candidates": args.get("num_candidates", 16),
        "num_selected": args.get("num_selected", 1),
        "gumbel_temperature": args.get("gumbel_temperature", 1.0),
        "use_moe": args.get("use_moe", False),
        "dropout": args.get("dropout", 0.0),
    }
    Logger(f"ğŸ“‹ è®°å¿†åº“é…ç½®:", accelerator)
    Logger(f"  - knowledge_num: {memory_cfg['knowledge_num']}", accelerator)
    Logger(f"  - knowledge_length: {memory_cfg['knowledge_length']}", accelerator)
    Logger(f"  - knowledge_dim: {memory_cfg['knowledge_dim']}", accelerator)
    
    # å¯¼å…¥æ¨¡å‹ç±»
    Logger("ğŸ“¦ å¯¼å…¥æ¨¡å‹æ¨¡å—...", accelerator)
    from models.core.ExplicitLM import ExplicitLM
    
    # è¾“å‡ºå½“å‰ç›®å½•
    Logger(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}", accelerator)
    
    # åŠ è½½ Qwen3 tokenizer
    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°tokenizerè·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»Qwenæ¨¡å‹è·¯å¾„åŠ è½½
    Logger("ğŸ”¤ åŠ è½½Qwen3 tokenizer...", accelerator)
    try:
        original_cwd = get_original_cwd()
    except ValueError:
        # é Hydra ç¯å¢ƒï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
        original_cwd = os.getcwd()
    local_tokenizer_path = Path(original_cwd) / "models" / "qwen_tokenizer"
    if local_tokenizer_path.exists() and (local_tokenizer_path / "tokenizer.json").exists():
        Logger(f"  - ä½¿ç”¨æœ¬åœ°tokenizer: {local_tokenizer_path}", accelerator)
        tokenizer = AutoTokenizer.from_pretrained(str(local_tokenizer_path), trust_remote_code=True)
    else:
        Logger(f"  - ä»Qwenæ¨¡å‹è·¯å¾„åŠ è½½: {qwen3_model_path}", accelerator)
        tokenizer = AutoTokenizer.from_pretrained(qwen3_model_path, trust_remote_code=True)
    
    # ç¡®ä¿Qwen tokenizerçš„ç‰¹æ®Štokené…ç½®æ­£ç¡®
    if tokenizer.pad_token is None:
        # Qwen tokenizerå¯èƒ½æ²¡æœ‰pad_tokenï¼Œä½¿ç”¨eos_tokenä½œä¸ºpad_token
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        Logger("âš ï¸  Qwen tokenizeræ²¡æœ‰pad_tokenï¼Œä½¿ç”¨eos_tokenä½œä¸ºpad_token", accelerator)
    
    Logger(f"âœ… TokenizeråŠ è½½å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}", accelerator)
    Logger(f"  - pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})", accelerator)
    Logger(f"  - bos_token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})", accelerator)
    Logger(f"  - eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})", accelerator)

    # åˆ›å»ºæ¨¡å‹
    Logger("ğŸ—ï¸  åˆ›å»ºæ¨¡å‹å®ä¾‹...", accelerator)
    model = ExplicitLM(qwen3_config=qwen3_config, memory_cfg=memory_cfg)
    Logger("âœ… æ¨¡å‹å®ä¾‹åˆ›å»ºå®Œæˆ", accelerator)
    
    # ä»Qwen3é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡
    Logger("ğŸ¯ ä»Qwen3é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡...", accelerator)
    try:
        from transformers import Qwen3ForCausalLM
        pretrained_model = Qwen3ForCausalLM.from_pretrained(
            qwen3_model_path,
            torch_dtype=torch.float32,  # å…ˆåŠ è½½ä¸ºfloat32ï¼Œåç»­å¯ä»¥è½¬æ¢
            device_map="cpu",
        )
        
        # åŠ è½½åŒ¹é…çš„æƒé‡
        model_state_dict = model.state_dict()
        pretrained_state_dict = pretrained_model.state_dict()
        
        def map_pretrained_key(pretrained_key: str) -> str:
            """å°†é¢„è®­ç»ƒæ¨¡å‹çš„å±‚åç§°æ˜ å°„åˆ°æˆ‘ä»¬çš„æ¨¡å‹å±‚åç§°"""
            # ç§»é™¤ "model." å‰ç¼€
            if pretrained_key.startswith("model."):
                key = pretrained_key[6:]  # ç§»é™¤ "model." å‰ç¼€
            else:
                key = pretrained_key
            
            # æ˜ å°„å±‚åç§°
            if key.startswith("layers."):
                # å°† layers.X.xxx æ˜ å°„ä¸º layers.X.qwen3_decoder.xxx
                parts = key.split(".", 2)
                if len(parts) >= 3:
                    layer_idx = parts[1]
                    rest = parts[2]
                    return f"layers.{layer_idx}.qwen3_decoder.{rest}"
                else:
                    return key
            else:
                # embed_tokens, norm, lm_head ç­‰ç›´æ¥ä½¿ç”¨
                return key
        
        loaded_keys = []
        missing_keys = []
        shape_mismatches = []
        
        for key in model_state_dict.keys():
            # è·³è¿‡æ–°å¢çš„å‚æ•°
            if key.startswith("memory_bank") or key.startswith("tok_embeddings") or \
               "memory_gate" in key or "gated_memory_fusion" in key or "memory_norm" in key:
                continue
            
            # å°è¯•ç›´æ¥åŒ¹é…
            if key in pretrained_state_dict:
                if model_state_dict[key].shape == pretrained_state_dict[key].shape:
                    model_state_dict[key] = pretrained_state_dict[key]
                    loaded_keys.append(key)
                else:
                    shape_mismatches.append(f"{key} (shape: {model_state_dict[key].shape} vs {pretrained_state_dict[key].shape})")
            else:
                # å°è¯•é€šè¿‡æ˜ å°„æ‰¾åˆ°å¯¹åº”çš„é¢„è®­ç»ƒæƒé‡
                found = False
                for pretrained_key in pretrained_state_dict.keys():
                    mapped_key = map_pretrained_key(pretrained_key)
                    if mapped_key == key:
                        if model_state_dict[key].shape == pretrained_state_dict[pretrained_key].shape:
                            model_state_dict[key] = pretrained_state_dict[pretrained_key]
                            loaded_keys.append(key)
                            found = True
                            break
                        else:
                            shape_mismatches.append(f"{key} (shape: {model_state_dict[key].shape} vs {pretrained_state_dict[pretrained_key].shape})")
                            found = True
                            break
                
                if not found:
                    missing_keys.append(key)
        
        model.load_state_dict(model_state_dict, strict=False)
        Logger(f"âœ… æƒé‡åŠ è½½å®Œæˆ: {len(loaded_keys)}ä¸ªå‚æ•°å·²åŠ è½½", accelerator)
        if missing_keys:
            Logger(f"âš ï¸  ä»¥ä¸‹å‚æ•°æœªåŠ è½½ï¼ˆå¯èƒ½æ˜¯æ–°å¢çš„è®°å¿†ç›¸å…³å‚æ•°ï¼‰: {len(missing_keys)}ä¸ª", accelerator)
            if len(missing_keys) <= 10:
                for key in missing_keys[:10]:
                    Logger(f"    - {key}", accelerator)
        if shape_mismatches:
            Logger(f"âš ï¸  ä»¥ä¸‹å‚æ•°å½¢çŠ¶ä¸åŒ¹é…: {len(shape_mismatches)}ä¸ª", accelerator)
            if len(shape_mismatches) <= 5:
                for key in shape_mismatches[:5]:
                    Logger(f"    - {key}", accelerator)
        
        # æ¸…ç†ä¸´æ—¶æ¨¡å‹
        del pretrained_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
    except Exception as e:
        Logger(f"âš ï¸  ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡å¤±è´¥: {e}", accelerator)
        Logger("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡", accelerator)

    # æ•°æ®åº“ / è®°å¿†åº“åˆå§‹åŒ–ï¼ˆMOE æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
    use_moe = memory_cfg.get("use_moe", False)
    if use_moe:
        Logger("âš ï¸  MOE æ¨¡å¼ï¼šè·³è¿‡è®°å¿†åº“åˆå§‹åŒ–ï¼ˆMOE æ¨¡å¼ä¸éœ€è¦ memory_bankï¼‰", accelerator)
    elif database_init_path:
        Logger("ğŸ—„ï¸  å¼€å§‹è®°å¿†åº“åˆå§‹åŒ–...", accelerator)
        Logger(f"  - æ•°æ®åº“è·¯å¾„: {database_init_path}", accelerator)
        Logger(f"  - ç¼“å­˜è·¯å¾„: {cache_path}", accelerator)
        Logger(f"  - çŸ¥è¯†åº“å¤§å°: {memory_cfg['knowledge_num']}", accelerator)
        Logger(f"  - çŸ¥è¯†æ¡ç›®é•¿åº¦: {memory_cfg['knowledge_length']}", accelerator)
        
        _initialize_database(
            model=model,
            tokenizer=tokenizer,
            database_path=database_init_path,
            cache_path=cache_path,
            knowledge_num=memory_cfg["knowledge_num"],
            knowledge_length=memory_cfg["knowledge_length"],
            recompute=recompute_cache,
            model_type="qwen3_explicitlm",
            database_attribute="memory_bank",
            accelerator=accelerator,
        )
        Logger("âœ… è®°å¿†åº“åˆå§‹åŒ–å®Œæˆ", accelerator)
    else:
        Logger("âš ï¸  æœªæŒ‡å®šæ•°æ®åº“åˆå§‹åŒ–è·¯å¾„ï¼Œè®°å¿†åº“å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–", accelerator)

    # å†»ç»“Qwenä¸»æ¨¡å‹å‚æ•°ï¼Œåªä¿ç•™è®°å¿†åº“ç›¸å…³å‚æ•°å¯è®­ç»ƒ
    Logger("ğŸ”’ å†»ç»“Qwenä¸»æ¨¡å‹å‚æ•°...", accelerator)
    frozen_params = 0
    trainable_params = 0
    
    use_ema_update = memory_cfg.get("use_ema_update", False)
    
    # å†»ç»“æ‰€æœ‰QwenåŸºç¡€ç»„ä»¶
    for name, param in model.named_parameters():
        # ä¿ç•™å¯è®­ç»ƒçš„å‚æ•°ï¼šè®°å¿†åº“ç›¸å…³ç»„ä»¶
        is_memory_component = any(keyword in name for keyword in [
            "memory_gate",  # è®°å¿†é—¨æ§æ¨¡å—
            "gated_memory_fusion",  # è®°å¿†èåˆæ¨¡å—
            "memory_norm",  # è®°å¿†å½’ä¸€åŒ–å±‚
        ])
        
        # memory_bankå­˜å‚¨çš„æ˜¯token IDsï¼ˆint64ï¼‰ï¼Œä¸åº”è¯¥ç›´æ¥é€šè¿‡æ¢¯åº¦æ›´æ–°
        # åº”è¯¥é€šè¿‡EMAæœºåˆ¶æ›´æ–°ï¼Œæ‰€ä»¥å§‹ç»ˆè®¾ç½®ä¸ºä¸å¯è®­ç»ƒ
        is_memory_bank = "memory_bank" in name
        if is_memory_bank:
            # memory_bankå§‹ç»ˆä¸å¯è®­ç»ƒï¼Œé¿å…DeepSpeedæ¢¯åº¦å¹³å‡æ—¶çš„ç±»å‹é”™è¯¯
            param.requires_grad = False
            frozen_params += param.numel()
        elif is_memory_component:
            # å…¶ä»–è®°å¿†ç›¸å…³ç»„ä»¶å§‹ç»ˆå¯è®­ç»ƒ
            param.requires_grad = True
            trainable_params += param.numel()
        else:
            # å†»ç»“æ‰€æœ‰å…¶ä»–å‚æ•°ï¼ˆQwenä¸»æ¨¡å‹ï¼‰
            param.requires_grad = False
            frozen_params += param.numel()
    
    Logger(f"âœ… å‚æ•°å†»ç»“å®Œæˆ:", accelerator)
    Logger(f"  - å†»ç»“å‚æ•°: {frozen_params / 1e6:.3f} ç™¾ä¸‡", accelerator)
    Logger(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e6:.3f} ç™¾ä¸‡", accelerator)
    if frozen_params + trainable_params > 0:
        Logger(f"  - å†»ç»“æ¯”ä¾‹: {frozen_params / (frozen_params + trainable_params) * 100:.2f}%", accelerator)
    
    # å‚æ•°ç»Ÿè®¡
    Logger("ğŸ“Š è®¡ç®—æ¨¡å‹å‚æ•°ç»Ÿè®¡...", accelerator)
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    Logger(f"ğŸ“ˆ å¯è®­ç»ƒå‚æ•°é‡ï¼š{total_params:.3f} ç™¾ä¸‡", accelerator)
    
    Logger("=" * 60, accelerator)
    Logger("ğŸ‰ æ¨¡å‹åˆå§‹åŒ–æµç¨‹å®Œæˆ", accelerator)
    Logger("=" * 60, accelerator)

    return model, tokenizer


# ---------- ä»¥ä¸‹è¾…åŠ©å‡½æ•° ----------
def _initialize_database(
    model: nn.Module,
    tokenizer: AutoTokenizer,
    database_path: str,
    cache_path: str,
    knowledge_num: int,
    knowledge_length: int,
    recompute: bool,
    model_type: str,
    database_attribute: str,
    accelerator=None,
) -> None:
    Logger("ğŸ” å¼€å§‹æ•°æ®åº“/è®°å¿†åº“åˆå§‹åŒ–è¯¦ç»†æµç¨‹", accelerator)
    Logger(f"ğŸ“Š åˆå§‹åŒ–å‚æ•°:", accelerator)
    Logger(f"  - æ¨¡å‹ç±»å‹: {model_type}", accelerator)
    Logger(f"  - æ•°æ®åº“è·¯å¾„: {database_path}", accelerator)
    Logger(f"  - ç¼“å­˜è·¯å¾„: {cache_path}", accelerator)
    Logger(f"  - çŸ¥è¯†åº“å¤§å°: {knowledge_num}", accelerator)
    Logger(f"  - çŸ¥è¯†æ¡ç›®é•¿åº¦: {knowledge_length}", accelerator)
    Logger(f"  - é‡æ–°è®¡ç®—ç¼“å­˜: {recompute}", accelerator)
    Logger(f"  - ç›®æ ‡å±æ€§è·¯å¾„: {database_attribute}", accelerator)
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(database_path):
        Logger(f"âŒ é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {database_path}", accelerator)
        raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {database_path}")
    
    # è·å–æ•°æ®åº“æ–‡ä»¶ä¿¡æ¯
    try:
        file_size = os.path.getsize(database_path)
        file_size_mb = file_size / (1024 * 1024)
        Logger(f"ğŸ“ æ•°æ®åº“æ–‡ä»¶ä¿¡æ¯:", accelerator)
        Logger(f"  - æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB ({file_size:,} bytes)", accelerator)
        Logger(f"  - æ–‡ä»¶è·¯å¾„: {os.path.abspath(database_path)}", accelerator)
    except Exception as e:
        Logger(f"âš ï¸  æ— æ³•è·å–æ•°æ®åº“æ–‡ä»¶ä¿¡æ¯: {e}", accelerator)
    
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    cache_dir = os.path.dirname(cache_path)
    if cache_dir and not os.path.exists(cache_dir):
        Logger(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {cache_dir}", accelerator)
        os.makedirs(cache_dir, exist_ok=True)
    
    # ä½¿ç”¨MemoryBankProcessorå¤„ç†è®°å¿†åº“æ•°æ®ï¼ˆQwen3æ¶æ„åªæ”¯æŒmemory_bankï¼‰
    Logger("ğŸ§  ä½¿ç”¨MemoryBankProcessorå¤„ç†è®°å¿†åº“æ•°æ®", accelerator)
    processor = MemoryBankProcessor(tokenizer)
    if not cache_path or cache_path == "cache/knowledge_cache.pt":
        cache_path = f"cache/memory_bank_init_{knowledge_num}_{knowledge_length}.pt"
        Logger(f"ğŸ”„ è‡ªåŠ¨è°ƒæ•´ç¼“å­˜è·¯å¾„ä¸º: {cache_path}", accelerator)
    
    Logger("ğŸš€ å¼€å§‹å¤„ç†è®°å¿†åº“æ•°æ®...", accelerator)
    start_time = time.time()
    processed_tensor = processor.process_memory_bank(
        database_path=database_path,
        cache_path=cache_path,
        knowledge_num=knowledge_num,
        knowledge_length=knowledge_length,
        recompute=recompute,
    )
    processing_time = time.time() - start_time
    Logger(f"â±ï¸  è®°å¿†åº“æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’", accelerator)
    
    # éªŒè¯å¤„ç†åçš„å¼ é‡
    Logger("ğŸ” éªŒè¯å¤„ç†åçš„æ•°æ®å¼ é‡...", accelerator)
    if processed_tensor is not None:
        Logger(f"âœ… å¼ é‡éªŒè¯é€šè¿‡:", accelerator)
        Logger(f"  - å¼ é‡å½¢çŠ¶: {processed_tensor.shape}", accelerator)
        Logger(f"  - å¼ é‡ç±»å‹: {processed_tensor.dtype}", accelerator)
        Logger(f"  - å¼ é‡è®¾å¤‡: {processed_tensor.device}", accelerator)
        Logger(f"  - å†…å­˜å ç”¨: {processed_tensor.numel() * processed_tensor.element_size() / (1024*1024):.2f} MB", accelerator)
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´
        if processed_tensor.numel() > 0:
            min_val = processed_tensor.min().item()
            max_val = processed_tensor.max().item()
            Logger(f"  - æ•°æ®èŒƒå›´: [{min_val}, {max_val}]", accelerator)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if min_val < 0:
                Logger(f"âš ï¸  è­¦å‘Š: å‘ç°è´Ÿå€¼token IDï¼Œæœ€å°å€¼: {min_val}", accelerator)
            if hasattr(tokenizer, 'vocab_size') and max_val >= tokenizer.vocab_size:
                Logger(f"âš ï¸  è­¦å‘Š: å‘ç°è¶…å‡ºè¯æ±‡è¡¨èŒƒå›´çš„token IDï¼Œæœ€å¤§å€¼: {max_val}, è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}", accelerator)
    else:
        Logger("âŒ é”™è¯¯: å¤„ç†åçš„å¼ é‡ä¸ºNone", accelerator)
        raise ValueError("æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¿”å›çš„å¼ é‡ä¸ºNone")
    
    # è®¾ç½®æ¨¡å‹å±æ€§
    Logger("ğŸ”§ è®¾ç½®æ¨¡å‹æ•°æ®åº“å±æ€§...", accelerator)
    start_time = time.time()
    _set_database_attribute(model, database_attribute, processed_tensor, accelerator)
    attribute_setting_time = time.time() - start_time
    Logger(f"â±ï¸  æ¨¡å‹å±æ€§è®¾ç½®å®Œæˆï¼Œè€—æ—¶: {attribute_setting_time:.4f} ç§’", accelerator)
    
    # éªŒè¯æ¨¡å‹å±æ€§æ˜¯å¦æ­£ç¡®è®¾ç½®
    Logger("ğŸ” éªŒè¯æ¨¡å‹å±æ€§è®¾ç½®...", accelerator)
    attributes = database_attribute.split(".")
    target = model
    for attr in attributes[:-1]:
        if hasattr(target, attr):
            target = getattr(target, attr)
        else:
            Logger(f"âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°ä¸­é—´å±æ€§ {attr}", accelerator)
            return
    
    final_attr = attributes[-1]
    if hasattr(target, final_attr):
        stored_tensor = getattr(target, final_attr)
        if torch.equal(stored_tensor, processed_tensor):
            Logger(f"âœ… æ¨¡å‹å±æ€§éªŒè¯æˆåŠŸ: model.{database_attribute}", accelerator)
            Logger(f"  - å­˜å‚¨å¼ é‡å½¢çŠ¶: {stored_tensor.shape}", accelerator)
            Logger(f"  - å­˜å‚¨å¼ é‡ç±»å‹: {stored_tensor.dtype}", accelerator)
            Logger(f"  - å­˜å‚¨å¼ é‡è®¾å¤‡: {stored_tensor.device}", accelerator)
        else:
            Logger(f"âŒ é”™è¯¯: æ¨¡å‹å±æ€§éªŒè¯å¤±è´¥ï¼Œå­˜å‚¨çš„å¼ é‡ä¸åŸå§‹å¼ é‡ä¸åŒ¹é…", accelerator)
    else:
        Logger(f"âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°ç›®æ ‡å±æ€§ {final_attr}", accelerator)
    
    # æ€»ä½“ç»Ÿè®¡
    total_time = time.time() - start_time
    Logger("ğŸ“Š æ•°æ®åº“/è®°å¿†åº“åˆå§‹åŒ–ç»Ÿè®¡:", accelerator)
    Logger(f"  - æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’", accelerator)
    Logger(f"  - æ•°æ®æ¡ç›®æ•°: {processed_tensor.shape[0]}", accelerator)
    Logger(f"  - æ¯æ¡ç›®é•¿åº¦: {processed_tensor.shape[1]}", accelerator)
    Logger(f"  - æ€»tokenæ•°: {processed_tensor.numel()}", accelerator)
    Logger(f"  - å¤„ç†é€Ÿåº¦: {processed_tensor.numel() / total_time:.0f} tokens/ç§’", accelerator)
    
    Logger("âœ… æ•°æ®åº“åµŒå…¥å’Œå¥å­å·²æˆåŠŸå­˜å‚¨åˆ°æ¨¡å‹", accelerator)


def _set_database_attribute(model: nn.Module, attribute_path: str, data: torch.Tensor, accelerator=None) -> None:
    attributes = attribute_path.split(".")
    target = model
    for attr in attributes[:-1]:
        if not hasattr(target, attr):
            Logger(f"è­¦å‘Š: æ‰¾ä¸åˆ°å±æ€§ {attr}ï¼Œæ— æ³•åˆå§‹åŒ–æ•°æ®åº“", accelerator)
            return
        target = getattr(target, attr)
    final_attr = attributes[-1]
    if hasattr(target, final_attr):
        getattr(target, final_attr).data.copy_(data)
        Logger(f"æˆåŠŸåˆå§‹åŒ– model.{attribute_path} ä½¿ç”¨å¤„ç†åçš„æ•°æ®", accelerator)
    else:
        Logger(f"è­¦å‘Š: æ‰¾ä¸åˆ° model.{attribute_path} è¿›è¡Œåˆå§‹åŒ–", accelerator)
        globals()["processed_database"] = data
