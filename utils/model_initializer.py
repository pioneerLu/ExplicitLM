"""
æ¨¡å‹åˆå§‹åŒ–å·¥å…·æ¨¡å—

æä¾›ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹å’Œåˆå§‹åŒ–ç­–ç•¥ã€‚
åŒ…å«æƒé‡åˆå§‹åŒ–ã€é¢„è®­ç»ƒåµŒå…¥åŠ è½½ã€çŸ¥è¯†æ•°æ®åº“å¤„ç†ç­‰åŠŸèƒ½ã€‚

å…¼å®¹æ–°ç‰ˆ dict é…ç½®ç³»ç»Ÿã€‚
"""

import os
import json
import time
from typing import Dict, List, Tuple, Optional, Any
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoConfig
from transformers.models.qwen3.modeling_qwen3 import Qwen3Config
from hydra.utils import get_original_cwd
from pathlib import Path
from utils.logger import Logger


# ---------- ä»¥ä¸‹ä»£ç ç”¨äºå¤„ç†è®°å¿†åº“æ•°æ® ----------
class MemoryBankProcessor:
    def __init__(self, tokenizer: AutoTokenizer):
        self.tokenizer = tokenizer
        self.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0

    def process_memory_bank(
        self,
        database_path: str,
        cache_path: str,
        knowledge_num: int,
        knowledge_length: int,
        recompute: bool = False,
    ) -> torch.Tensor:
        if not recompute and os.path.exists(cache_path):
            Logger(f"ä»ç¼“å­˜åŠ è½½memory_bankåˆå§‹åŒ–æ•°æ®: {cache_path}")
            processed_tensor = torch.load(cache_path)
            Logger(f"åŠ è½½çš„memory_bankæ•°æ®å½¢çŠ¶: {processed_tensor.shape}")
            return processed_tensor
        Logger(f"å¤„ç†æ–‡æœ¬æ•°æ®ç”¨äºmemory_bankåˆå§‹åŒ–: {database_path}")
        with open(database_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        Logger(f"ä» {database_path} åŠ è½½äº† {len(data)} æ¡å¥å­")
        processed_tensor, database_mapping = self._process_memory_sentences(
            data, knowledge_num, knowledge_length
        )
        self._save_memory_cache(
            processed_tensor, database_mapping, cache_path, database_path
        )
        return processed_tensor

    def _process_memory_sentences(
        self,
        data: List,
        knowledge_num: int,
        knowledge_length: int,
    ) -> Tuple[torch.Tensor, List[Dict]]:
        processed_rows = []
        database_mapping = []
        total_sentences = len(data)
        truncated_sentences = 0
        num_to_process = min(len(data), knowledge_num)
        Logger(f"å¤„ç† {num_to_process}/{total_sentences} æ¡å¥å­")
        for idx, item in enumerate(data[:num_to_process]):
            if idx % 1000 == 0:
                Logger(f"å¤„ç†å¥å­ {idx+1}/{num_to_process}")
            sentence_info = self._extract_sentence_info(item)
            sentence = sentence_info["sentence"]
            try:
                tokens_result = self.tokenizer(
                    sentence,
                    add_special_tokens=True,
                    truncation=True,
                    max_length=len(sentence),
                    padding=False,
                    return_tensors="pt",
                )
                tokens = tokens_result["input_ids"].squeeze().tolist()
                if not isinstance(tokens, list):
                    tokens = [tokens]
                original_length = len(tokens)
                if len(tokens) > knowledge_length:
                    tokens = tokens[:knowledge_length]
                    truncated_sentences += 1
                elif len(tokens) < knowledge_length:
                    tokens.extend([self.pad_token_id] * (knowledge_length - len(tokens)))
                processed_rows.append(tokens)
                database_mapping.append(
                    {
                        "database_index": idx,
                        "uuid": sentence_info["uuid"],
                        "sentence": sentence,
                        "subject": sentence_info["subject"],
                        "predicate": sentence_info["predicate"],
                        "object": sentence_info["object"],
                        "token_count": len(tokens),
                        "is_truncated": original_length > knowledge_length,
                    }
                )
            except Exception as e:
                Logger(f"å¤„ç†å¥å­ {idx} æ—¶å‡ºé”™: {e}")
                empty_tokens = [self.pad_token_id] * knowledge_length
                processed_rows.append(empty_tokens)
                database_mapping.append(
                    {
                        "database_index": idx,
                        "uuid": sentence_info["uuid"],
                        "sentence": sentence,
                        "subject": sentence_info["subject"],
                        "predicate": sentence_info["predicate"],
                        "object": sentence_info["object"],
                        "token_count": knowledge_length,
                        "is_truncated": False,
                        "processing_error": str(e),
                    }
                )
        while len(processed_rows) < knowledge_num:
            processed_rows.append([self.pad_token_id] * knowledge_length)
        processed_tensor = torch.tensor(processed_rows, dtype=torch.long)
        self._log_memory_statistics(
            total_sentences, truncated_sentences, num_to_process,
            knowledge_num, knowledge_length, processed_tensor.shape
        )
        return processed_tensor, database_mapping

    def _extract_sentence_info(self, item: Any) -> Dict[str, str]:
        if isinstance(item, dict):
            if "target" in item and len(item["target"]) > 0:
                target = item["target"][0]
                return {
                    "sentence": target.get("sentence", ""),
                    "uuid": target.get("uuid", ""),
                    "subject": target.get("subject", ""),
                    "predicate": target.get("predicate", ""),
                    "object": target.get("object", ""),
                }
            else:
                return {
                    "sentence": item.get("sentence", "") or item.get("text", "") or str(item),
                    "uuid": item.get("uuid", ""),
                    "subject": item.get("subject", ""),
                    "predicate": item.get("predicate", ""),
                    "object": item.get("object", ""),
                }
        else:
            return {
                "sentence": str(item),
                "uuid": "",
                "subject": "",
                "predicate": "",
                "object": "",
            }

    def _log_memory_statistics(
        self,
        total_sentences: int,
        truncated_sentences: int,
        num_processed: int,
        knowledge_num: int,
        knowledge_length: int,
        final_shape: torch.Size,
    ) -> None:
        truncation_ratio = truncated_sentences / total_sentences if total_sentences > 0 else 0.0
        Logger(f"æˆªæ–­å¥å­ç»Ÿè®¡:")
        Logger(f"  - æ€»å¥å­æ•°: {total_sentences}")
        Logger(f"  - æˆªæ–­å¥å­æ•°: {truncated_sentences}")
        Logger(f"  - æˆªæ–­å æ¯”: {truncation_ratio:.4f} ({truncation_ratio*100:.2f}%)")
        Logger(f"Memory_bankæ•°æ®å¤„ç†å®Œæˆ:")
        Logger(f"  - å¤„ç†å¥å­æ•°: {num_processed}")
        Logger(f"  - æ·»åŠ ç©ºæ¡ç›®æ•°: {knowledge_num - num_processed}")
        Logger(f"  - æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        Logger(f"  - æœŸæœ›å½¢çŠ¶: ({knowledge_num}, {knowledge_length})")

    def _save_memory_cache(
        self,
        processed_tensor: torch.Tensor,
        database_mapping: List[Dict],
        cache_path: str,
        database_path: str,
    ) -> None:
        try:
            torch.save(processed_tensor, cache_path)
            Logger(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {cache_path}")
        except Exception as e:
            Logger(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {e}")
        try:
            mapping_file_path = cache_path.replace(".pt", "_mapping.json")
            mapping_data = {
                "metadata": {
                    "total_entries": len(database_mapping),
                    "knowledge_num": processed_tensor.shape[0],
                    "knowledge_length": processed_tensor.shape[1],
                    "source_file": database_path,
                    "generation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                "mappings": database_mapping,
            }
            with open(mapping_file_path, "w", encoding="utf-8") as f:
                json.dump(mapping_data, f, ensure_ascii=False, indent=2)
            Logger(f"æ•°æ®åº“æ˜ å°„å·²ä¿å­˜åˆ°: {mapping_file_path}")
        except Exception as e:
            Logger(f"ä¿å­˜æ•°æ®åº“æ˜ å°„å¤±è´¥: {e}")


# ------------------------------------------------------------------
# ç»Ÿä¸€å…¥å£å‡½æ•°ï¼šåªæ”¹å‚æ•°è¯»å–æ–¹å¼ï¼Œå…¶ä½™ä¸åŠ¨
# ------------------------------------------------------------------
def init_model(args: dict, accelerator=None):
    """
    ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–æ¥å£ï¼ˆä»…æ”¯æŒQwen3æ¶æ„ï¼‰

    Args:
        args: é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - qwen3_model_path: Qwen3é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
            - è®°å¿†åº“ç›¸å…³é…ç½®ï¼ˆknowledge_num, knowledge_dimç­‰ï¼‰
        accelerator: Accelerator å¯¹è±¡ï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒæ—¶çš„æ—¥å¿—è¾“å‡º

    Returns:
        (model, tokenizer) tuple
    """
    # åªæ”¯æŒ Qwen3 æ¶æ„
    qwen3_model_path = args.get("qwen3_model_path", None)
    if qwen3_model_path is None:
        raise ValueError("å¿…é¡»æŒ‡å®šqwen3_model_pathå‚æ•°ï¼ŒæŒ‡å‘Qwen3-4Bé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    
    return _init_qwen3_model(args, accelerator)


def _init_qwen3_model(args: dict, accelerator=None):
    """Qwen3 æ¶æ„æ¨¡å‹åˆå§‹åŒ–"""
    qwen3_model_path = args.get("qwen3_model_path", None)
    if qwen3_model_path is None:
        raise ValueError("å¿…é¡»æŒ‡å®šqwen3_model_pathå‚æ•°ï¼ŒæŒ‡å‘Qwen3-4Bé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    
    database_init_path = args.get("database_init_path", None)
    cache_path = args.get("cache_path", "cache/knowledge_cache.pt")
    recompute_cache = args.get("recompute_cache", False)

    Logger("å¼€å§‹æ¨¡å‹åˆå§‹åŒ–æµç¨‹ï¼ˆQwen3æ¶æ„ï¼‰", accelerator)
    
    qwen3_config = Qwen3Config.from_pretrained(qwen3_model_path)
    
    # æå–è®°å¿†åº“é…ç½®
    memory_cfg = {
        "knowledge_num": args.get("knowledge_num", 1024 * 1024),
        "knowledge_length": args.get("knowledge_length", 16),
        "knowledge_dim": args.get("knowledge_dim", 128),
        # Memory bankåœ¨è®­ç»ƒæ—¶å›ºå®šï¼Œæ¨ç†æ—¶é€šè¿‡LLMLinguaæ›´æ–°ï¼ˆä¸å†ä½¿ç”¨EMAï¼‰
        "freeze_ratio": args.get("freeze_ratio", 0.2),
        "num_candidates": args.get("num_candidates", 16),
        "num_selected": args.get("num_selected", 1),
        "gumbel_temperature": args.get("gumbel_temperature", 1.0),
        "use_moe": args.get("use_moe", False),
        "dropout": args.get("dropout", 0.0),
    }
    from models.core.ExplicitLM import ExplicitLM
    
    try:
        original_cwd = get_original_cwd()
    except ValueError:
        # é Hydra ç¯å¢ƒï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
        original_cwd = os.getcwd()
    local_tokenizer_path = Path(original_cwd) / "models" / "qwen_tokenizer"
    if local_tokenizer_path.exists() and (local_tokenizer_path / "tokenizer.json").exists():
        Logger(f"  - ä½¿ç”¨æœ¬åœ°tokenizer: {local_tokenizer_path}", accelerator)
        tokenizer = AutoTokenizer.from_pretrained(str(local_tokenizer_path), trust_remote_code=True)
    else:
        Logger(f"  - ä»Qwenæ¨¡å‹è·¯å¾„åŠ è½½: {qwen3_model_path}", accelerator)
        tokenizer = AutoTokenizer.from_pretrained(qwen3_model_path, trust_remote_code=True)
    
    # ç¡®ä¿Qwen tokenizerçš„ç‰¹æ®Štokené…ç½®æ­£ç¡®
    if tokenizer.pad_token is None:
        # Qwen tokenizerå¯èƒ½æ²¡æœ‰pad_tokenï¼Œä½¿ç”¨eos_tokenä½œä¸ºpad_token
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        Logger("è­¦å‘Š: Qwen tokenizeræ²¡æœ‰pad_tokenï¼Œä½¿ç”¨eos_tokenä½œä¸ºpad_token", accelerator)

    # åˆ›å»ºæ¨¡å‹
    model = ExplicitLM(qwen3_config=qwen3_config, memory_cfg=memory_cfg)

    # ä»Qwen3é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡
    try:
        from transformers import Qwen3ForCausalLM
        pretrained_model = Qwen3ForCausalLM.from_pretrained(
            qwen3_model_path,
            torch_dtype=torch.float32,  # å…ˆåŠ è½½ä¸ºfloat32ï¼Œåç»­å¯ä»¥è½¬æ¢
            device_map="cpu",
        )
        
        # åŠ è½½åŒ¹é…çš„æƒé‡
        model_state_dict = model.state_dict()
        pretrained_state_dict = pretrained_model.state_dict()
        
        def map_pretrained_key(pretrained_key: str) -> str:
            """å°†é¢„è®­ç»ƒæ¨¡å‹çš„å±‚åç§°æ˜ å°„åˆ°æˆ‘ä»¬çš„æ¨¡å‹å±‚åç§°"""
            # ç§»é™¤ "model." å‰ç¼€
            if pretrained_key.startswith("model."):
                key = pretrained_key[6:]  # ç§»é™¤ "model." å‰ç¼€
            else:
                key = pretrained_key
            
            # æ˜ å°„å±‚åç§°
            if key.startswith("layers."):
                # å°† layers.X.xxx æ˜ å°„ä¸º layers.X.qwen3_decoder.xxx
                parts = key.split(".", 2)
                if len(parts) >= 3:
                    layer_idx = parts[1]
                    rest = parts[2]
                    return f"layers.{layer_idx}.qwen3_decoder.{rest}"
                else:
                    return key
            else:
                # embed_tokens, norm, lm_head ç­‰ç›´æ¥ä½¿ç”¨
                return key
        
        loaded_keys = []
        missing_keys = []
        shape_mismatches = []
        
        for key in model_state_dict.keys():
            # è·³è¿‡æ–°å¢çš„å‚æ•°
            if key.startswith("memory_bank") or key.startswith("tok_embeddings") or \
               "memory_gate" in key or "gated_memory_fusion" in key or "memory_norm" in key:
                continue
            
            # å°è¯•ç›´æ¥åŒ¹é…
            if key in pretrained_state_dict:
                if model_state_dict[key].shape == pretrained_state_dict[key].shape:
                    model_state_dict[key] = pretrained_state_dict[key]
                    loaded_keys.append(key)
                else:
                    shape_mismatches.append(f"{key} (shape: {model_state_dict[key].shape} vs {pretrained_state_dict[key].shape})")
            else:
                # å°è¯•é€šè¿‡æ˜ å°„æ‰¾åˆ°å¯¹åº”çš„é¢„è®­ç»ƒæƒé‡
                found = False
                for pretrained_key in pretrained_state_dict.keys():
                    mapped_key = map_pretrained_key(pretrained_key)
                    if mapped_key == key:
                        if model_state_dict[key].shape == pretrained_state_dict[pretrained_key].shape:
                            model_state_dict[key] = pretrained_state_dict[pretrained_key]
                            loaded_keys.append(key)
                            found = True
                            break
                        else:
                            shape_mismatches.append(f"{key} (shape: {model_state_dict[key].shape} vs {pretrained_state_dict[pretrained_key].shape})")
                            found = True
                            break
                
                if not found:
                    missing_keys.append(key)
        
        model.load_state_dict(model_state_dict, strict=False)
        Logger(f"æƒé‡åŠ è½½å®Œæˆ: {len(loaded_keys)}ä¸ªå‚æ•°", accelerator)
        if missing_keys:
            Logger(f"è­¦å‘Š: {len(missing_keys)}ä¸ªå‚æ•°æœªåŠ è½½", accelerator)
            if len(missing_keys) <= 10:
                for key in missing_keys[:10]:
                    Logger(f"    - {key}", accelerator)
        if shape_mismatches:
            Logger(f"è­¦å‘Š: {len(shape_mismatches)}ä¸ªå‚æ•°å½¢çŠ¶ä¸åŒ¹é…", accelerator)
            if len(shape_mismatches) <= 5:
                for key in shape_mismatches[:5]:
                    Logger(f"    - {key}", accelerator)
        
        # æ¸…ç†ä¸´æ—¶æ¨¡å‹
        del pretrained_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
    except Exception as e:
        Logger(f"è­¦å‘Š: ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡å¤±è´¥: {e}", accelerator)
        Logger("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡", accelerator)

    # æ•°æ®åº“ / è®°å¿†åº“åˆå§‹åŒ–ï¼ˆMOE æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
    use_moe = memory_cfg.get("use_moe", False)
    if use_moe:
        Logger("MOE æ¨¡å¼ï¼šè·³è¿‡è®°å¿†åº“åˆå§‹åŒ–", accelerator)
    elif database_init_path:
        Logger(f"  - æ•°æ®åº“è·¯å¾„: {database_init_path}", accelerator)
        Logger(f"  - ç¼“å­˜è·¯å¾„: {cache_path}", accelerator)
        Logger(f"  - çŸ¥è¯†åº“å¤§å°: {memory_cfg['knowledge_num']}", accelerator)
        Logger(f"  - çŸ¥è¯†æ¡ç›®é•¿åº¦: {memory_cfg['knowledge_length']}", accelerator)
        
        _initialize_database(
            model=model,
            tokenizer=tokenizer,
            database_path=database_init_path,
            cache_path=cache_path,
            knowledge_num=memory_cfg["knowledge_num"],
            knowledge_length=memory_cfg["knowledge_length"],
            recompute=recompute_cache,
            model_type="qwen3_explicitlm",
            database_attribute="memory_bank",
            accelerator=accelerator,
        )
    else:
        Logger("è­¦å‘Š: æœªæŒ‡å®šæ•°æ®åº“åˆå§‹åŒ–è·¯å¾„ï¼Œè®°å¿†åº“å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–", accelerator)

    # å†»ç»“Qwenä¸»æ¨¡å‹å‚æ•°ï¼Œåªä¿ç•™è®°å¿†åº“ç›¸å…³å‚æ•°å¯è®­ç»ƒ
    Logger("ğŸ”’ å†»ç»“Qwenä¸»æ¨¡å‹å‚æ•°...", accelerator)
    frozen_params = 0
    trainable_params = 0
    
    # å†»ç»“æ‰€æœ‰QwenåŸºç¡€ç»„ä»¶
    for name, param in model.named_parameters():
        # ä¿ç•™å¯è®­ç»ƒçš„å‚æ•°ï¼šè®°å¿†åº“ç›¸å…³ç»„ä»¶
        is_memory_component = any(keyword in name for keyword in [
            "memory_gate",  # è®°å¿†é—¨æ§æ¨¡å—
            "gated_memory_fusion",  # è®°å¿†èåˆæ¨¡å—
            "memory_norm",  # è®°å¿†å½’ä¸€åŒ–å±‚
        ])
        
        # memory_bankå­˜å‚¨çš„æ˜¯token IDsï¼ˆint64ï¼‰ï¼Œè®­ç»ƒæ—¶å›ºå®šï¼Œæ¨ç†æ—¶é€šè¿‡LLMLinguaæ›´æ–°
        # æ‰€ä»¥å§‹ç»ˆè®¾ç½®ä¸ºä¸å¯è®­ç»ƒ
        is_memory_bank = "memory_bank" in name
        if is_memory_bank:
            # memory_bankå§‹ç»ˆä¸å¯è®­ç»ƒï¼Œé¿å…DeepSpeedæ¢¯åº¦å¹³å‡æ—¶çš„ç±»å‹é”™è¯¯
            param.requires_grad = False
            frozen_params += param.numel()
        elif is_memory_component:
            # å…¶ä»–è®°å¿†ç›¸å…³ç»„ä»¶å§‹ç»ˆå¯è®­ç»ƒ
            param.requires_grad = True
            trainable_params += param.numel()
        else:
            # å†»ç»“æ‰€æœ‰å…¶ä»–å‚æ•°ï¼ˆQwenä¸»æ¨¡å‹ï¼‰
            param.requires_grad = False
            frozen_params += param.numel()
    
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    Logger(f"å‚æ•°å†»ç»“å®Œæˆ: å†»ç»“ {frozen_params / 1e6:.3f}M, å¯è®­ç»ƒ {trainable_params / 1e6:.3f}M", accelerator)

    return model, tokenizer


def load_pretrained_memory_gate(model: nn.Module, memory_gate_path: str, accelerator=None):
    """
    åŠ è½½é¢„è®­ç»ƒçš„ MemoryGate æƒé‡åˆ° ExplicitLM çš„æ‰€æœ‰å±‚
    
    Args:
        model: ExplicitLM æ¨¡å‹å®ä¾‹
        memory_gate_path: MemoryGate æƒé‡æ–‡ä»¶è·¯å¾„
        accelerator: Accelerator å®ä¾‹ï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—ï¼‰
    """
    if accelerator is None:
        from utils.logger import Logger
        Logger = lambda msg, acc: print(msg)
    
    Logger(f"åŠ è½½é¢„è®­ç»ƒ MemoryGate æƒé‡: {memory_gate_path}", accelerator)
    
    if not os.path.exists(memory_gate_path):
        raise FileNotFoundError(f"MemoryGate æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {memory_gate_path}")
    
    # åŠ è½½æƒé‡
    memory_gate_state = torch.load(memory_gate_path, map_location='cpu')
    
    # ç»Ÿè®¡åŠ è½½æƒ…å†µ
    loaded_layers = 0
    total_params = 0
    missing_keys = []
    unexpected_keys = []
    
    # éå†æ‰€æœ‰å±‚ï¼ŒåŠ è½½ MemoryGate æƒé‡
    for layer_idx, layer in enumerate(model.layers):
        if hasattr(layer, 'memory_gate') and layer.memory_gate is not None:
            try:
                # å°è¯•åŠ è½½æƒé‡
                missing, unexpected = layer.memory_gate.load_state_dict(memory_gate_state, strict=False)
                
                if missing:
                    missing_keys.extend([f"layer_{layer_idx}.{k}" for k in missing])
                if unexpected:
                    unexpected_keys.extend([f"layer_{layer_idx}.{k}" for k in unexpected])
                
                loaded_layers += 1
                total_params += sum(p.numel() for p in layer.memory_gate.parameters())
            except Exception as e:
                Logger(f"è­¦å‘Š: å±‚ {layer_idx} åŠ è½½ MemoryGate å¤±è´¥: {e}", accelerator)
    
    if loaded_layers == 0:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½• MemoryGate æ¨¡å—ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„")
    
    Logger(f"âœ“ MemoryGate åŠ è½½å®Œæˆ: {loaded_layers} å±‚, {total_params / 1e6:.3f}M å‚æ•°", accelerator)
    
    if missing_keys:
        Logger(f"è­¦å‘Š: {len(missing_keys)} ä¸ªå‚æ•°æœªæ‰¾åˆ°ï¼ˆå‰5ä¸ªï¼‰: {missing_keys[:5]}", accelerator)
    if unexpected_keys:
        Logger(f"è­¦å‘Š: {len(unexpected_keys)} ä¸ªæ„å¤–å‚æ•°ï¼ˆå‰5ä¸ªï¼‰: {unexpected_keys[:5]}", accelerator)
    
    return loaded_layers


def _initialize_database(
    model: nn.Module,
    tokenizer: AutoTokenizer,
    database_path: str,
    cache_path: str,
    knowledge_num: int,
    knowledge_length: int,
    recompute: bool,
    model_type: str,
    database_attribute: str,
    accelerator=None,
) -> None:
    if not os.path.exists(database_path):
        Logger(f"é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {database_path}", accelerator)
        raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {database_path}")
    
    cache_dir = os.path.dirname(cache_path)
    if cache_dir and not os.path.exists(cache_dir):
        os.makedirs(cache_dir, exist_ok=True)
    
        processor = MemoryBankProcessor(tokenizer)
        if not cache_path or cache_path == "cache/knowledge_cache.pt":
            cache_path = f"cache/memory_bank_init_{knowledge_num}_{knowledge_length}.pt"
        
        start_time = time.time()
        processed_tensor = processor.process_memory_bank(
            database_path=database_path,
            cache_path=cache_path,
            knowledge_num=knowledge_num,
            knowledge_length=knowledge_length,
            recompute=recompute,
        )
    
    if processed_tensor is None:
        raise ValueError("æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¿”å›çš„å¼ é‡ä¸ºNone")
    
    _set_database_attribute(model, database_attribute, processed_tensor, accelerator)
    
    total_time = time.time() - start_time
    Logger(f"è®°å¿†åº“åˆå§‹åŒ–å®Œæˆ: {processed_tensor.shape[0]}æ¡ç›®, è€—æ—¶{total_time:.2f}ç§’", accelerator)


def _set_database_attribute(model: nn.Module, attribute_path: str, data: torch.Tensor, accelerator=None) -> None:
    attributes = attribute_path.split(".")
    target = model
    for attr in attributes[:-1]:
        if not hasattr(target, attr):
            Logger(f"è­¦å‘Š: æ‰¾ä¸åˆ°å±æ€§ {attr}", accelerator)
            return
        target = getattr(target, attr)
    final_attr = attributes[-1]
    if hasattr(target, final_attr):
        getattr(target, final_attr).data.copy_(data)
    else:
        Logger(f"è­¦å‘Š: æ‰¾ä¸åˆ° model.{attribute_path}", accelerator)
        globals()["processed_database"] = data
