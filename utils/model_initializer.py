"""
æ¨¡å‹åˆå§‹åŒ–å·¥å…·æ¨¡å—

æä¾›ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹å’Œåˆå§‹åŒ–ç­–ç•¥ã€‚
åŒ…å«æƒé‡åˆå§‹åŒ–ã€é¢„è®­ç»ƒåµŒå…¥åŠ è½½ã€çŸ¥è¯†æ•°æ®åº“å¤„ç†ç­‰åŠŸèƒ½ã€‚

å…¼å®¹æ–°ç‰ˆ dict é…ç½®ç³»ç»Ÿã€‚
"""

import os
import json
import time
from typing import Dict, List, Tuple, Optional, Any
import torch
import torch.nn as nn
from transformers import AutoTokenizer
from hydra.utils import get_original_cwd
from pathlib import Path
from models.configs.LMConfig import LMConfig
from utils.logger import Logger


# ---------- ä»¥ä¸‹ä»£ç å®Œå…¨ä¸å˜ï¼Œä»…æŠŠ args å½“æˆ dict ä½¿ç”¨ ----------
class ModelTypeConfig:
    """æ¨¡å‹ç±»å‹é…ç½®æ˜ å°„"""
    SUPPORTED_TYPES = {
        "model": {
            "module_path": "model.core.ExplicitLM",
            "class_name": "ExplicitLM",
            "requires_weight_init": True,
            "database_attribute": "knowledge_dataset.knowledge_dataset",
        },
        "model_original": {
            "module_path": "model.model_original",
            "class_name": "ExplicitLM",
            "requires_weight_init": False,
            "database_attribute": None,
        },
        "model_no_feed": {
            "module_path": "model.model_no_feed",
            "class_name": "ExplicitLM",
            "requires_weight_init": True,
            "database_attribute": "knowledge_dataset.knowledge_dataset",
        },
        "model_memory": {
            "module_path": "models.core.ExplicitLM",
            "class_name": "ExplicitLM",
            "requires_weight_init": True,
            "database_attribute": "memory_bank",
            "memory_optimization": True,
        },
    }

    @classmethod
    def get_config(cls, model_type: str) -> Dict[str, Any]:
        if model_type not in cls.SUPPORTED_TYPES:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}ã€‚"
                f"æ”¯æŒçš„ç±»å‹: {list(cls.SUPPORTED_TYPES.keys())}"
            )
        return cls.SUPPORTED_TYPES[model_type]


class WeightInitializer:
    """æƒé‡åˆå§‹åŒ–å™¨"""

    @staticmethod
    def initialize_model_weights(model: nn.Module, model_type: str, accelerator=None) -> None:
        Logger("æ‰§è¡Œæ¨¡å‹æƒé‡åˆå§‹åŒ–...", accelerator)
        RMSNorm = WeightInitializer._import_rmsnorm(model_type, accelerator)
        WeightInitializer._init_embeddings(model, accelerator)
        WeightInitializer._init_layers(model, RMSNorm, accelerator)
        WeightInitializer._init_knowledge_components(model, accelerator)
        Logger("æ¨¡å‹æƒé‡åˆå§‹åŒ–å®Œæˆ", accelerator)

    @staticmethod
    def _import_rmsnorm(model_type: str, accelerator=None):
        try:
            config = ModelTypeConfig.get_config(model_type)
            module = __import__(config["module_path"], fromlist=["RMSNorm"])
            return module.RMSNorm
        except (ImportError, AttributeError):
            Logger("è­¦å‘Š: æ— æ³•å¯¼å…¥RMSNormï¼Œè·³è¿‡RMSNormåˆå§‹åŒ–", accelerator)
            return None

    @staticmethod
    def _init_embeddings(model: nn.Module, accelerator=None) -> None:
        if hasattr(model, "tok_embeddings"):
            nn.init.normal_(model.tok_embeddings.weight, mean=0.0, std=0.02)
        if hasattr(model, "output"):
            is_shared = (
                hasattr(model, "tok_embeddings")
                and hasattr(model.tok_embeddings, "weight")
                and model.output.weight is model.tok_embeddings.weight
            )
            if not is_shared:
                nn.init.normal_(model.output.weight, mean=0.0, std=0.02)

    @staticmethod
    def _init_layers(model: nn.Module, RMSNorm, accelerator=None) -> None:
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
            elif RMSNorm and isinstance(module, RMSNorm):
                if hasattr(module, "weight"):
                    nn.init.ones_(module.weight)

    @staticmethod
    def _init_knowledge_components(model: nn.Module, accelerator=None) -> None:
        if hasattr(model, "knowledge_dataset") and hasattr(model.knowledge_dataset, "keys"):
            nn.init.normal_(model.knowledge_dataset.keys, mean=0.0, std=0.02)


class EmbeddingLoader:
    @staticmethod
    def load_pretrained_embeddings(model: nn.Module, embedding_path: str, accelerator=None) -> None:
        Logger(f"åŠ è½½é¢„è®­ç»ƒåµŒå…¥æƒé‡: {embedding_path}", accelerator)
        pretrained_embeddings = torch.load(embedding_path)
        if hasattr(model, "tok_embeddings"):
            model.tok_embeddings.weight.data.copy_(pretrained_embeddings)
        if hasattr(model, "output"):
            model.output.weight.data.copy_(pretrained_embeddings)
        Logger("é¢„è®­ç»ƒåµŒå…¥æƒé‡åŠ è½½å®Œæˆ", accelerator)


class DatabaseProcessor:
    def __init__(self, tokenizer: AutoTokenizer):
        self.tokenizer = tokenizer
        self.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0

    # ä»¥ä¸‹æ‰€æœ‰æ–¹æ³•å®Œå…¨ä¸å˜ï¼Œä»…æŠŠ args å½“ dict ä½¿ç”¨
    def load_or_process_database(
        self,
        database_path: str,
        cache_path: str,
        knowledge_num: int,
        knowledge_length: int,
        recompute: bool = False,
    ) -> torch.Tensor:
        cache_dir = os.path.dirname(cache_path)
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)
        processed_tensor = self._try_load_cache(
            cache_path, knowledge_num, knowledge_length, recompute
        )
        if processed_tensor is None:
            processed_tensor = self._process_database(
                database_path, cache_path, knowledge_num, knowledge_length
            )
        return processed_tensor

    def _try_load_cache(
        self,
        cache_path: str,
        knowledge_num: int,
        knowledge_length: int,
        recompute: bool,
    ) -> Optional[torch.Tensor]:
        if recompute or not os.path.exists(cache_path):
            return None
        try:
            Logger(f"åŠ è½½ç¼“å­˜æ–‡ä»¶: {cache_path}")
            processed_tensor = torch.load(cache_path)
            cached_num, cached_length = processed_tensor.shape
            if cached_length != knowledge_length:
                Logger("ç¼“å­˜ knowledge_length ä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—...")
                return None
            if cached_num < knowledge_num:
                Logger("ç¼“å­˜ knowledge_num ä¸è¶³ï¼Œé‡æ–°è®¡ç®—...")
                return None
            if cached_num > knowledge_num:
                processed_tensor = processed_tensor[:knowledge_num, :]
            Logger(f"æˆåŠŸåŠ è½½ç¼“å­˜æ•°æ®ï¼Œå½¢çŠ¶: {processed_tensor.shape}")
            return processed_tensor
        except Exception as e:
            Logger(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°è®¡ç®—...")
            return None

    def _process_database(
        self,
        database_path: str,
        cache_path: str,
        knowledge_num: int,
        knowledge_length: int,
    ) -> torch.Tensor:
        Logger(f"åŠ è½½æ•°æ®åº“æ–‡ä»¶: {database_path}")
        with open(database_path, "r", encoding="utf-8") as f:
            database_data = json.load(f)
        sentences_data = self._extract_sentences(database_data)
        Logger(f"ä»æ•°æ®åº“åŠ è½½äº† {len(sentences_data)} æ¡å¥å­")
        processed_tensor, database_mapping = self._process_sentences(
            sentences_data, knowledge_num, knowledge_length
        )
        self._save_cache_and_mapping(
            processed_tensor, database_mapping, cache_path, database_path
        )
        return processed_tensor

    def _extract_sentences(self, database_data: List[Dict]) -> List[Dict[str, str]]:
        sentences_data = []
        for data in database_data:
            if "target" in data and len(data["target"]) > 0:
                target = data["target"][0]
                sentences_data.append(
                    {
                        "sentence": target.get("sentence", ""),
                        "uuid": target.get("uuid", ""),
                        "subject": target.get("subject", ""),
                        "predicate": target.get("predicate", ""),
                        "object": target.get("object", ""),
                    }
                )
        return sentences_data

    def _process_sentences(
        self,
        sentences_data: List[Dict[str, str]],
        knowledge_num: int,
        knowledge_length: int,
    ) -> Tuple[torch.Tensor, List[Dict]]:
        Logger("å¤„ç†å¥å­æ•°æ®...")
        processed_rows = []
        database_mapping = []
        num_to_process = min(knowledge_num, len(sentences_data))
        total_sentences = 0
        truncated_sentences = 0
        for i in range(num_to_process):
            sentence_data = sentences_data[i]
            sentence = sentence_data["sentence"]
            sentence_tokens = self.tokenizer.encode(sentence, add_special_tokens=False)
            original_length = len(sentence_tokens)
            total_sentences += 1
            if len(sentence_tokens) > knowledge_length:
                truncated_sentences += 1
                sentence_tokens = sentence_tokens[:knowledge_length]
            elif len(sentence_tokens) < knowledge_length:
                sentence_tokens.extend([self.pad_token_id] * (knowledge_length - len(sentence_tokens)))
            processed_rows.append(sentence_tokens)
            database_mapping.append(
                {
                    "database_index": i,
                    "uuid": sentence_data["uuid"],
                    "sentence": sentence,
                    "subject": sentence_data.get("subject", ""),
                    "predicate": sentence_data.get("predicate", ""),
                    "object": sentence_data.get("object", ""),
                    "token_count": len(sentence_tokens),
                    "is_truncated": original_length > knowledge_length,
                }
            )
        while len(processed_rows) < knowledge_num:
            processed_rows.append([self.pad_token_id] * knowledge_length)
        processed_tensor = torch.tensor(processed_rows, dtype=torch.long)
        self._log_statistics(
            total_sentences, truncated_sentences, num_to_process,
            knowledge_num, knowledge_length, processed_tensor.shape
        )
        return processed_tensor, database_mapping

    def _log_statistics(
        self,
        total_sentences: int,
        truncated_sentences: int,
        num_processed: int,
        knowledge_num: int,
        knowledge_length: int,
        final_shape: torch.Size,
    ) -> None:
        truncation_ratio = truncated_sentences / total_sentences if total_sentences > 0 else 0.0
        Logger(f"æˆªæ–­å¥å­ç»Ÿè®¡:")
        Logger(f"  - æ€»å¥å­æ•°: {total_sentences}")
        Logger(f"  - æˆªæ–­å¥å­æ•°: {truncated_sentences}")
        Logger(f"  - æˆªæ–­å æ¯”: {truncation_ratio:.4f} ({truncation_ratio*100:.2f}%)")
        Logger(f"æ•°æ®å¤„ç†å®Œæˆ:")
        Logger(f"  - å¤„ç†å¥å­æ•°: {num_processed}")
        Logger(f"  - æ·»åŠ ç©ºæ¡ç›®æ•°: {knowledge_num - num_processed}")
        Logger(f"  - æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        Logger(f"  - æœŸæœ›å½¢çŠ¶: ({knowledge_num}, {knowledge_length})")

    def _save_cache_and_mapping(
        self,
        processed_tensor: torch.Tensor,
        database_mapping: List[Dict],
        cache_path: str,
        database_path: str,
    ) -> None:
        try:
            torch.save(processed_tensor, cache_path)
            Logger(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {cache_path}")
        except Exception as e:
            Logger(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {e}")
        try:
            mapping_file_path = cache_path.replace(".pt", "_mapping.json")
            mapping_data = {
                "metadata": {
                    "total_entries": len(database_mapping),
                    "knowledge_num": processed_tensor.shape[0],
                    "knowledge_length": processed_tensor.shape[1],
                    "source_file": database_path,
                    "generation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                "mappings": database_mapping,
            }
            with open(mapping_file_path, "w", encoding="utf-8") as f:
                json.dump(mapping_data, f, ensure_ascii=False, indent=2)
            Logger(f"æ•°æ®åº“æ˜ å°„å·²ä¿å­˜åˆ°: {mapping_file_path}")
        except Exception as e:
            Logger(f"ä¿å­˜æ•°æ®åº“æ˜ å°„å¤±è´¥: {e}")


class MemoryBankProcessor:
    def __init__(self, tokenizer: AutoTokenizer):
        self.tokenizer = tokenizer
        self.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0

    def process_memory_bank(
        self,
        database_path: str,
        cache_path: str,
        knowledge_num: int,
        knowledge_length: int,
    ) -> torch.Tensor:
        if os.path.exists(cache_path):
            Logger(f"ä»ç¼“å­˜åŠ è½½memory_bankåˆå§‹åŒ–æ•°æ®: {cache_path}")
            processed_tensor = torch.load(cache_path)
            Logger(f"åŠ è½½çš„memory_bankæ•°æ®å½¢çŠ¶: {processed_tensor.shape}")
            return processed_tensor
        Logger(f"å¤„ç†æ–‡æœ¬æ•°æ®ç”¨äºmemory_bankåˆå§‹åŒ–: {database_path}")
        with open(database_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        Logger(f"ä» {database_path} åŠ è½½äº† {len(data)} æ¡å¥å­")
        processed_tensor, database_mapping = self._process_memory_sentences(
            data, knowledge_num, knowledge_length
        )
        self._save_memory_cache(
            processed_tensor, database_mapping, cache_path, database_path
        )
        return processed_tensor

    def _process_memory_sentences(
        self,
        data: List,
        knowledge_num: int,
        knowledge_length: int,
    ) -> Tuple[torch.Tensor, List[Dict]]:
        processed_rows = []
        database_mapping = []
        total_sentences = len(data)
        truncated_sentences = 0
        num_to_process = min(len(data), knowledge_num)
        Logger(f"å¤„ç† {num_to_process}/{total_sentences} æ¡å¥å­")
        for idx, item in enumerate(data[:num_to_process]):
            if idx % 1000 == 0:
                Logger(f"å¤„ç†å¥å­ {idx+1}/{num_to_process}")
            sentence_info = self._extract_sentence_info(item)
            sentence = sentence_info["sentence"]
            try:
                tokens_result = self.tokenizer(
                    sentence,
                    add_special_tokens=True,
                    truncation=True,
                    max_length=len(sentence),
                    padding=False,
                    return_tensors="pt",
                )
                tokens = tokens_result["input_ids"].squeeze().tolist()
                if not isinstance(tokens, list):
                    tokens = [tokens]
                original_length = len(tokens)
                if len(tokens) > knowledge_length:
                    tokens = tokens[:knowledge_length]
                    truncated_sentences += 1
                elif len(tokens) < knowledge_length:
                    tokens.extend([self.pad_token_id] * (knowledge_length - len(tokens)))
                processed_rows.append(tokens)
                database_mapping.append(
                    {
                        "database_index": idx,
                        "uuid": sentence_info["uuid"],
                        "sentence": sentence,
                        "subject": sentence_info["subject"],
                        "predicate": sentence_info["predicate"],
                        "object": sentence_info["object"],
                        "token_count": len(tokens),
                        "is_truncated": original_length > knowledge_length,
                    }
                )
            except Exception as e:
                Logger(f"å¤„ç†å¥å­ {idx} æ—¶å‡ºé”™: {e}")
                empty_tokens = [self.pad_token_id] * knowledge_length
                processed_rows.append(empty_tokens)
                database_mapping.append(
                    {
                        "database_index": idx,
                        "uuid": sentence_info["uuid"],
                        "sentence": sentence,
                        "subject": sentence_info["subject"],
                        "predicate": sentence_info["predicate"],
                        "object": sentence_info["object"],
                        "token_count": knowledge_length,
                        "is_truncated": False,
                        "processing_error": str(e),
                    }
                )
        while len(processed_rows) < knowledge_num:
            processed_rows.append([self.pad_token_id] * knowledge_length)
        processed_tensor = torch.tensor(processed_rows, dtype=torch.long)
        self._log_memory_statistics(
            total_sentences, truncated_sentences, num_to_process,
            knowledge_num, knowledge_length, processed_tensor.shape
        )
        return processed_tensor, database_mapping

    def _extract_sentence_info(self, item: Any) -> Dict[str, str]:
        if isinstance(item, dict):
            if "target" in item and len(item["target"]) > 0:
                target = item["target"][0]
                return {
                    "sentence": target.get("sentence", ""),
                    "uuid": target.get("uuid", ""),
                    "subject": target.get("subject", ""),
                    "predicate": target.get("predicate", ""),
                    "object": target.get("object", ""),
                }
            else:
                return {
                    "sentence": item.get("sentence", "") or item.get("text", "") or str(item),
                    "uuid": item.get("uuid", ""),
                    "subject": item.get("subject", ""),
                    "predicate": item.get("predicate", ""),
                    "object": item.get("object", ""),
                }
        else:
            return {
                "sentence": str(item),
                "uuid": "",
                "subject": "",
                "predicate": "",
                "object": "",
            }

    def _log_memory_statistics(
        self,
        total_sentences: int,
        truncated_sentences: int,
        num_processed: int,
        knowledge_num: int,
        knowledge_length: int,
        final_shape: torch.Size,
    ) -> None:
        truncation_ratio = truncated_sentences / total_sentences if total_sentences > 0 else 0.0
        Logger(f"æˆªæ–­å¥å­ç»Ÿè®¡:")
        Logger(f"  - æ€»å¥å­æ•°: {total_sentences}")
        Logger(f"  - æˆªæ–­å¥å­æ•°: {truncated_sentences}")
        Logger(f"  - æˆªæ–­å æ¯”: {truncation_ratio:.4f} ({truncation_ratio*100:.2f}%)")
        Logger(f"Memory_bankæ•°æ®å¤„ç†å®Œæˆ:")
        Logger(f"  - å¤„ç†å¥å­æ•°: {num_processed}")
        Logger(f"  - æ·»åŠ ç©ºæ¡ç›®æ•°: {knowledge_num - num_processed}")
        Logger(f"  - æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        Logger(f"  - æœŸæœ›å½¢çŠ¶: ({knowledge_num}, {knowledge_length})")

    def _save_memory_cache(
        self,
        processed_tensor: torch.Tensor,
        database_mapping: List[Dict],
        cache_path: str,
        database_path: str,
    ) -> None:
        try:
            torch.save(processed_tensor, cache_path)
            Logger(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {cache_path}")
        except Exception as e:
            Logger(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {e}")
        try:
            mapping_file_path = cache_path.replace(".pt", "_mapping.json")
            mapping_data = {
                "metadata": {
                    "total_entries": len(database_mapping),
                    "knowledge_num": processed_tensor.shape[0],
                    "knowledge_length": processed_tensor.shape[1],
                    "source_file": database_path,
                    "generation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                "mappings": database_mapping,
            }
            with open(mapping_file_path, "w", encoding="utf-8") as f:
                json.dump(mapping_data, f, ensure_ascii=False, indent=2)
            Logger(f"æ•°æ®åº“æ˜ å°„å·²ä¿å­˜åˆ°: {mapping_file_path}")
        except Exception as e:
            Logger(f"ä¿å­˜æ•°æ®åº“æ˜ å°„å¤±è´¥: {e}")


# ------------------------------------------------------------------
# ç»Ÿä¸€å…¥å£å‡½æ•°ï¼šåªæ”¹å‚æ•°è¯»å–æ–¹å¼ï¼Œå…¶ä½™ä¸åŠ¨
# ------------------------------------------------------------------
def init_model(args: dict, accelerator=None):
    """
    ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–æ¥å£ï¼ˆç›´æ¥ä½¿ç”¨ dict é…ç½®ï¼‰

    Args:
        args: é…ç½®å­—å…¸ï¼Œå«å…¨éƒ¨è¶…å‚
        accelerator: Accelerator å¯¹è±¡ï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒæ—¶çš„æ—¥å¿—è¾“å‡º

    Returns:
        (model, tokenizer) tuple
    """
    model_type = args.get("model_variant", "model_memory")
    pretrained_embedding_path = args.get("pretrained_embedding_path", None)
    database_init_path = args.get("database_init_path", None)
    cache_path = args.get("cache_path", "cache/knowledge_cache.pt")
    recompute_cache = args.get("recompute_cache", False)

    Logger("=" * 60, accelerator)
    Logger("ğŸš€ å¼€å§‹æ¨¡å‹åˆå§‹åŒ–æµç¨‹", accelerator)
    Logger("=" * 60, accelerator)
    Logger(f"ğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯:", accelerator)
    Logger(f"  - æ¨¡å‹ç±»å‹: {model_type}", accelerator)
    Logger(f"  - é¢„è®­ç»ƒåµŒå…¥è·¯å¾„: {pretrained_embedding_path if pretrained_embedding_path else 'æœªæŒ‡å®š'}", accelerator)
    Logger(f"  - æ•°æ®åº“åˆå§‹åŒ–è·¯å¾„: {database_init_path if database_init_path else 'æœªæŒ‡å®š'}", accelerator)
    Logger(f"  - ç¼“å­˜è·¯å¾„: {cache_path}", accelerator)
    Logger(f"  - é‡æ–°è®¡ç®—ç¼“å­˜: {recompute_cache}", accelerator)
    
    type_config = ModelTypeConfig.get_config(model_type)
    Logger(f"  - æ•°æ®åº“å±æ€§: {type_config.get('database_attribute', 'æ— ')}", accelerator)
    Logger(f"  - éœ€è¦æƒé‡åˆå§‹åŒ–: {type_config.get('requires_weight_init', False)}", accelerator)
    Logger(f"  - å†…å­˜ä¼˜åŒ–: {type_config.get('memory_optimization', False)}", accelerator)

    # åŠ¨æ€å¯¼å…¥æ¨¡å‹ç±»
    Logger(f"ğŸ“¦ å¯¼å…¥æ¨¡å‹æ¨¡å—: {type_config['module_path']}", accelerator)
    module = __import__(type_config["module_path"], fromlist=[type_config["class_name"]])
    ExplicitLM = getattr(module, type_config["class_name"])
    
    # è¾“å‡ºå½“å‰ç›®å½•
    Logger(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}", accelerator)
    
    # åŠ è½½ tokenizer
    Logger("ğŸ”¤ åŠ è½½tokenizer...", accelerator)
    tokenizer_dir = Path(get_original_cwd()) / "models" / "ExplicitLM_tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_dir))
    Logger(f"âœ… TokenizeråŠ è½½å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}", accelerator)

    # æ„é€  LMConfig å¯¹è±¡ï¼ˆä»…ç”¨äºæ»¡è¶³æ—§æ„é€ å‡½æ•°ç­¾åï¼‰
    # lm_config = LMConfig(
    #     dim=args["dim"],
    #     n_layers=args["n_layers"],
    #     n_heads=args["n_heads"],
    #     n_kv_heads=args["n_kv_heads"],
    #     vocab_size=args["vocab_size"],
    #     max_seq_len=args["max_seq_len"],
    #     knowledge_num=args["knowledge_num"],
    #     knowledge_length=args["knowledge_length"],
    #     knowledge_dim=args["knowledge_dim"],
    #     model_variant=model_type,
    #     pretrained_embedding_path=pretrained_embedding_path,
    #     database_init_path=database_init_path,
    #     cache_path=cache_path,
    #     recompute_cache=recompute_cache,
    #     use_moe=args.get("use_moe", False),
    #     flash_attn=args.get("flash_attn", True),
    #     dropout=args.get("dropout", 0.0),
    # )

    # åˆ›å»ºæ¨¡å‹
    Logger("ğŸ—ï¸  åˆ›å»ºæ¨¡å‹å®ä¾‹...", accelerator)
    model = ExplicitLM(args)
    Logger("âœ… æ¨¡å‹å®ä¾‹åˆ›å»ºå®Œæˆ", accelerator)

    # æƒé‡åˆå§‹åŒ–
    if type_config["requires_weight_init"]:
        Logger("âš–ï¸  æ‰§è¡Œæ¨¡å‹æƒé‡åˆå§‹åŒ–...", accelerator)
        WeightInitializer.initialize_model_weights(model, model_type, accelerator)
        Logger("âœ… æ¨¡å‹æƒé‡åˆå§‹åŒ–å®Œæˆ", accelerator)

    if type_config.get("memory_optimization"):
        Logger("âœ… æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ï¼šå€™é€‰é¡¹å‡å°‘(32â†’16) + DeepSpeedå‚æ•°offload", accelerator)

    # é¢„è®­ç»ƒåµŒå…¥
    if pretrained_embedding_path:
        Logger("ğŸ¯ åŠ è½½é¢„è®­ç»ƒåµŒå…¥æƒé‡...", accelerator)
        EmbeddingLoader.load_pretrained_embeddings(model, pretrained_embedding_path, accelerator)
        Logger("âœ… é¢„è®­ç»ƒåµŒå…¥æƒé‡åŠ è½½å®Œæˆ", accelerator)

    # æ•°æ®åº“ / è®°å¿†åº“åˆå§‹åŒ–
    if database_init_path and type_config["database_attribute"]:
        Logger("ğŸ—„ï¸  å¼€å§‹æ•°æ®åº“/è®°å¿†åº“åˆå§‹åŒ–...", accelerator)
        Logger(f"  - æ•°æ®åº“è·¯å¾„: {database_init_path}", accelerator)
        Logger(f"  - ç¼“å­˜è·¯å¾„: {cache_path}", accelerator)
        Logger(f"  - çŸ¥è¯†åº“å¤§å°: {args.knowledge_num}", accelerator)
        Logger(f"  - çŸ¥è¯†æ¡ç›®é•¿åº¦: {args.knowledge_length}", accelerator)
        Logger(f"  - ç›®æ ‡å±æ€§: {type_config['database_attribute']}", accelerator)
        
        _initialize_database(
            model=model,
            tokenizer=tokenizer,
            database_path=database_init_path,
            cache_path=cache_path,
            knowledge_num=args.knowledge_num,
            knowledge_length=args.knowledge_length,
            recompute=recompute_cache,
            model_type=model_type,
            database_attribute=type_config["database_attribute"],
            accelerator=accelerator,
        )
        Logger("âœ… æ•°æ®åº“/è®°å¿†åº“åˆå§‹åŒ–å®Œæˆ", accelerator)
    else:
        if not database_init_path:
            Logger("âš ï¸  æœªæŒ‡å®šæ•°æ®åº“åˆå§‹åŒ–è·¯å¾„ï¼Œè·³è¿‡æ•°æ®åº“åˆå§‹åŒ–", accelerator)
        if not type_config["database_attribute"]:
            Logger("âš ï¸  å½“å‰æ¨¡å‹ç±»å‹ä¸æ”¯æŒæ•°æ®åº“åˆå§‹åŒ–ï¼Œè·³è¿‡", accelerator)

    # å‚æ•°ç»Ÿè®¡
    Logger("ğŸ“Š è®¡ç®—æ¨¡å‹å‚æ•°ç»Ÿè®¡...", accelerator)
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    Logger(f"ğŸ“ˆ LLMæ€»å‚æ•°é‡ï¼š{total_params:.3f} ç™¾ä¸‡", accelerator)
    
    Logger("=" * 60, accelerator)
    Logger("ğŸ‰ æ¨¡å‹åˆå§‹åŒ–æµç¨‹å®Œæˆ", accelerator)
    Logger("=" * 60, accelerator)

    return model, tokenizer


# ---------- ä»¥ä¸‹è¾…åŠ©å‡½æ•°å®Œå…¨ä¸å˜ ----------
def _initialize_database(
    model: nn.Module,
    tokenizer: AutoTokenizer,
    database_path: str,
    cache_path: str,
    knowledge_num: int,
    knowledge_length: int,
    recompute: bool,
    model_type: str,
    database_attribute: str,
    accelerator=None,
) -> None:
    Logger("ğŸ” å¼€å§‹æ•°æ®åº“/è®°å¿†åº“åˆå§‹åŒ–è¯¦ç»†æµç¨‹", accelerator)
    Logger(f"ğŸ“Š åˆå§‹åŒ–å‚æ•°:", accelerator)
    Logger(f"  - æ¨¡å‹ç±»å‹: {model_type}", accelerator)
    Logger(f"  - æ•°æ®åº“è·¯å¾„: {database_path}", accelerator)
    Logger(f"  - ç¼“å­˜è·¯å¾„: {cache_path}", accelerator)
    Logger(f"  - çŸ¥è¯†åº“å¤§å°: {knowledge_num}", accelerator)
    Logger(f"  - çŸ¥è¯†æ¡ç›®é•¿åº¦: {knowledge_length}", accelerator)
    Logger(f"  - é‡æ–°è®¡ç®—ç¼“å­˜: {recompute}", accelerator)
    Logger(f"  - ç›®æ ‡å±æ€§è·¯å¾„: {database_attribute}", accelerator)
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(database_path):
        Logger(f"âŒ é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {database_path}", accelerator)
        raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {database_path}")
    
    # è·å–æ•°æ®åº“æ–‡ä»¶ä¿¡æ¯
    try:
        file_size = os.path.getsize(database_path)
        file_size_mb = file_size / (1024 * 1024)
        Logger(f"ğŸ“ æ•°æ®åº“æ–‡ä»¶ä¿¡æ¯:", accelerator)
        Logger(f"  - æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB ({file_size:,} bytes)", accelerator)
        Logger(f"  - æ–‡ä»¶è·¯å¾„: {os.path.abspath(database_path)}", accelerator)
    except Exception as e:
        Logger(f"âš ï¸  æ— æ³•è·å–æ•°æ®åº“æ–‡ä»¶ä¿¡æ¯: {e}", accelerator)
    
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    cache_dir = os.path.dirname(cache_path)
    if cache_dir and not os.path.exists(cache_dir):
        Logger(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {cache_dir}", accelerator)
        os.makedirs(cache_dir, exist_ok=True)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¤„ç†å™¨
    if model_type == "model_memory":
        Logger("ğŸ§  ä½¿ç”¨MemoryBankProcessorå¤„ç†è®°å¿†åº“æ•°æ®", accelerator)
        processor = MemoryBankProcessor(tokenizer)
        if not cache_path or cache_path == "cache/knowledge_cache.pt":
            cache_path = f"cache/memory_bank_init_{knowledge_num}_{knowledge_length}.pt"
            Logger(f"ğŸ”„ è‡ªåŠ¨è°ƒæ•´ç¼“å­˜è·¯å¾„ä¸º: {cache_path}", accelerator)
        
        Logger("ğŸš€ å¼€å§‹å¤„ç†è®°å¿†åº“æ•°æ®...", accelerator)
        start_time = time.time()
        processed_tensor = processor.process_memory_bank(
            database_path=database_path,
            cache_path=cache_path,
            knowledge_num=knowledge_num,
            knowledge_length=knowledge_length,
        )
        processing_time = time.time() - start_time
        Logger(f"â±ï¸  è®°å¿†åº“æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’", accelerator)
        
    else:
        Logger("ğŸ’¾ ä½¿ç”¨DatabaseProcessorå¤„ç†çŸ¥è¯†åº“æ•°æ®", accelerator)
        processor = DatabaseProcessor(tokenizer)
        
        Logger("ğŸš€ å¼€å§‹å¤„ç†çŸ¥è¯†åº“æ•°æ®...", accelerator)
        start_time = time.time()
        processed_tensor = processor.load_or_process_database(
            database_path=database_path,
            cache_path=cache_path,
            knowledge_num=knowledge_num,
            knowledge_length=knowledge_length,
            recompute=recompute,
        )
        processing_time = time.time() - start_time
        Logger(f"â±ï¸  çŸ¥è¯†åº“æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’", accelerator)
    
    # éªŒè¯å¤„ç†åçš„å¼ é‡
    Logger("ğŸ” éªŒè¯å¤„ç†åçš„æ•°æ®å¼ é‡...", accelerator)
    if processed_tensor is not None:
        Logger(f"âœ… å¼ é‡éªŒè¯é€šè¿‡:", accelerator)
        Logger(f"  - å¼ é‡å½¢çŠ¶: {processed_tensor.shape}", accelerator)
        Logger(f"  - å¼ é‡ç±»å‹: {processed_tensor.dtype}", accelerator)
        Logger(f"  - å¼ é‡è®¾å¤‡: {processed_tensor.device}", accelerator)
        Logger(f"  - å†…å­˜å ç”¨: {processed_tensor.numel() * processed_tensor.element_size() / (1024*1024):.2f} MB", accelerator)
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´
        if processed_tensor.numel() > 0:
            min_val = processed_tensor.min().item()
            max_val = processed_tensor.max().item()
            Logger(f"  - æ•°æ®èŒƒå›´: [{min_val}, {max_val}]", accelerator)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if min_val < 0:
                Logger(f"âš ï¸  è­¦å‘Š: å‘ç°è´Ÿå€¼token IDï¼Œæœ€å°å€¼: {min_val}", accelerator)
            if hasattr(tokenizer, 'vocab_size') and max_val >= tokenizer.vocab_size:
                Logger(f"âš ï¸  è­¦å‘Š: å‘ç°è¶…å‡ºè¯æ±‡è¡¨èŒƒå›´çš„token IDï¼Œæœ€å¤§å€¼: {max_val}, è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}", accelerator)
    else:
        Logger("âŒ é”™è¯¯: å¤„ç†åçš„å¼ é‡ä¸ºNone", accelerator)
        raise ValueError("æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¿”å›çš„å¼ é‡ä¸ºNone")
    
    # è®¾ç½®æ¨¡å‹å±æ€§
    Logger("ğŸ”§ è®¾ç½®æ¨¡å‹æ•°æ®åº“å±æ€§...", accelerator)
    start_time = time.time()
    _set_database_attribute(model, database_attribute, processed_tensor, accelerator)
    attribute_setting_time = time.time() - start_time
    Logger(f"â±ï¸  æ¨¡å‹å±æ€§è®¾ç½®å®Œæˆï¼Œè€—æ—¶: {attribute_setting_time:.4f} ç§’", accelerator)
    
    # éªŒè¯æ¨¡å‹å±æ€§æ˜¯å¦æ­£ç¡®è®¾ç½®
    Logger("ğŸ” éªŒè¯æ¨¡å‹å±æ€§è®¾ç½®...", accelerator)
    attributes = database_attribute.split(".")
    target = model
    for attr in attributes[:-1]:
        if hasattr(target, attr):
            target = getattr(target, attr)
        else:
            Logger(f"âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°ä¸­é—´å±æ€§ {attr}", accelerator)
            return
    
    final_attr = attributes[-1]
    if hasattr(target, final_attr):
        stored_tensor = getattr(target, final_attr)
        if torch.equal(stored_tensor, processed_tensor):
            Logger(f"âœ… æ¨¡å‹å±æ€§éªŒè¯æˆåŠŸ: model.{database_attribute}", accelerator)
            Logger(f"  - å­˜å‚¨å¼ é‡å½¢çŠ¶: {stored_tensor.shape}", accelerator)
            Logger(f"  - å­˜å‚¨å¼ é‡ç±»å‹: {stored_tensor.dtype}", accelerator)
            Logger(f"  - å­˜å‚¨å¼ é‡è®¾å¤‡: {stored_tensor.device}", accelerator)
        else:
            Logger(f"âŒ é”™è¯¯: æ¨¡å‹å±æ€§éªŒè¯å¤±è´¥ï¼Œå­˜å‚¨çš„å¼ é‡ä¸åŸå§‹å¼ é‡ä¸åŒ¹é…", accelerator)
    else:
        Logger(f"âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°ç›®æ ‡å±æ€§ {final_attr}", accelerator)
    
    # æ€»ä½“ç»Ÿè®¡
    total_time = time.time() - start_time
    Logger("ğŸ“Š æ•°æ®åº“/è®°å¿†åº“åˆå§‹åŒ–ç»Ÿè®¡:", accelerator)
    Logger(f"  - æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’", accelerator)
    Logger(f"  - æ•°æ®æ¡ç›®æ•°: {processed_tensor.shape[0]}", accelerator)
    Logger(f"  - æ¯æ¡ç›®é•¿åº¦: {processed_tensor.shape[1]}", accelerator)
    Logger(f"  - æ€»tokenæ•°: {processed_tensor.numel()}", accelerator)
    Logger(f"  - å¤„ç†é€Ÿåº¦: {processed_tensor.numel() / total_time:.0f} tokens/ç§’", accelerator)
    
    Logger("âœ… æ•°æ®åº“åµŒå…¥å’Œå¥å­å·²æˆåŠŸå­˜å‚¨åˆ°æ¨¡å‹", accelerator)


def _set_database_attribute(model: nn.Module, attribute_path: str, data: torch.Tensor, accelerator=None) -> None:
    attributes = attribute_path.split(".")
    target = model
    for attr in attributes[:-1]:
        if not hasattr(target, attr):
            Logger(f"è­¦å‘Š: æ‰¾ä¸åˆ°å±æ€§ {attr}ï¼Œæ— æ³•åˆå§‹åŒ–æ•°æ®åº“", accelerator)
            return
        target = getattr(target, attr)
    final_attr = attributes[-1]
    if hasattr(target, final_attr):
        getattr(target, final_attr).data.copy_(data)
        Logger(f"æˆåŠŸåˆå§‹åŒ– model.{attribute_path} ä½¿ç”¨å¤„ç†åçš„æ•°æ®", accelerator)
    else:
        Logger(f"è­¦å‘Š: æ‰¾ä¸åˆ° model.{attribute_path} è¿›è¡Œåˆå§‹åŒ–", accelerator)
        globals()["processed_database"] = data
